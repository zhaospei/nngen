inspect_mount_handle ( guestfs_h * g ) <nl> exit ( EXIT_FAILURE ); <nl> } <nl>  <nl> + /* Free old global if there is one . */ <nl> + free ( root ); <nl> + <nl> root = roots [ 0 ]; <nl> free ( roots ); <nl> 
static char * replace_str ( const char * str , const char * old , /* {{{ */ <nl> } else <nl> retlen = strlen ( str ); <nl>  <nl> - ret = malloc ( retlen + 1 ); <nl> + ret = calloc ( 1 , retlen + 1 ); <nl> if ( ret == NULL ) <nl> return NULL ; <nl> // added to original : not optimized , but keeps valgrind happy . <nl> - memset ( ret , 0 , retlen + 1 ); <nl>  <nl> r = ret ; <nl> p = str ;
static int cpu_read ( void ) <nl> submit ( cpu , " wait ", wait ); <nl> submit ( cpu , " interrupt ", intr ); <nl> submit ( cpu , " softirq ", sitr ); <nl> + <nl> + if ( numfields >= 9 ) <nl> + submit ( cpu , " steal ", atoll ( fields [ 8 ])); <nl> } <nl> } <nl> 
krb5_dbe_def_cpw ( krb5_context context , <nl> krb5_boolean keepold , <nl> krb5_db_entry * db_entry ); <nl>  <nl> + krb5_error_code <nl> + krb5_def_promote_db ( krb5_context , char *, char **); <nl> + <nl> krb5_error_code <nl> krb5_db_create_policy ( krb5_context kcontext , <nl> osa_policy_ent_t policy );
errcode_t profile_open_file ( const_profile_filespec_t filespec , <nl> } <nl> if ( data ) { <nl> data -> refcount ++; <nl> + data -> last_stat = 0 ; /* Make sure to stat when updating . */ <nl> k5_mutex_unlock (& g_shared_trees_mutex ); <nl> retval = profile_update_file_data ( data , NULL ); <nl> free ( expanded_filename );
krb5_get_realm_domain ( realm , domain ) <nl> krb5_xfree ( realmlist [ 0 ]); <nl> krb5_xfree ( realmlist ); <nl> } <nl> - * domain = NULL ; <nl> + if (( retdomain = malloc ( strlen ( realm ) + 2 )) == NULL ) <nl> + return ENOMEM ; <nl> + strcpy ( retdomain , "."); <nl> + strcat ( retdomain , realm ); /* return the realm as the domain <nl> + if lookup fails */ <nl> + * domain = retdomain ; <nl> return 0 ; <nl> } <nl> continue ;
static int cabd_read_headers ( struct mspack_system * sys , <nl> } <nl> else { <nl> /* ignore invalid file and continue parsing */ <nl> + if ( file -> filename ) { <nl> + sys -> free ( file -> filename ); <nl> + file -> filename = NULL ; <nl> + } <nl> sys -> free ( file ); <nl> sys -> message ( fh , " WARNING ; omitting file % d of % d from file list ", i , num_files ); <nl> }
static int handle_stream ( client_conn_t * conn , struct fd_buf * buf , const struct o <nl> pthread_mutex_unlock (& exit_mutex ); <nl> } <nl> * error = 1 ; <nl> + return - 1 ; <nl> } else { <nl> pos = 4 ; <nl> memmove ( buf -> buffer , & buf -> buffer [ pos ], buf -> off - pos );
get_rsc_restart_list ( lrm_rsc_t * rsc , lrm_op_t * op ) <nl> } <nl>  <nl> metadata = string2xml ( metadata_str ); <nl> + if ( metadata == NULL ) { <nl> + crm_err (" Metadata for % s ::% s :% s is not valid XML ", <nl> + rsc -> provider , rsc -> class , rsc -> type ); <nl> + return NULL ; <nl> + } <nl> + <nl> actions = find_xml_node ( metadata , " actions ", TRUE ); <nl>  <nl> xml_child_iter_filter (
bool ParseHDKeypath ( std :: string keypath_str , std :: vector < uint32_t >& keypath ) <nl> return false ; <nl> } <nl> uint32_t number ; <nl> - ParseUInt32 ( item , & number ); <nl> + if (! ParseUInt32 ( item , & number )) { <nl> + return false ; <nl> + } <nl> path |= number ; <nl>  <nl> keypath . push_back ( path );
void ServiceConnection ( AcceptedConnection * conn ) <nl> // Read HTTP message headers and body <nl> ReadHTTPMessage ( conn -> stream (), mapHeaders , strRequest , nProto ); <nl>  <nl> + if ( strURI != "/") { <nl> + conn -> stream () << HTTPReply ( HTTP_NOT_FOUND , "", false ) << std :: flush ; <nl> + break ; <nl> + } <nl> + <nl> // Check authorization <nl> if ( mapHeaders . count (" authorization ") == 0 ) <nl> {
int git_libgit2_opts ( int key , ...) <nl> void git_strarray_free ( git_strarray * array ) <nl> { <nl> size_t i ; <nl> + <nl> + if ( array == NULL ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < array -> count ; ++ i ) <nl> git__free ( array -> strings [ i ]); <nl> 
int git_reference_foreach ( <nl> if ( list_flags & GIT_REF_PACKED ) { <nl> const char * ref_name ; <nl> void * ref ; <nl> + GIT_UNUSED ( ref ); <nl>  <nl> if ( packed_load ( repo ) < 0 ) <nl> return - 1 ;
void * Type_MLU_Read ( struct _cms_typehandler_struct * self , cmsIOHANDLER * io , cmsU <nl>  <nl> // Check for overflow <nl> if ( Offset < ( SizeOfHeader + 8 )) goto Error ; <nl> + if (( Offset + Len ) > SizeOfTag + 8 ) goto Error ; <nl>  <nl> // True begin of the string <nl> BeginOfThisString = Offset - SizeOfHeader - 8 ;
void HandleSwitches ( int argc , char * argv []) <nl> case ' d ': <nl> case ' D ': { <nl> cmsFloat64Number ObserverAdaptationState = atof ( xoptarg ); <nl> - if ( ObserverAdaptationState < 0 && <nl> + if ( ObserverAdaptationState < 0 || <nl> ObserverAdaptationState > 1 . 0 ) <nl> FatalError (" Adaptation states should be between 0 and 1 "); <nl> 
static void lookup_hostname ( struct hostinfo * hi ) <nl> char ** ap ; <nl> static char addrbuf [ HOST_NAME_MAX + 1 ]; <nl>  <nl> - hent = gethostbyname ( hostname . buf ); <nl> + hent = gethostbyname ( hi -> hostname . buf ); <nl> if ( hent ) { <nl> ap = hent -> h_addr_list ; <nl> memset (& sa , 0 , sizeof sa );
static int find_common ( int fd [ 2 ], unsigned char * result_sha1 , <nl> retval = 0 ; <nl> in_vain = 0 ; <nl> got_continue = 1 ; <nl> + if ( ack == ACK_ready ) <nl> + rev_list = NULL ; <nl> break ; <nl> } <nl> }
static struct child_process * git_connect_git ( int fd [ 2 ], char * hostandport , <nl> target_host = xstrdup ( hostandport ); <nl>  <nl> transport_check_allowed (" git "); <nl> + if ( strchr ( target_host , '\ n ') || strchr ( path , '\ n ')) <nl> + die ( _ (" newline is forbidden in git :// hosts and repo paths ")); <nl>  <nl> /* <nl> * These underlying connection commands die () if they
static void show_line ( struct grep_opt * opt , char * bol , char * eol , <nl>  <nl> * eol = '\ 0 '; <nl> while ( next_match ( opt , bol , eol , ctx , & match , eflags )) { <nl> + if ( match . rm_so == match . rm_eo ) <nl> + break ; <nl> printf ("%.* s % s %.* s % s ", <nl> ( int ) match . rm_so , bol , <nl> opt -> color_match ,
int send_pack ( struct send_pack_args * args , <nl> ref -> status = REF_STATUS_NONE ; <nl> if ( args -> stateless_rpc ) <nl> close ( out ); <nl> + if ( git_connection_is_socket ( conn )) <nl> + shutdown ( fd [ 0 ], SHUT_WR ); <nl> if ( use_sideband ) <nl> finish_async (& demux ); <nl> return - 1 ;
static int write_pseudoref ( const char * pseudoref , const struct object_id * oid , <nl> struct strbuf buf = STRBUF_INIT ; <nl> int ret = - 1 ; <nl>  <nl> + if (! oid ) <nl> + return 0 ; <nl> + <nl> strbuf_addf (& buf , "% s \ n ", oid_to_hex ( oid )); <nl>  <nl> filename = git_path ("% s ", pseudoref );
static int get_sha1_oneline ( const char * prefix , unsigned char * sha1 ) <nl> unsigned long size ; <nl>  <nl> commit = pop_most_recent_commit (& list , ONELINE_SEEN ); <nl> - parse_object ( commit -> object . sha1 ); <nl> + if (! parse_object ( commit -> object . sha1 )) <nl> + continue ; <nl> if ( temp_commit_buffer ) <nl> free ( temp_commit_buffer ); <nl> if ( commit -> buffer )
unsigned long count_delta ( void * delta_buf , unsigned long delta_size ) <nl> /* delete size is what was _not_ copied from source . <nl> * edit size is that and literal additions . <nl> */ <nl> + if ( src_size + added_literal < copied_from_source ) <nl> + /* we ended up overcounting and underflowed */ <nl> + return 0 ; <nl> return ( src_size - copied_from_source ) + added_literal ; <nl> }
static int setup_index ( unsigned char * sha1 ) <nl> return - 1 ; <nl>  <nl> new_pack = parse_pack_index ( sha1 ); <nl> + if (! new_pack ) <nl> + return - 1 ; /* parse_pack_index () already issued error message */ <nl> new_pack -> next = repo -> packs ; <nl> repo -> packs = new_pack ; <nl> return 0 ;
void master_service_deinit ( struct master_service ** _service ) <nl> lib_signals_deinit (); <nl> io_loop_destroy (& service -> ioloop ); <nl>  <nl> + if ( service -> listener_names != NULL ) <nl> + p_strsplit_free ( default_pool , service -> listener_names ); <nl> i_free ( service -> listeners ); <nl> i_free ( service -> getopt_str ); <nl> i_free ( service -> name );
squat_trie_lookup_real ( struct squat_trie * trie , const char * str , <nl> unsigned int i , start , bytes , str_bytelen , str_charlen ; <nl> int ret = 0 ; <nl>  <nl> + array_clear ( definite_uids ); <nl> + array_clear ( maybe_uids ); <nl> + <nl> memset (& ctx , 0 , sizeof ( ctx )); <nl> ctx . trie = trie ; <nl> ctx . type = type ;
void imapc_connection_connect ( struct imapc_connection * conn , <nl> unsigned int ips_count ; <nl> int ret ; <nl>  <nl> - if ( conn -> fd != - 1 ) { <nl> + if ( conn -> fd != - 1 || conn -> dns_lookup != NULL ) { <nl> i_assert ( login_callback == NULL ); <nl> return ; <nl> }
static void sasl_callback ( struct client * _client , enum sasl_server_reply reply , <nl> data : AUTH_FAILED_MSG , NULL ); <nl> client_send_line ( client , msg ); <nl>  <nl> - if (! client -> destroyed ) { <nl> + if (! client -> destroyed && ! client -> auth_initializing ) { <nl> /* get back to normal client input . */ <nl> if ( client -> io != NULL ) <nl> io_remove (& client -> io );
static void driver_pgsql_close ( struct pgsql_db * db ) <nl> db -> io_dir = 0 ; <nl> db -> fatal_error = FALSE ; <nl>  <nl> + driver_pgsql_stop_io ( db ); <nl> + <nl> PQfinish ( db -> pg ); <nl> db -> pg = NULL ; <nl>  <nl> - driver_pgsql_stop_io ( db ); <nl> if ( db -> to_connect != NULL ) <nl> timeout_remove (& db -> to_connect ); <nl> 
static int <nl> imapc_mailbox_exists ( struct mailbox * box , bool auto_boxes ATTR_UNUSED , <nl> enum mailbox_existence * existence_r ) <nl> { <nl> + if ( strcmp ( box -> list -> name , MAILBOX_LIST_NAME_IMAPC ) != 0 ) { <nl> + if ( box -> inbox_any ) <nl> + * existence_r = MAILBOX_EXISTENCE_SELECT ; <nl> + else <nl> + * existence_r = MAILBOX_EXISTENCE_NONE ; <nl> + return 0 ; <nl> + } <nl> + <nl> enum mailbox_info_flags flags ; <nl>  <nl> struct imapc_mailbox_list * list = ( struct imapc_mailbox_list *) box -> list ;
struct file_dict_transaction_context { <nl> }; <nl>  <nl> static struct dotlock_settings file_dict_dotlock_settings = { <nl> - . timeout = 30 , <nl> - . stale_timeout = 5 <nl> + . timeout = 60 * 2 , <nl> + . stale_timeout = 60 , <nl> + . use_io_notify = TRUE <nl> }; <nl>  <nl> static struct dict * file_dict_init ( struct dict * driver , const char * uri ,
static void mail_transaction_log_2_unlink_old ( struct mail_transaction_log * log ) <nl> return ; <nl> } <nl>  <nl> - if ( st . st_mtime + log -> index -> log_rotate_log2_stale_secs <= ioloop_time && <nl> + if ( ioloop_time - st . st_mtime >= ( time_t ) log -> index -> log_rotate_log2_stale_secs && <nl> ! log -> index -> readonly ) <nl> i_unlink_if_exists ( log -> filepath2 ); <nl> }
void program_client_program_input ( struct program_client * pclient ) <nl> } <nl> if ( program_client_input_pending ( pclient )) <nl> return ; <nl> - if (! input -> eof ) { <nl> + if ( pclient -> program_input != NULL && ! input -> eof ) { <nl> program_client_fail ( pclient , <nl> PROGRAM_CLIENT_ERROR_IO ); <nl> return ;
imap_msgpart_crlf_seek ( struct mail * mail , struct istream * input , <nl> if ( message_skip_virtual ( input , virtual_skip , & cr_skipped ) < 0 ) { <nl> errinput = i_stream_create_error ( errno ); <nl> i_stream_set_name ( errinput , i_stream_get_name ( input )); <nl> + i_stream_unref (& input ); <nl> return errinput ; <nl> } <nl> 
void buffer_verify_pool ( buffer_t * _buf ) <nl> const struct real_buffer * buf = ( const struct real_buffer *) _buf ; <nl> void * ret ; <nl>  <nl> - if ( buf -> pool != NULL && buf -> pool -> datastack_pool ) { <nl> + if ( buf -> pool != NULL && buf -> pool -> datastack_pool && buf -> alloc > 0 ) { <nl> /* this doesn ' t really do anything except verify the <nl> stack frame */ <nl> ret = p_realloc ( buf -> pool , buf -> w_buffer ,
int mountpoint_get ( const char * path , pool_t pool , struct mountpoint * point_r ) <nl> if ( device_path == NULL ) <nl> return 0 ; <nl>  <nl> + memset ( point_r , 0 , sizeof (* point_r )); <nl> point_r -> device_path = p_strdup ( pool , device_path ); <nl> point_r -> mount_path = p_strdup ( pool , mount_path ); <nl> point_r -> type = p_strdup ( pool , type );
client_deliver ( struct client * client , const struct mail_recipient * rcpt , <nl> input = mail_storage_service_user_get_input ( rcpt -> service_user ); <nl> username = t_strdup ( input -> username ); <nl>  <nl> - if ( rcpt -> parallel_count >= client -> lmtp_set -> lmtp_user_concurrency_limit ) { <nl> + if ( client -> lmtp_set -> lmtp_user_concurrency_limit > 0 && <nl> + rcpt -> parallel_count >= client -> lmtp_set -> lmtp_user_concurrency_limit ) { <nl> client_send_line ( client , ERRSTR_TEMP_USERDB_FAIL_PREFIX <nl> " Too many concurrent deliveries for user ", <nl> rcpt -> address );
initialize_global_set_options ( void ) <nl>  <nl> GlobalSetOptions . maxclients = ServerInfo . default_max_clients ; <nl>  <nl> - if ( GlobalSetOptions . maxclients > ( maxconnections - MAX_BUFFER )) <nl> + if ( GlobalSetOptions . maxclients > ( maxconnections - MAX_BUFFER ) || ( GlobalSetOptions . maxclients <= 0 )) <nl> GlobalSetOptions . maxclients = maxconnections - MAX_BUFFER ; <nl>  <nl> GlobalSetOptions . autoconn = 1 ;
static CURLcode tftp_rx ( tftp_state_data_t * state , tftp_event_t event ) <nl> } <nl>  <nl> /* Check if completed ( That is , a less than full packet is received ) */ <nl> - if ( state -> rbytes < sizeof ( state -> spacket )){ <nl> + if ( state -> rbytes < ( ssize_t ) sizeof ( state -> spacket )){ <nl> state -> state = TFTP_STATE_FIN ; <nl> } <nl> else {
ConnectionExists ( struct Curl_easy * data , <nl> if ( chosen ) { <nl> /* mark it as used before releasing the lock */ <nl> chosen -> inuse = TRUE ; <nl> + chosen -> data = data ; /* own it ! */ <nl> Curl_conncache_unlock ( needle ); <nl> * usethis = chosen ; <nl> return TRUE ; /* yes , we found one to use ! */
int Curl_select ( int nfds , <nl> SET_SOCKERRNO ( EINVAL ); <nl> return - 1 ; <nl> } <nl> - timeout_ms = ( timeout -> tv_sec * 1000 ) + ( timeout -> tv_usec / 1000 ); <nl> + timeout_ms = ( int )( timeout -> tv_sec * 1000 ) + ( int )( timeout -> tv_usec / 1000 ); <nl> } <nl> else { <nl> timeout_ms = - 1 ;
static CURLcode AddFormData ( struct FormData ** formp , <nl> file */ <nl> if (! strequal ("-", newform -> line )) { <nl> struct_stat file ; <nl> - if (! stat ( newform -> line , & file ) && S_ISREG ( file . st_mode )) <nl> + if (! stat ( newform -> line , & file ) && ! S_ISDIR ( file . st_mode )) <nl> * size += file . st_size ; <nl> else <nl> return CURLE_BAD_FUNCTION_ARGUMENT ;
static CURLcode imap_disconnect ( struct connectdata * conn ) <nl>  <nl> Curl_pp_disconnect (& imapc -> pp ); <nl>  <nl> + free ( imapc -> mailbox ); <nl> + <nl> return CURLE_OK ; <nl> } <nl> 
CURLcode Curl_conncache_add_conn ( struct conncache * connc , <nl> return result ; <nl>  <nl> key = hashkey ( conn ); <nl> - if (! key ) <nl> + if (! key ) { <nl> + bundle_destroy ( new_bundle ); <nl> return CURLE_OUT_OF_MEMORY ; <nl> + } <nl>  <nl> rc = conncache_add_bundle ( data -> state . conn_cache , key , new_bundle ); <nl> free ( key );
CURLcode http_auth_headers ( struct connectdata * conn , <nl> if (! data -> state . authstage ) { <nl> if ( conn -> bits . httpproxy && conn -> bits . proxy_user_passwd ) <nl> Curl_http_auth_stage ( data , 407 ); <nl> - else <nl> + else if ( conn -> bits . user_passwd ) <nl> Curl_http_auth_stage ( data , 401 ); <nl> + else <nl> + return CURLE_OK ; /* no authentication with no user or password */ <nl> } <nl>  <nl> /* To prevent the user + password to get sent to other than the original
_pango_emoji_iter_next ( PangoEmojiIter * iter ) <nl> if ( iter -> is_emoji == PANGO_EMOJI_TYPE_IS_EMOJI ( current_emoji_type )) <nl> { <nl> iter -> is_emoji = ! PANGO_EMOJI_TYPE_IS_EMOJI ( current_emoji_type ); <nl> + <nl> + /* Make sure we make progress . Weird sequences , like a VC15 followed <nl> + * by VC16 , can trick us into stalling otherwise . */ <nl> + if ( iter -> start == iter -> end ) <nl> + iter -> end = g_utf8_next_char ( iter -> end ); <nl> + <nl> return TRUE ; <nl> } <nl> }
# include < stdlib . h > <nl> # include < stdio . h > <nl> # include " pbs_error . h " <nl> +# include " momctl . h " <nl>  <nl> extern int flush_rc ; <nl> extern char * string_read ;
static char * active_pbs_server ; <nl> pbs_net_t trq_server_addr ; <nl> char trq_hostname [ PBS_MAXSERVERNAME + 1 ]; <nl>  <nl> +/* Get the name of the active pbs_server */ <nl> int load_trqauthd_config ( <nl>  <nl> char ** default_server_name ,
static int mod_init ( void ) <nl> */ <nl> static int child_init ( int rank ) <nl> { <nl> + if ( rank == PROC_INIT ) { <nl> + return 0 ; <nl> + } <nl> _apy_process_rank = rank ; <nl> PyOS_AfterFork (); <nl> return apy_init_script ( rank );
int mp_header_process_sequence_header ( mp_mpeg_header_t * picture , const unsigne <nl> picture -> mpeg1 = 1 ; <nl> picture -> picture_structure = 3 ; // FRAME_PICTURE ; <nl> picture -> display_time = 100 ; <nl> + picture -> frame_rate_extension_n = 1 ; <nl> + picture -> frame_rate_extension_d = 1 ; <nl> return 0 ; <nl> } <nl> 
static void pass_add_hook ( struct gl_video * p , struct tex_hook hook ) <nl> p -> tex_hooks [ p -> tex_hook_num ++] = hook ; <nl> } else { <nl> MP_ERR ( p , " Too many hooks ! Limit is % d .\ n ", MAX_TEXTURE_HOOKS ); <nl> + <nl> + if ( hook . free ) <nl> + hook . free (& hook ); <nl> } <nl> } <nl> 
static const struct gl_functions gl_functions [] = { <nl> // uniform buffer object extensions , requires OpenGL 3 . 1 . <nl> { <nl> . ver_core = 310 , <nl> - . extension = " ARB_uniform_buffer_object ", <nl> + . extension = " GL_ARB_uniform_buffer_object ", <nl> . functions = ( const struct gl_function []) { <nl> DEF_FN ( GetUniformBlockIndex ), <nl> DEF_FN ( UniformBlockBinding ),
static void uninit ( struct dec_audio * da ) <nl> av_freep (& lavf_ctx -> pb -> buffer ); <nl> av_freep (& lavf_ctx -> pb ); <nl> avformat_free_context ( lavf_ctx ); <nl> + spdif_ctx -> lavf_ctx = NULL ; <nl> } <nl> } <nl> 
void vo_seek_reset ( struct vo * vo ) <nl> { <nl> vo_control ( vo , VOCTRL_RESET , NULL ); <nl> vo -> frame_loaded = false ; <nl> + vo -> hasframe = false ; <nl> mp_image_unrefp (& vo -> waiting_mpi ); <nl> } <nl> 
void add_subtitles ( char * filename , float fps , int silent ) <nl> # ifdef USE_ASS <nl> if ( ass_enabled ) <nl> asst = ass_read_file ( filename ); <nl> - if ( ass_enabled && ! asst ) <nl> + if ( ass_enabled && subd && ! asst ) <nl> asst = ass_read_subdata ( subd , fps ); <nl>  <nl> if (! asst && ! subd && ! silent )
static void update_osd_msg ( void ) { <nl> # ifdef USE_OSD <nl> if ( sh_video ) vo_osd_changed ( OSDTYPE_OSD ); else <nl> # endif <nl> - if ( term_osd ) printf ("% s % s \ n ", term_osd_esc , msg -> msg ); <nl> + if ( term_osd ) mp_msg ( MSGT_CPLAYER , MSGL_STATUS ,"% s % s \ n ", term_osd_esc , msg -> msg ); <nl> } <nl> return ; <nl> }
static bool get_screenshot_from_texture ( d3d_priv * priv , mp_image_t * image ) <nl> struct texplane * plane = & priv -> planes [ n ]; <nl>  <nl> int width = priv -> src_width >> plane -> shift_x ; <nl> - int height = priv -> src_height >> plane -> shift_x ; <nl> + int height = priv -> src_height >> plane -> shift_y ; <nl>  <nl> memcpy_pic ( image -> planes [ n ], plane -> locked_rect . pBits , <nl> width * plane -> bytes_per_pixel , height ,
int init_audio_codec ( sh_audio_t * sh_audio ) <nl> sh_audio -> sample_format = AFMT_S16_LE ; <nl> # endif <nl> sh_audio -> samplerate = 0 ; <nl> + sh_audio -> channels = 0 ; <nl> sh_audio -> i_bps = 0 ; // input rate ( bytes / sec ) <nl> sh_audio -> o_bps = 0 ; // output rate ( bytes / sec ) <nl> 
extern FILE * open_bitstream ( const char * dname , const char * filename ); <nl>  <nl> extern void close_device_fd ( struct thr_info *); <nl>  <nl> +# define for_each_managed_proc ( procvar , dev ) \ <nl> + for ( struct cgpu_info * procvar = dev ; procvar ; procvar = procvar -> next_proc ) <nl> +# define for_each_logical_proc ( procvar , dev ) \ <nl> + for ( struct cgpu_info * procvar = dev ; procvar -> proc_id < dev -> procs ; procvar = procvar -> next_proc ) <nl> + <nl> # endif
bool hashbusterusb_vrm_unlock ( struct cgpu_info * const proc , const char * const <nl> hex2bin (& buf [ 1 ], code , size ); <nl>  <nl> hashbusterusb_io ( h , buf , buf ); <nl> - return memcmp ( buf , "\ x12 \ 0 ", 2 ); <nl> + return ! memcmp ( buf , "\ x12 \ 0 ", 2 ); <nl> } <nl>  <nl> static
/****************************************************************************** <nl> * <nl> * Module Name : exutils - interpreter / scanner utilities <nl> - * $ Revision : 1 . 116 $ <nl> + * $ Revision : 1 . 117 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AcpiExEisaIdToString ( <nl> * <nl> * RETURN : None , string <nl> * <nl> - * DESCRIPTOIN : Convert a number to string representation . Assumes string <nl> + * DESCRIPTION : Convert a number to string representation . Assumes string <nl> * buffer is large enough to hold the string . <nl> * <nl> ******************************************************************************/
/****************************************************************************** <nl> * <nl> * Module Name : evregion - ACPI AddressSpace / OpRegion handler dispatch <nl> - * $ Revision : 1 . 79 $ <nl> + * $ Revision : 1 . 80 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AcpiEvInstallDefaultAddressSpaceHandlers ( <nl>  <nl> FUNCTION_TRACE (" EvInstallDefaultAddressSpaceHandlers "); <nl>  <nl> - /* <nl> + /* <nl> * All address spaces ( PCI Config , EC , SMBus ) are scope dependent <nl> * and registration must occur for a specific device . In the case <nl> * system memory and IO address spaces there is currently no device
void ppce500_init ( PPCE500Params * params ) <nl>  <nl> /* Fixup Memory size on a alignment boundary */ <nl> ram_size &= ~( RAM_SIZES_ALIGN - 1 ); <nl> + params -> ram_size = ram_size ; <nl>  <nl> /* Register Memory */ <nl> memory_region_init_ram ( ram , " mpc8544ds . ram ", ram_size );
static int mode_sense_page ( SCSIDiskState * s , int page , uint8_t ** p_outbuf , <nl>  <nl> case MODE_PAGE_R_W_ERROR : <nl> length = 10 ; <nl> + if ( page_control == 1 ) { /* Changeable Values */ <nl> + break ; <nl> + } <nl> p [ 0 ] = 0x80 ; /* Automatic Write Reallocation Enabled */ <nl> if ( s -> qdev . type == TYPE_ROM ) { <nl> p [ 1 ] = 0x20 ; /* Read Retry Count */
static void qemu_tcg_init_cpu_signals ( void ) <nl> } <nl> # endif /* _WIN32 */ <nl>  <nl> - QemuMutex qemu_global_mutex ; <nl> + static QemuMutex qemu_global_mutex ; <nl> static QemuCond qemu_io_proceeded_cond ; <nl> static bool iothread_requesting_mutex ; <nl> 
BlockDriverAIOCB * paio_ioctl ( BlockDriverState * bs , int fd , <nl> acb -> aio_type = QEMU_AIO_IOCTL ; <nl> acb -> aio_fildes = fd ; <nl> acb -> ev_signo = SIGUSR2 ; <nl> + acb -> async_context_id = get_async_context_id (); <nl> acb -> aio_offset = 0 ; <nl> acb -> aio_ioctl_buf = buf ; <nl> acb -> aio_ioctl_cmd = req ;
static GSList * gd_vc_gfx_init ( GtkDisplayState * s , VirtualConsole * vc , <nl>  <nl> if ( dpy_ui_info_supported ( vc -> gfx . dcl . con )) { <nl> gtk_menu_item_activate ( GTK_MENU_ITEM ( s -> zoom_fit_item )); <nl> + s -> free_scale = true ; <nl> } <nl>  <nl> return group ;
static int alloc_refcount_block ( BlockDriverState * bs , <nl> uint64_t last_table_size ; <nl> uint64_t blocks_clusters ; <nl> do { <nl> - uint64_t table_clusters = size_to_clusters ( s , table_size ); <nl> + uint64_t table_clusters = <nl> + size_to_clusters ( s , table_size * sizeof ( uint64_t )); <nl> blocks_clusters = 1 + <nl> (( table_clusters + refcount_block_clusters - 1 ) <nl> / refcount_block_clusters );
static void cloop_close ( BlockDriverState * bs ) <nl> { <nl> BDRVCloopState * s = bs -> opaque ; <nl> if ( s -> n_blocks > 0 ) { <nl> - free ( s -> offsets ); <nl> + g_free ( s -> offsets ); <nl> } <nl> - free ( s -> compressed_block ); <nl> - free ( s -> uncompressed_block ); <nl> + g_free ( s -> compressed_block ); <nl> + g_free ( s -> uncompressed_block ); <nl> inflateEnd (& s -> zstream ); <nl> } <nl> 
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } else { <nl> dev -> attached = 1 ; <nl> } <nl> - if ( dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> + if ( dev -> setup_index < 0 || <nl> + dev -> setup_len < 0 || <nl> + dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> return - EINVAL ; <nl> }
void qcow2_free_any_clusters ( BlockDriverState * bs , uint64_t l2_entry , <nl> } <nl> break ; <nl> case QCOW2_CLUSTER_NORMAL : <nl> - qcow2_free_clusters ( bs , l2_entry & L2E_OFFSET_MASK , <nl> - nb_clusters << s -> cluster_bits , type ); <nl> + case QCOW2_CLUSTER_ZERO : <nl> + if ( l2_entry & L2E_OFFSET_MASK ) { <nl> + qcow2_free_clusters ( bs , l2_entry & L2E_OFFSET_MASK , <nl> + nb_clusters << s -> cluster_bits , type ); <nl> + } <nl> break ; <nl> case QCOW2_CLUSTER_UNALLOCATED : <nl> - case QCOW2_CLUSTER_ZERO : <nl> break ; <nl> default : <nl> abort ();
struct m_hdr { <nl> struct mbuf { <nl> struct m_hdr m_hdr ; <nl> Slirp * slirp ; <nl> + bool arp_requested ; <nl> + uint64_t expiration_date ; <nl> + /* start of dynamic buffer area , must be last element */ <nl> union M_dat { <nl> char m_dat_ [ 1 ]; /* ANSI don ' t like 0 sized arrays */ <nl> char * m_ext_ ; <nl> } M_dat ; <nl> - bool arp_requested ; <nl> - uint64_t expiration_date ; <nl> }; <nl>  <nl> # define m_next m_hdr . mh_next
int msix_init ( struct PCIDevice * dev , unsigned short nentries , <nl> return 0 ; <nl>  <nl> err_config : <nl> + dev -> msix_entries_nr = 0 ; <nl> cpu_unregister_io_memory ( dev -> msix_mmio_index ); <nl> err_index : <nl> qemu_free ( dev -> msix_table_page );
int cli_parse_add ( struct cli_matcher * root , const char * virname , const char * hex <nl> if (( n = cli_strtok ( pt , 2 , "-"))) { /* strict check */ <nl> error = 1 ; <nl> free ( n ); <nl> + break ; <nl> } <nl> } <nl> }
static void print_value ( int output , int num , const char * devname , <nl> print_udev_format ( name , value ); <nl>  <nl> } else if ( output & OUTPUT_EXPORT_LIST ) { <nl> + if ( num == 1 && devname ) <nl> + printf (" DEVNAME =% s \ n ", devname ); <nl> fputs ( name , stdout ); <nl> fputs ("=", stdout ); <nl> safe_print ( value , valsz );
static char * replace_u ( char * str , char * username ) <nl> } <nl> sz = strlen ( str ); <nl>  <nl> - if ( p == str && sz == 2 ) <nl> + if ( p == str && sz == 2 ) { <nl> /* ' str ' contains only '\ u ' */ <nl> + free ( old ); <nl> return username ; <nl> + } <nl>  <nl> tp = entry = malloc ( sz + usz ); <nl> if (! tp )
blkid_partition blkid_partlist_devno_to_partition ( blkid_partlist ls , dev_t devno <nl> if ( blkid_partition_get_start ( par ) == start && <nl> blkid_partition_get_size ( par ) == size ) <nl> return par ; <nl> + <nl> + /* exception for extended dos partitions */ <nl> + if ( blkid_partition_get_start ( par ) == start && <nl> + blkid_partition_is_extended ( par ) && size <= 1024 ) <nl> + return par ; <nl> + <nl> } <nl> return NULL ; <nl> }
display_summary ( void ) <nl> continue ; <nl> cn = canonicalize_path ( dev ); <nl> if ( cn ) <nl> - printf ("%- 40s % s ", cn , p ); <nl> + printf ("%- 39s % s ", cn , p ); <nl> free ( dev ); <nl> free ( cn ); <nl> }
void rpmProblemSetFree ( rpmProblemSet probs ); <nl> void rpmProblemSetFilter ( rpmProblemSet ps , int flags ); <nl> int rpmRunTransactions ( rpmTransactionSet ts , rpmCallbackFunction notify , <nl> void * notifyData , rpmProblemSet okProbs , <nl> - rpmProblemSet * newProbs , int flags ); <nl> + rpmProblemSet * newProbs , int flags , int ignoreSet ); <nl>  <nl> # define RPMPROB_FILTER_IGNOREOS ( 1 << 0 ) <nl> # define RPMPROB_FILTER_IGNOREARCH ( 1 << 1 )
EXPORTED void jmap_closembox ( jmap_req_t * req , struct mailbox ** mboxp ) <nl> struct _mboxcache_rec * rec = NULL ; <nl> int i ; <nl>  <nl> + if (! mboxp || !* mboxp ) return ; <nl> + <nl> for ( i = 0 ; i < req -> mboxes -> count ; i ++) { <nl> rec = ( struct _mboxcache_rec *) ptrarray_nth ( req -> mboxes , i ); <nl> if ( rec -> mbox == * mboxp )
static int setCalendarEvents ( struct jmap_req * req ) <nl>  <nl> json_incref ( set ); <nl> json_t * item = json_pack ("[]"); <nl> - json_array_append_new ( item , json_string (" calendarsEventsSet ")); <nl> + json_array_append_new ( item , json_string (" calendarEventsSet ")); <nl> json_array_append_new ( item , set ); <nl> json_array_append_new ( item , json_string ( req -> tag )); <nl> json_array_append_new ( req -> response , item );
int mailbox_rename_cleanup ( struct mailbox ** mailboxptr , int isinbox ) <nl> */ <nl> int mailbox_copyfile ( const char * from , const char * to , int nolink ) <nl> { <nl> - int flags = 0 ; <nl> + int flags = COPYFILE_MKDIR ; <nl> if ( nolink ) flags |= COPYFILE_NOLINK ; <nl> - return cyrus_copyfile ( from , to , flags ); <nl> + <nl> + if ( cyrus_copyfile ( from , to , flags )) <nl> + return IMAP_IOERROR ; <nl> + <nl> + return 0 ; <nl> } <nl>  <nl> /* ---------------------------------------------------------------------- */
int mailbox_create ( const char * name , <nl> mailbox -> i . options = options ; <nl> mailbox -> i . highestmodseq = 1 ; <nl>  <nl> + /* initialise header size field so appends calculate the <nl> + * correct map size */ <nl> + mailbox -> index_size = INDEX_HEADER_SIZE ; <nl> + <nl> mailbox -> header_dirty = 1 ; <nl> if (! uniqueid ) { <nl> mailbox_make_uniqueid ( mailbox );
int meth_delete ( struct transaction_t * txn , void * params ) <nl> else if ( r == IMAP_MAILBOX_NONEXISTENT ) ret = HTTP_NOT_FOUND ; <nl> else if ( r ) ret = HTTP_SERVER_ERROR ; <nl>  <nl> - dparams -> davdb . close_db ( davdb ); <nl> - <nl> goto done ; <nl> } <nl> 
EXPORTED int carddav_writecard ( struct carddav_db * carddavdb , struct carddav_data <nl> else if (! strcmp ( name , " x - fm - otheraccount - member ")) { <nl> if ( strncmp ( propval , " urn : uuid :", 9 )) continue ; <nl> struct vparse_param * param = vparse_get_param ( ventry , " userid "); <nl> + if (! param ) continue ; <nl> strarray_append (& member_uids , propval + 9 ); <nl> strarray_append (& member_uids , param -> value ); <nl> }
void clusterCommand ( redisClient * c ) { <nl> addReplyError ( c ," Invalid CLUSTER SETSLOT action or number of arguments "); <nl> return ; <nl> } <nl> + clusterUpdateState (); <nl> clusterSaveConfigOrDie (); <nl> addReply ( c , shared . ok ); <nl> } else if (! strcasecmp ( c -> argv [ 1 ]-> ptr ," info ") && c -> argc == 2 ) {
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> pics = 0 ; <nl> while ( h -> delayed_pic [ pics ]) pics ++; <nl> + <nl> + assert ( pics + 1 < sizeof ( h -> delayed_pic ) / sizeof ( h -> delayed_pic [ 0 ])); <nl> + <nl> h -> delayed_pic [ pics ++] = cur ; <nl> if ( cur -> reference == 0 ) <nl> cur -> reference = 1 ;
static av_cold int rl2_read_header ( AVFormatContext * s ) <nl> rate = avio_rl16 ( pb ); <nl> channels = avio_rl16 ( pb ); <nl> def_sound_size = avio_rl16 ( pb ); <nl> + if (! channels || channels > 42 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels : % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> /** setup video stream */ <nl> st = avformat_new_stream ( s , NULL );
int ff_rtmp_packet_create ( RTMPPacket * pkt , int channel_id , RTMPPacketType type , <nl> int timestamp , int size ) <nl> { <nl> if ( size ) { <nl> - pkt -> data = av_malloc ( size ); <nl> + pkt -> data = av_realloc ( NULL , size ); <nl> if (! pkt -> data ) <nl> return AVERROR ( ENOMEM ); <nl> }
static int xa_read_packet ( AVFormatContext * s , <nl> unsigned int packet_size ; <nl> int ret ; <nl>  <nl> - if ( xa -> sent_bytes > xa -> out_size ) <nl> - return AVERROR ( EIO ); <nl> + if ( xa -> sent_bytes >= xa -> out_size ) <nl> + return AVERROR_EOF ; <nl> /* 1 byte header and 14 bytes worth of samples * number channels per block */ <nl> packet_size = 15 * st -> codec -> channels ; <nl> 
# define XAVS_PART_B8X8 0x100 /* Analyze b16x8 , b */ <nl>  <nl> typedef struct XavsContext { <nl> + AVClass * class ; <nl> xavs_param_t params ; <nl> xavs_t * enc ; <nl> xavs_picture_t pic ;
av_cold int ff_dvvideo_init ( AVCodecContext * avctx ) <nl> /* 248DCT setup */ <nl> s -> fdct [ 1 ] = dsp . fdct248 ; <nl> s -> idct_put [ 1 ] = ff_simple_idct248_put ; // FIXME : need to add it to DSP <nl> - memcpy ( s -> dv_zigzag [ 1 ], ff_dv_zigzag248_direct , 64 ); <nl> + memcpy ( s -> dv_zigzag [ 1 ], ff_dv_zigzag248_direct , sizeof ( s -> dv_zigzag [ 1 ])); <nl>  <nl> s -> avctx = avctx ; <nl> avctx -> chroma_sample_location = AVCHROMA_LOC_TOPLEFT ;
static int process_line ( URLContext * h , char * line , int line_count , <nl> # ifdef DEBUG <nl> printf (" http_code =% d \ n ", s -> http_code ); <nl> # endif <nl> + /* error codes are 4xx and 5xx */ <nl> + if ( s -> http_code >= 400 && s -> http_code < 600 ) <nl> + return - 1 ; <nl> } else { <nl> while (* p != '\ 0 ' && * p != ':') <nl> p ++;
static inline void put_cabac_ueg ( CABACContext * c , uint8_t * state , int v , int ma <nl> } <nl>  <nl> static void refill ( CABACContext * c ){ <nl> - if ( c -> bytestream < c -> bytestream_end ) <nl> + if ( c -> bytestream <= c -> bytestream_end ) <nl> # if CABAC_BITS == 16 <nl> c -> low += (( c -> bytestream [ 0 ]<< 9 ) + ( c -> bytestream [ 1 ])<< 1 ); <nl> # else
static int sql_close ( SQLSOCK * sqlsocket , SQL_CONFIG * config ) { <nl>  <nl> rlm_sql_mysql_sock * mysql_sock = sqlsocket -> conn ; <nl>  <nl> - mysql_close ( mysql_sock -> sock ); <nl> + if ( mysql_sock -> sock ) <nl> + mysql_close ( mysql_sock -> sock ); <nl> mysql_sock -> sock = NULL ; <nl>  <nl> return 0 ;
static int dhcp_process ( REQUEST * request ) <nl> break ; <nl> } <nl>  <nl> + /* <nl> + * Releases don ' t get replies . <nl> + */ <nl> + if ( request -> packet -> code == PW_DHCP_RELEASE ) { <nl> + request -> reply -> code = 0 ; <nl> + } <nl> + <nl> return 1 ; <nl> } <nl> 
static void * mqtt_message_decode_copy_cb ( void * dest , const void * orig , size_t le <nl> const mqtt_message_decode_t * o = ( const mqtt_message_decode_t *) orig ; <nl> mqtt_message_decode_t * d = ( mqtt_message_decode_t *) dest ; <nl>  <nl> + d -> match_criteria = o -> match_criteria ; <nl> d -> topic_pattern = g_strdup ( o -> topic_pattern ); <nl> d -> payload_proto_name = g_strdup ( o -> payload_proto_name ); <nl> 
color_global_cb ( GtkWidget * widget _U_ , gpointer data ) <nl>  <nl> gtk_file_chooser_select_filename ( GTK_FILE_CHOOSER ( fs_widget ), path ); <nl>  <nl> - g_free (( gchar *) path ); <nl> + g_free ( path ); <nl> } <nl>  <nl> /* Import color filters */
file_open_error_message ( int err , gboolean for_writing ) <nl> break ; <nl> # endif <nl>  <nl> + case EINVAL : <nl> + errmsg = " The file \"% s \" could not be created because an invalid filename was specified ."; <nl> + break ; <nl> + <nl> default : <nl> g_snprintf ( errmsg_errno , sizeof ( errmsg_errno ), <nl> " The file \"%% s \" could not be % s : % s .",
epan_init ( void (* register_all_protocols_func )( register_cb cb , gpointer client_da <nl> register_cb cb , <nl> gpointer client_data ) <nl> { <nl> - gboolean status = TRUE ; <nl> + volatile gboolean status = TRUE ; <nl>  <nl> /* initialize memory allocation subsystem */ <nl> wmem_init ();
dissect_esp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> /* Copy back the Authentication which was not encrypted */ <nl> if ( decrypted_len >= esp_auth_len ) <nl> { <nl> - tvb_memcpy ( tvb , decrypted_data + decrypted_len - esp_auth_len , sizeof ( struct newesp )+ decrypted_len - esp_auth_len , esp_auth_len ); <nl> + tvb_memcpy ( tvb , decrypted_data + decrypted_len - esp_auth_len , ( gint )( sizeof ( struct newesp )+ decrypted_len - esp_auth_len ), esp_auth_len ); <nl> } <nl>  <nl> /* Decryption has finished */
PS_READ_FUNC ( files ) <nl> return FAILURE ; <nl>  <nl> data -> st_size = * vallen = sbuf . st_size ; <nl> + <nl> + if ( sbuf . st_size == 0 ) { <nl> + * val = STR_EMPTY_ALLOC (); <nl> + return SUCCESS ; <nl> + } <nl> + <nl> * val = emalloc ( sbuf . st_size ); <nl>  <nl> # if defined ( HAVE_PREAD )
static void _free_mysql_result ( zend_rsrc_list_entry * rsrc TSRMLS_DC ) <nl> MYSQL_RES * mysql_result = ( MYSQL_RES *) rsrc -> ptr ; <nl>  <nl> mysql_free_result ( mysql_result ); <nl> + MySG ( result_allocated )--; <nl> } <nl> /* }}} */ <nl> 
void php_mysqli_fetch_into_hash ( INTERNAL_FUNCTION_PARAMETERS , int override_flags <nl> { <nl> MYSQL_RES * result ; <nl> zval * mysql_result ; <nl> - int fetchtype ; <nl> + long fetchtype ; <nl> unsigned int i ; <nl> MYSQL_FIELD * fields ; <nl> MYSQL_ROW row ;
flatpak_installation_drop_caches ( FlatpakInstallation * self , <nl> { <nl> priv -> dir_unlocked = clone ; <nl> g_object_unref ( old ); <nl> + res = TRUE ; <nl> } <nl>  <nl> G_UNLOCK ( dir );
flatpak_run_add_environment_args ( GPtrArray * argv_array , <nl> "/ dev / dri ", <nl> /* mali */ <nl> "/ dev / mali ", <nl> + "/ dev / mali0 ", <nl> "/ dev / umplock ", <nl> /* nvidia */ <nl> "/ dev / nvidiactl ",
int switch_root ( const char * new_root ) { <nl>  <nl> if ( fstat ( old_root_fd , & rb ) < 0 ) <nl> log_warning (" Failed to stat old root directory , leaving : % m "); <nl> - else <nl> + else { <nl> rm_rf_children ( old_root_fd , false , false , & rb ); <nl> + old_root_fd = - 1 ; <nl> + } <nl> } <nl>  <nl> r = 0 ;
static void output_units_list ( const struct unit_info * unit_infos , unsigned c ) { <nl>  <nl> n_shown ++; <nl>  <nl> - if ( streq ( u -> load_state , " error ")) { <nl> + if ( streq ( u -> load_state , " error ") || <nl> + streq ( u -> load_state , " not - found ")) { <nl> on_loaded = on = ansi_highlight_red ( true ); <nl> off_loaded = off = ansi_highlight_red ( false ); <nl> } else
subst : <nl> cpos ++; <nl> while ( isspace ( cpos [ 0 ])) <nl> cpos ++; <nl> + if ( cpos [ 0 ] == '\ 0 ') <nl> + break ; <nl> } <nl> if ( i > 0 ) { <nl> log_error (" requested part of result string not found ");
static int builtin_kmod ( struct udev_device * dev , int argc , char * argv [], bool te <nl> struct udev * udev = udev_device_get_udev ( dev ); <nl> int i ; <nl>  <nl> - if ( ctx ) <nl> + if (! ctx ) <nl> return 0 ; <nl>  <nl> if ( argc < 3 || strcmp ( argv [ 1 ], " load ")) {
static int parse_password ( const char * filename , char ** wall ) { <nl> } <nl> } <nl>  <nl> + if ( pid > 0 && <nl> + kill ( pid , 0 ) < 0 && <nl> + errno == ESRCH ) { <nl> + r = 0 ; <nl> + goto finish ; <nl> + } <nl> + <nl> if ( arg_action == ACTION_LIST ) <nl> printf ("'% s ' ( PID % u )\ n ", message , pid ); <nl> else if ( arg_action == ACTION_WALL ) {
int sd_rtnl_call ( sd_rtnl * rtnl , <nl> r = rtnl_poll ( rtnl , true , left ); <nl> if ( r < 0 ) <nl> return r ; <nl> + else if ( r == 0 ) <nl> + return - ETIMEDOUT ; <nl>  <nl> r = dispatch_wqueue ( rtnl ); <nl> if ( r < 0 )
int selinux_setup ( bool * loaded_policy ) { <nl> * loaded_policy = true ; <nl>  <nl> } else { <nl> + log_open (); <nl> + <nl> if ( enforce > 0 ) { <nl> - log_error (" Failed to load SELinux policy ."); <nl> + log_error (" Failed to load SELinux policy . Freezing ."); <nl> return - EIO ; <nl> } else <nl> - log_debug (" Unable to load SELinux policy ."); <nl> + log_debug (" Unable to load SELinux policy . Ignoring ."); <nl> } <nl> # endif <nl> 
static int parse_request ( uint8_t code , uint8_t len , const void * option , void * us <nl>  <nl> break ; <nl> case SD_DHCP_OPTION_MAXIMUM_MESSAGE_SIZE : <nl> - if ( len == 2 ) <nl> + <nl> + if ( len == 2 && unaligned_read_be16 ( option ) >= sizeof ( DHCPPacket )) <nl> req -> max_optlen = unaligned_read_be16 ( option ) - sizeof ( DHCPPacket ); <nl>  <nl> break ;
gss_pseudo_random ( OM_uint32 * minor_status , <nl> gss_buffer_t prf_out ) <nl> { <nl> struct _gss_context * ctx = ( struct _gss_context *) context ; <nl> - gssapi_mech_interface m = ctx -> gc_mech ; <nl> + gssapi_mech_interface m ; <nl> OM_uint32 major_status ; <nl>  <nl> _mg_buffer_zero ( prf_out ); <nl> gss_pseudo_random ( OM_uint32 * minor_status , <nl> return GSS_S_NO_CONTEXT ; <nl> } <nl>  <nl> + m = ctx -> gc_mech ; <nl> + <nl> if ( m -> gm_pseudo_random == NULL ) <nl> return GSS_S_UNAVAILABLE ; <nl> 
main ( int argc , char ** argv ) <nl>  <nl> setprogname ( argv [ 0 ]); <nl>  <nl> + setlocale ( LC_ALL , ""); <nl> + bindtextdomain (" heimdal_kuser ", HEIMDAL_LOCALEDIR ); <nl> + textdomain (" heimdal_kuser "); <nl> + <nl> ret = krb5_init_context (& context ); <nl> if ( ret == KRB5_CONFIG_BADFORMAT ) <nl> errx ( 1 , " krb5_init_context failed to parse configuration file ");
void shadow_client_refresh_rect ( rdpShadowClient * client , BYTE count , RECTANGLE_1 <nl> wParam = ( SHADOW_MSG_IN_REFRESH_OUTPUT *) calloc ( 1 , sizeof ( SHADOW_MSG_IN_REFRESH_OUTPUT )); <nl>  <nl> if (! wParam || ! areas ) <nl> + { <nl> + if ( wParam ) <nl> + free ( wParam ); <nl> return ; <nl> + } <nl>  <nl> wParam -> numRects = ( UINT32 ) count ; <nl> 
static CACHE_BITMAP_V3_ORDER * update_read_cache_bitmap_v3_order ( rdpUpdate * updat <nl> Stream_Read_UINT16 ( s , bitmapData -> height ); /* height ( 2 bytes ) */ <nl> Stream_Read_UINT32 ( s , new_len ); /* length ( 4 bytes ) */ <nl>  <nl> - if ( Stream_GetRemainingLength ( s ) < new_len ) <nl> + if (( new_len == 0 ) || ( Stream_GetRemainingLength ( s ) < new_len )) <nl> goto fail ; <nl>  <nl> new_data = ( BYTE *) realloc ( bitmapData -> data , new_len );
boolean fastpath_send_update_pdu ( rdpFastPath * fastpath , uint8 updateCode , STREAM <nl> try_comp = rdp -> settings -> compression ; <nl> comp_update = stream_new ( 0 ); <nl>  <nl> - for ( fragment = 0 ; totalLength > 0 ; fragment ++) <nl> + for ( fragment = 0 ; totalLength > 0 || fragment == 0 ; fragment ++) <nl> { <nl> stream_get_mark ( s , holdp ); <nl> ls = s ;
static UINT drive_process_irp_query_directory ( DRIVE_DEVICE * drive , IRP * irp ) <nl> Stream_Read_UINT32 ( irp -> input , PathLength ); <nl> Stream_Seek ( irp -> input , 23 ); /* Padding */ <nl> path = ( WCHAR *) Stream_Pointer ( irp -> input ); <nl> + if (! Stream_CheckAndLogRequiredLength ( TAG , irp -> input , PathLength )) <nl> + return ERROR_INVALID_DATA ; <nl> + <nl> file = drive_get_file_by_id ( drive , irp -> FileId ); <nl>  <nl> if ( file == NULL )
int freerdp_assistance_parse_file ( rdpAssistanceFile * file , const char * name ) <nl> FILE * fp = NULL ; <nl> size_t readSize ; <nl> INT64 fileSize ; <nl> + <nl> + if (! name ) <nl> + return - 1 ; <nl> + <nl> fp = fopen ( name , " r "); <nl>  <nl> if (! fp )
INT32 progressive_decompress ( PROGRESSIVE_CONTEXT * progressive , <nl>  <nl> if ( progressive -> cTiles < surface -> gridSize ) <nl> { <nl> - progressive -> tiles = ( RFX_PROGRESSIVE_TILE **) realloc ( progressive -> tiles , <nl> + BYTE * tmpBuf = ( RFX_PROGRESSIVE_TILE **) realloc ( progressive -> tiles , <nl> surface -> gridSize * sizeof ( RFX_PROGRESSIVE_TILE *)); <nl> + if (! tmpBuf ) <nl> + return - 1025 ; <nl> + <nl> + progressive -> tiles = tmpBuf ; <nl> progressive -> cTiles = surface -> gridSize ; <nl> } <nl> 
BOOL glyph_cache_put ( rdpGlyphCache * glyphCache , UINT32 id , UINT32 index , rdpGlyp <nl> return FALSE ; <nl> } <nl>  <nl> - if ( index > glyphCache -> glyphCache [ id ]. number ) <nl> + if ( index >= glyphCache -> glyphCache [ id ]. number ) <nl> { <nl> WLog_ERR ( TAG , " invalid glyph cache index : %" PRIu32 " in cache id : %" PRIu32 "", index , id ); <nl> return FALSE ;
int ff_mov_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> else <nl> samples_in_chunk = 1 ; <nl>  <nl> + if ( samples_in_chunk < 1 ) { <nl> + av_log ( s , AV_LOG_ERROR , " fatal error , input packet contains no samples \ n "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> + <nl> /* copy extradata if it exists */ <nl> if ( trk -> vos_len == 0 && par -> extradata_size > 0 && <nl> ! TAG_IS_AVCI ( trk -> tag ) &&
static int decode_pic_hdr ( IVI5DecContext * ctx , AVCodecContext * avctx ) <nl> ctx -> gop_invalid = 0 ; <nl> } <nl>  <nl> + if ( ctx -> frame_type == FRAMETYPE_INTER_SCAL && ! ctx -> is_scalable ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Scalable inter frame in non scaleable stream \ n "); <nl> + ctx -> frame_type = FRAMETYPE_INTER ; <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( ctx -> frame_type != FRAMETYPE_NULL ) { <nl> ctx -> frame_flags = get_bits (& ctx -> gb , 8 ); <nl> 
int avcodec_copy_context ( AVCodecContext * dest , const AVCodecContext * src ) <nl> memcpy ( dest , src , sizeof (* dest )); <nl>  <nl> dest -> priv_data = orig_priv_data ; <nl> + <nl> + if ( orig_priv_data ) <nl> + av_opt_copy ( orig_priv_data , src -> priv_data ); <nl> + <nl> dest -> codec = orig_codec ; <nl>  <nl> /* set values specific to opened codecs back to their default state */
static int ea_read_header ( AVFormatContext * s ) <nl> ea -> audio_codec = 0 ; <nl> return 1 ; <nl> } <nl> - if ( ea -> bytes <= 0 ) { <nl> + if ( ea -> bytes <= 0 || ea -> bytes > 2 ) { <nl> av_log ( s , AV_LOG_ERROR , <nl> " Invalid number of bytes per sample : % d \ n ", ea -> bytes ); <nl> ea -> audio_codec = AV_CODEC_ID_NONE ;
static int process_audio_header_elements ( AVFormatContext * s ) <nl> } <nl>  <nl> switch ( compression_type ) { <nl> + case 0 : ea -> audio_codec = CODEC_ID_PCM_S16LE ; break ; <nl> case 7 : ea -> audio_codec = CODEC_ID_ADPCM_EA ; break ; <nl> default : <nl> av_log ( s , AV_LOG_ERROR , " unsupported stream type ; compression_type =% i \ n ", compression_type );
struct dshow_ctx { <nl> HANDLE event ; <nl> AVPacketList * pktl ; <nl>  <nl> - unsigned int curbufsize ; <nl> + int64_t curbufsize ; <nl> unsigned int video_frame_num ; <nl>  <nl> IMediaControl * control ;
static int decode_frame ( AVCodecContext * avctx , const uint8_t * databuf , <nl>  <nl>  <nl> /* set the bitstream reader at the start of the second Sound Unit */ <nl> - init_get_bits8 (& q -> gb , <nl> + ret = init_get_bits8 (& q -> gb , <nl> ptr1 , q -> decoded_bytes_buffer + js_block_align - ptr1 ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> /* Fill the Weighting coeffs delay buffer */ <nl> memmove ( q -> weighting_delay [ js_pair ], & q -> weighting_delay [ js_pair ][ 2 ],
static int rm_write_audio ( AVFormatContext * s , const uint8_t * buf , int size , int <nl>  <nl> /* XXX : suppress this malloc */ <nl> buf1 = av_malloc ( size * sizeof ( uint8_t )); <nl> + if (! buf1 ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> write_packet_header ( s , stream , size , !!( flags & AV_PKT_FLAG_KEY )); <nl> 
int ff_rtmp_packet_create ( RTMPPacket * pkt , int channel_id , RTMPPacketType type , <nl> int timestamp , int size ) <nl> { <nl> if ( size ) { <nl> - pkt -> data = av_malloc ( size ); <nl> + pkt -> data = av_realloc ( NULL , size ); <nl> if (! pkt -> data ) <nl> return AVERROR ( ENOMEM ); <nl> }
static int tscc2_decode_mb ( TSCC2Context * c , int * q , int vlc_set , <nl> if ( ac == 0x1000 ) <nl> ac = get_bits ( gb , 12 ); <nl> bpos += ac & 0xF ; <nl> - if ( bpos >= 64 ) <nl> + if ( bpos >= 16 ) <nl> return AVERROR_INVALIDDATA ; <nl> val = sign_extend ( ac >> 4 , 8 ); <nl> c -> block [ tscc2_zigzag [ bpos ++]] = val ;
static int cine_read_header ( AVFormatContext * avctx ) <nl>  <nl> /* parse image offsets */ <nl> avio_seek ( pb , offImageOffsets , SEEK_SET ); <nl> - for ( i = 0 ; i < st -> duration ; i ++) <nl> + for ( i = 0 ; i < st -> duration ; i ++) { <nl> + if ( avio_feof ( pb )) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> av_add_index_entry ( st , avio_rl64 ( pb ), i , 0 , 0 , AVINDEX_KEYFRAME ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static int add_candidate_ref ( HEVCContext * s , RefPicList * list , <nl> { <nl> HEVCFrame * ref = find_ref_idx ( s , poc ); <nl>  <nl> - if ( ref == s -> ref ) <nl> + if ( ref == s -> ref || list -> nb_refs >= HEVC_MAX_REFS ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if (! ref ) {
reload : <nl>  <nl> return ret ; <nl> } <nl> - if ( c -> http_persistent ) { <nl> + if ( c -> http_persistent && av_strstart ( seg -> url , " http ", NULL )) { <nl> v -> input_read_done = 1 ; <nl> } else { <nl> ff_format_io_close ( v -> parent , & v -> input );
int Configure ( void ** ctxp , int argc , char * argv []) <nl> if ( argc > 1 ) <nl> { <nl> * ctxp = av_mallocz ( sizeof ( ContextInfo )); <nl> - if ( ctxp != NULL && argc > 1 ) <nl> + if ( * ctxp != NULL && argc > 1 ) <nl> { <nl> ContextInfo * info = ( ContextInfo *)* ctxp ; <nl> info -> rw = rwpipe_open ( argc - 1 , & argv [ 1 ] );
static int rv10_decode_frame ( AVCodecContext * avctx , <nl> offset + FFMAX ( size , size2 ) > buf_size ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> - if ( rv10_decode_packet ( avctx , buf + offset , size , size2 ) > 8 * size ) <nl> + if (( ret = rv10_decode_packet ( avctx , buf + offset , size , size2 )) < 0 ) <nl> + return ret ; <nl> + <nl> + if ( ret > 8 * size ) <nl> i ++; <nl> } <nl> 
FF_ENABLE_DEPRECATION_WARNINGS <nl> return 0 ; <nl>  <nl> failed_alloc : <nl> - av_destruct_packet ( pkt ); <nl> + av_free_packet ( pkt ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl>  <nl> int av_copy_packet_side_data ( AVPacket * pkt , AVPacket * src ) <nl> return 0 ; <nl>  <nl> failed_alloc : <nl> - av_destruct_packet ( pkt ); <nl> + av_free_packet ( pkt ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> 
pp_mode * pp_get_mode_by_name_and_quality ( const char * name , int quality ) <nl> if ( filterToken == NULL ) break ; <nl> p += strlen ( filterToken ) + 1 ; // p points to next filterToken <nl> filterName = strtok ( filterToken , optionDelimiters ); <nl> + if ( filterName == NULL ) { <nl> + ppMode -> error ++; <nl> + break ; <nl> + } <nl> av_log ( NULL , AV_LOG_DEBUG , " pp : % s ::% s \ n ", filterToken , filterName ); <nl>  <nl> if (* filterName == '-'){
static int filter_packet ( void * log_ctx , AVPacket * pkt , <nl> AVFormatContext * fmt_ctx , AVBitStreamFilterContext * bsf_ctx ) <nl> { <nl> AVCodecContext * enc_ctx = fmt_ctx -> streams [ pkt -> stream_index ]-> codec ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> while ( bsf_ctx ) { <nl> AVPacket new_pkt = * pkt ;
static int gxf_write_header ( AVFormatContext * s ) <nl> if ( ff_audio_interleave_init ( s , GXF_samples_per_frame , ( AVRational ){ 1 , 48000 }) < 0 ) <nl> return - 1 ; <nl>  <nl> - if ( tcr ) <nl> + if ( tcr && vsc ) <nl> gxf_init_timecode ( s , & gxf -> tc , tcr -> value , vsc -> fields ); <nl>  <nl> gxf_init_timecode_track (& gxf -> timecode_track , vsc );
static int encode_block ( SVQ1Context * s , uint8_t * src , uint8_t * ref , uint8_t * dec <nl> } <nl>  <nl> best_count = 0 ; <nl> - best_score -= (( block_sum [ 0 ]* block_sum [ 0 ])>>( level + 3 )); <nl> + best_score -= ( int )((( unsigned ) block_sum [ 0 ]* block_sum [ 0 ])>>( level + 3 )); <nl> best_mean = ( block_sum [ 0 ] + ( size >> 1 )) >> ( level + 3 ); <nl>  <nl> if ( level < 4 ){
static int vorbis_parse ( AVCodecParserContext * s1 , AVCodecContext * avctx , <nl>  <nl> if (! s -> vp && avctx -> extradata && avctx -> extradata_size ) { <nl> s -> vp = av_vorbis_parse_init ( avctx -> extradata , avctx -> extradata_size ); <nl> - if (! s -> vp ) <nl> - goto end ; <nl> } <nl> + if (! s -> vp ) <nl> + goto end ; <nl>  <nl> if (( duration = av_vorbis_parse_frame ( s -> vp , buf , buf_size )) >= 0 ) <nl> s1 -> duration = duration ;
static av_always_inline void pred_8x16_motion ( const H264Context * const h , <nl> if ( IS_INTERLACED ( type )) { \ <nl> refn >>= 1 ; \ <nl> AV_COPY32 ( mvbuf [ idx ], mvn ); \ <nl> - mvbuf [ idx ][ 1 ] <<= 1 ; \ <nl> + mvbuf [ idx ][ 1 ] *= 2 ; \ <nl> mvn = mvbuf [ idx ]; \ <nl> } \ <nl> } \
static void start_children ( FFServerStream * feed ) <nl> av_free ( pathname ); <nl> _exit ( 1 ); <nl> } <nl> + av_free ( pathname ); <nl> } <nl>  <nl> /* open a listening socket */
static inline int l3_unscale ( int value , int exponent ) <nl> if ( e < 1 ) <nl> av_log ( NULL , AV_LOG_WARNING , " l3_unscale : e is % d \ n ", e ); <nl> # endif <nl> - if ( e > 31 ) <nl> + if ( e > ( SUINT ) 31 ) <nl> return 0 ; <nl> m = ( m + ( 1 << ( e - 1 ))) >> e ; <nl> 
static av_always_inline void RENAME ( decode_line )( FFV1Context * s , int w , <nl> } <nl>  <nl> if ( sign ) <nl> - diff = - diff ; <nl> + diff = -( unsigned ) diff ; <nl>  <nl> sample [ 1 ][ x ] = av_mod_uintp2 ( RENAME ( predict )( sample [ 1 ] + x , sample [ 0 ] + x ) + ( SUINT ) diff , bits ); <nl> }
int ff_read_riff_info ( AVFormatContext * s , int64_t size ) <nl> AV_WL32 ( key , chunk_code ); <nl>  <nl> if ( avio_read ( pb , value , chunk_size ) != chunk_size ) { <nl> - av_freep ( key ); <nl> - av_freep ( value ); <nl> + av_free ( value ); <nl> av_log ( s , AV_LOG_ERROR , " premature end of file while reading INFO tag \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
resync : <nl> /* for AC3 , needs to swap bytes */ <nl> if ( st -> codec -> codec_id == CODEC_ID_AC3 ) { <nl> ptr = pkt -> data ; <nl> - for ( j = 0 ; j < len ; j += 2 ) { <nl> + for ( j = 0 ; j < pkt -> size ; j += 2 ) { <nl> FFSWAP ( int , ptr [ 0 ], ptr [ 1 ]); <nl> ptr += 2 ; <nl> }
static int ea_read_header ( AVFormatContext * s , <nl> ea -> audio_codec = 0 ; <nl> return 1 ; <nl> } <nl> + if ( ea -> bytes <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of bytes per sample : % d \ n ", ea -> bytes ); <nl> + ea -> audio_codec = CODEC_ID_NONE ; <nl> + return 1 ; <nl> + } <nl>  <nl> /* initialize the audio decoder stream */ <nl> st = avformat_new_stream ( s , NULL );
static av_cold int dirac_decode_end ( AVCodecContext * avctx ) <nl> static inline int coeff_unpack_golomb ( GetBitContext * gb , int qfactor , int qoffset ) <nl> { <nl> int coeff = dirac_get_se_golomb ( gb ); <nl> - const int sign = FFSIGN ( coeff ); <nl> + const unsigned sign = FFSIGN ( coeff ); <nl> if ( coeff ) <nl> coeff = sign *(( sign * coeff * qfactor + qoffset ) >> 2 ); <nl> return coeff ;
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl>  <nl> /* first slice */ <nl> if ( si . start == 0 ) { <nl> - if ( s -> mb_num_left > 0 ) { <nl> + if ( s -> mb_num_left > 0 && s -> current_picture_ptr ) { <nl> av_log ( avctx , AV_LOG_ERROR , " New frame but still % d MB left .\ n ", <nl> s -> mb_num_left ); <nl> ff_er_frame_end (& s -> er );
enum OutputFormat { <nl> # define MPEG_BUF_SIZE ( 16 * 1024 ) <nl>  <nl> # define QMAT_SHIFT_MMX 16 <nl> -# define QMAT_SHIFT 22 <nl> +# define QMAT_SHIFT 21 <nl>  <nl> # define MAX_FCODE 7 <nl> # define MAX_MV 2048
static int init ( AVFilterContext * ctx , const char * args ) <nl> eval -> class = & aevalsrc_class ; <nl> av_opt_set_defaults ( eval ); <nl>  <nl> + if (! args1 ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Argument is empty \ n "); <nl> + ret = args ? AVERROR ( ENOMEM ) : AVERROR ( EINVAL ); <nl> + goto end ; <nl> + } <nl> + <nl> /* parse expressions */ <nl> buf = args1 ; <nl> i = 0 ;
static inline void tm2_apply_deltas ( TM2Context * ctx , int * Y , int stride , int * de <nl> } <nl> } <nl>  <nl> - static inline void tm2_high_chroma ( int * data , int stride , int * last , int * CD , int * deltas ) <nl> + static inline void tm2_high_chroma ( int * data , int stride , int * last , unsigned * CD , int * deltas ) <nl> { <nl> int i , j ; <nl> for ( j = 0 ; j < 2 ; j ++) {
static inline uint64_t get_bits64 ( GetBitContext * s , int n ) <nl> */ <nl> static inline int get_sbits_long ( GetBitContext * s , int n ) <nl> { <nl> + // sign_extend ( x , 0 ) is undefined <nl> + if (! n ) <nl> + return 0 ; <nl> + <nl> return sign_extend ( get_bits_long ( s , n ), n ); <nl> } <nl> 
void ff_slice_thread_free ( AVCodecContext * avctx ) <nl> pthread_mutex_destroy (& c -> current_job_lock ); <nl> pthread_cond_destroy (& c -> current_job_cond ); <nl> pthread_cond_destroy (& c -> last_job_cond ); <nl> - av_free ( c -> workers ); <nl> + av_freep (& c -> workers ); <nl> av_freep (& avctx -> internal -> thread_ctx ); <nl> } <nl> 
static int rtmp_open ( URLContext * s , const char * uri , int flags ) <nl> } <nl> if ( ctx -> swfurl ) { <nl> av_strlcat ( filename , " swfUrl =", len ); <nl> - av_strlcat ( filename , ctx -> pageurl , len ); <nl> + av_strlcat ( filename , ctx -> swfurl , len ); <nl> } <nl> if ( ctx -> flashver ) { <nl> av_strlcat ( filename , " flashVer =", len );
typedef struct { <nl> static const AVOption amerge_options [] = { <nl> { " inputs ", " specify the number of inputs ", OFFSET ( nb_inputs ), <nl> AV_OPT_TYPE_INT , { . dbl = 2 }, 2 , SWR_CH_MAX }, <nl> + { 0 } <nl> }; <nl>  <nl> static const AVClass amerge_class = {
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> uint8_t mask2 = -( esc_count < 3 ); <nl> uint8_t * end = dst + ( width - 2 ); <nl>  <nl> + avpriv_request_sample ( l -> avctx , " zero_run_line "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + <nl> output_zeros : <nl> if ( l -> zeros_rem ) { <nl> count = FFMIN ( l -> zeros_rem , width - i );
static int stream_component_open ( VideoState * is , int stream_index ) <nl> goto fail ; <nl> link = is -> out_audio_filter -> inputs [ 0 ]; <nl> sample_rate = link -> sample_rate ; <nl> - nb_channels = link -> channels ; <nl> + nb_channels = avfilter_link_get_channels ( link ); <nl> channel_layout = link -> channel_layout ; <nl> } <nl> # else
static int rtp_write ( URLContext * h , const uint8_t * buf , int size ) <nl> int ret ; <nl> URLContext * hd ; <nl>  <nl> + if ( size < 2 ) <nl> + return AVERROR ( EINVAL ); <nl> + <nl> if ( RTP_PT_IS_RTCP ( buf [ 1 ])) { <nl> /* RTCP payload type */ <nl> hd = s -> rtcp_hd ;
static int alac_set_info ( ALACContext * alac ) <nl>  <nl> alac -> max_samples_per_frame = bytestream2_get_be32u (& gb ); <nl> if (! alac -> max_samples_per_frame || <nl> - alac -> max_samples_per_frame > INT_MAX / sizeof ( int32_t )) { <nl> + alac -> max_samples_per_frame > 4096 * 4096 ) { <nl> av_log ( alac -> avctx , AV_LOG_ERROR , <nl> " max samples per frame invalid : %" PRIu32 "\ n ", <nl> alac -> max_samples_per_frame );
static int compute_mask ( int step , uint32_t * mask ) <nl> ret = AVERROR ( ENOMEM ); <nl> goto end ; <nl> } <nl> - counter = av_mallocz ( counter_size ); <nl> + counter = av_mallocz ( sizeof ( uint32_t *) * ( 2 * step + 1 )); <nl> if (! counter ) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto end ;
static int decode_slice ( struct AVCodecContext * avctx , void * arg ) <nl> return 0 ; <nl> } else { <nl> ff_er_add_slice ( s , s -> resync_mb_x , s -> resync_mb_y , <nl> - s -> mb_x , s -> mb_y , <nl> + s -> mb_x - 1 , s -> mb_y , <nl> ER_MB_END & part_mask ); <nl>  <nl> return - 1 ;
static int read_access_unit ( AVCodecContext * avctx , void * data , <nl> int ret ; <nl>  <nl> if ( buf_size < 4 ) <nl> - return 0 ; <nl> + return AVERROR_INVALIDDATA ; <nl>  <nl> length = ( AV_RB16 ( buf ) & 0xfff ) * 2 ; <nl> 
static int mpegps_read_packet ( AVFormatContext * s , <nl> MpegDemuxContext * m = s -> priv_data ; <nl> AVStream * st ; <nl> int len , startcode , i , es_type , ret ; <nl> - int lpcm_header_len ; <nl> + int lpcm_header_len = - 1 ; // Init to supress warning <nl> int request_probe = 0 ; <nl> enum AVCodecID codec_id = AV_CODEC_ID_NONE ; <nl> enum AVMediaType type ;
FF_ENABLE_DEPRECATION_WARNINGS <nl>  <nl> if ( mxg -> soi_ptr - mxg -> buffer > mxg -> cache_size ) { <nl> if ( mxg -> cache_size > 0 ) { <nl> - memcpy ( mxg -> buffer , mxg -> buffer_ptr , mxg -> cache_size ); <nl> + memmove ( mxg -> buffer , mxg -> buffer_ptr , mxg -> cache_size ); <nl> } <nl>  <nl> mxg -> buffer_ptr = mxg -> buffer ;
int av_cold ff_ivi_init_tiles ( IVIPlaneDesc * planes , int tile_width , int tile_hei <nl> t_width >>= 1 ; <nl> t_height >>= 1 ; <nl> } <nl> + if ( t_width <= 0 || t_height <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> for ( b = 0 ; b < planes [ p ]. num_bands ; b ++) { <nl> band = & planes [ p ]. bands [ b ];
cl_program av_opencl_compile ( const char * program_name , const char * build_opts ) <nl> int i ; <nl> cl_int status , build_status ; <nl> int kernel_code_idx = 0 ; <nl> - const char * kernel_source ; <nl> + const char * kernel_source = NULL ; <nl> size_t kernel_code_len ; <nl> char * ptr = NULL ; <nl> cl_program program = NULL ;
static int xvid_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> xvid_enc_frame_t xvid_enc_frame = { 0 }; <nl> xvid_enc_stats_t xvid_enc_stats = { 0 }; <nl>  <nl> - if (( ret = ff_alloc_packet2 ( avctx , pkt , mb_width * mb_height * MAX_MB_BYTES + FF_MIN_BUFFER_SIZE )) < 0 ) <nl> + if (( ret = ff_alloc_packet2 ( avctx , pkt , mb_width *( int64_t ) mb_height * MAX_MB_BYTES + FF_MIN_BUFFER_SIZE )) < 0 ) <nl> return ret ; <nl>  <nl> /* Start setting up the frame */
static void vp56_decode_mb ( VP56Context * s , int row , int col , int is_alpha ) <nl>  <nl> frame_current = s -> framep [ VP56_FRAME_CURRENT ]; <nl> frame_ref = s -> framep [ ref_frame ]; <nl> + if ( mb_type != VP56_MB_INTRA && ! frame_ref -> data [ 0 ]) <nl> + return ; <nl>  <nl> ab = 6 * is_alpha ; <nl> b_max = 6 - 2 * is_alpha ;
recover : <nl> pes_flags = avio_rb16 ( pb ); <nl> pes_header_data_length = avio_r8 ( pb ); <nl>  <nl> + if ( avio_feof ( pb )) { <nl> + return AVERROR_EOF ; <nl> + } <nl> + <nl> if ( pes_signal != 1 || pes_header_data_length == 0 ) { <nl> pva_log ( s , AV_LOG_WARNING , " expected non empty signaled PES packet , " <nl> " trying to recover \ n ");
static int adpcm_decode_frame ( AVCodecContext * avctx , void * data , <nl> /* Each EA ADPCM frame has a 12 - byte header followed by 30 - byte pieces , <nl> each coding 28 stereo samples . */ <nl>  <nl> + if ( avctx -> channels != 2 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> src += 4 ; // skip sample count ( already read ) <nl>  <nl> current_left_sample = ( int16_t ) bytestream_get_le16 (& src );
typedef struct Context { <nl>  <nl> static int cmp ( const void * key , const void * node ) <nl> { <nl> - return (*( const int64_t *) key ) - (( const CacheEntry *) node )-> logical_pos ; <nl> + return FFDIFFSIGN (*( const int64_t *) key , (( const CacheEntry *) node )-> logical_pos ); <nl> } <nl>  <nl> static int cache_open ( URLContext * h , const char * arg , int flags , AVDictionary ** options )
void ff_rtp_send_h263 ( AVFormatContext * s1 , const uint8_t * buf1 , int size ) <nl>  <nl> while ( size > 0 ) { <nl> q = s -> buf ; <nl> - if (( buf1 [ 0 ] == 0 ) && ( buf1 [ 1 ] == 0 )) { <nl> + if ( size >= 2 && ( buf1 [ 0 ] == 0 ) && ( buf1 [ 1 ] == 0 )) { <nl> * q ++ = 0x04 ; <nl> buf1 += 2 ; <nl> size -= 2 ;
static void qdm2_fft_decode_tones ( QDM2Context * q , int duration , GetBitContext * <nl> return ; <nl>  <nl> local_int_14 = ( offset >> local_int_8 ); <nl> + if ( local_int_14 >= FF_ARRAY_ELEMS ( fft_level_index_table )) <nl> + return ; <nl>  <nl> if ( q -> nb_channels > 1 ) { <nl> channel = get_bits1 ( gb );
static av_cold void RENAME ( sws_init_swScale )( SwsContext * c ) <nl> enum PixelFormat srcFormat = c -> srcFormat , <nl> dstFormat = c -> dstFormat ; <nl>  <nl> - if (! is16BPS ( dstFormat ) && ! is9_OR_10BPS ( dstFormat )) { <nl> + if (! is16BPS ( dstFormat ) && ! is9_OR_10BPS ( dstFormat ) && <nl> + dstFormat != PIX_FMT_NV12 && dstFormat != PIX_FMT_NV21 ) { <nl> if (!( c -> flags & SWS_BITEXACT )) { <nl> if ( c -> flags & SWS_ACCURATE_RND ) { <nl> c -> yuv2yuv1 = RENAME ( yuv2yuv1_ar );
static void search_for_ms_mips ( AACEncContext * s , ChannelElement * cpe ) <nl> # endif /* HAVE_INLINE_ASM */ <nl>  <nl> void ff_aac_coder_init_mips ( AACEncContext * c ) { <nl> -# if HAVE_INLINE_ASM <nl> +# if 0 // HAVE_INLINE_ASM <nl> AACCoefficientsEncoder * e = c -> coder ; <nl> int option = c -> options . aac_coder ; <nl> 
int av_find_stream_info ( AVFormatContext * ic ) <nl> st -> codec -> frame_size = 0 ; <nl> st -> codec -> channels = 0 ; <nl> } <nl> - if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO ){ <nl> + if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO || <nl> + st -> codec -> codec_type == AVMEDIA_TYPE_SUBTITLE ) { <nl> /* if (! st -> time_base . num ) <nl> st -> time_base = */ <nl> if (! st -> codec -> time_base . num )
static int gxf_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> gxf -> flt_entries_nb + 500 , <nl> sizeof (* gxf -> flt_entries ))) < 0 ) { <nl> gxf -> flt_entries_nb = 0 ; <nl> + gxf -> nb_fields = 0 ; <nl> av_log ( s , AV_LOG_ERROR , " could not reallocate flt entries \ n "); <nl> return err ; <nl> }
AVFilterFormats * avfilter_merge_formats ( AVFilterFormats * a , AVFilterFormats * b ) <nl> AVFilterFormats * ret ; <nl> unsigned i , j , k = 0 ; <nl>  <nl> + if ( a == b ) return a ; <nl> + <nl> ret = av_mallocz ( sizeof ( AVFilterFormats )); <nl>  <nl> /* merge list of formats */
# include < stddef . h > <nl> # include < stdint . h > <nl>  <nl> - typedef int16_t dwtcoef ; <nl> + typedef int32_t dwtcoef ; <nl>  <nl> enum VC2TransformType { <nl> VC2_TRANSFORM_9_7 = 0 , /* Deslauriers - Dubuc ( 9 , 7 ) */
av_cold void ff_cavsdsp_init_x86 ( CAVSDSPContext * c , AVCodecContext * avctx ) <nl> { <nl> av_unused int cpu_flags = av_get_cpu_flags (); <nl>  <nl> - cavsdsp_init_mmx ( c , avctx ); <nl> + if ( X86_MMX ( cpu_flags )) <nl> + cavsdsp_init_mmx ( c , avctx ); <nl> + <nl> # if HAVE_AMD3DNOW_INLINE <nl> if ( INLINE_AMD3DNOW ( cpu_flags )) <nl> cavsdsp_init_3dnow ( c , avctx );
void ff_ivi_process_empty_tile ( AVCodecContext * avctx , IVIBandDesc * band , <nl> if ( band -> inherit_qdelta && ref_mb ) <nl> mb -> q_delta = ref_mb -> q_delta ; <nl>  <nl> - if ( band -> inherit_mv ) { <nl> + if ( band -> inherit_mv && ref_mb ) { <nl> /* motion vector inheritance */ <nl> if ( mv_scale ) { <nl> mb -> mv_x = ivi_scale_mv ( ref_mb -> mv_x , mv_scale );
static int read_filter_params ( MLPDecodeContext * m , GetBitContext * gbp , <nl> /* TODO : Check validity of state data . */ <nl>  <nl> for ( i = 0 ; i < order ; i ++) <nl> - fp -> state [ i ] = state_bits ? get_sbits ( gbp , state_bits ) << state_shift : 0 ; <nl> + fp -> state [ i ] = state_bits ? get_sbits ( gbp , state_bits ) * ( 1 << state_shift ) : 0 ; <nl> } <nl> } <nl> 
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> } <nl> st -> info -> duration_count ++; <nl> // ignore the first 4 values , they might have some random jitter <nl> - if ( st -> info -> duration_count > 3 ) <nl> + if ( st -> info -> duration_count > 3 && is_relative ( pkt -> dts ) == is_relative ( last )) <nl> st -> info -> duration_gcd = av_gcd ( st -> info -> duration_gcd , duration ); <nl> } <nl> if ( pkt -> dts != AV_NOPTS_VALUE )
static inline int l3_unscale ( int value , int exponent ) <nl> # endif <nl> if ( e > ( SUINT ) 31 ) <nl> return 0 ; <nl> - m = ( m + ( 1 << ( e - 1 ))) >> e ; <nl> + m = ( m + (( 1U << e )>> 1 )) >> e ; <nl>  <nl> return m ; <nl> }
static int read_connect ( URLContext * s , RTMPContext * rt ) <nl> return ret ; <nl>  <nl> // Chunk size <nl> - if (( ret = ff_rtmp_packet_create (& pkt , RTMP_SYSTEM_CHANNEL , <nl> + if (( ret = ff_rtmp_packet_create (& pkt , RTMP_NETWORK_CHANNEL , <nl> RTMP_PT_CHUNK_SIZE , 0 , 4 )) < 0 ) <nl> return ret ; <nl> 
static int smacker_decode_header_tree ( SmackVContext * smk , GetBitContext * gb , int <nl> huff . maxlength = 0 ; <nl> huff . current = 0 ; <nl> huff . values = av_mallocz ( huff . length * sizeof ( int )); <nl> + if (! huff . values ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> if ( smacker_decode_bigtree ( gb , & huff , & ctx ) < 0 ) <nl> err = - 1 ;
int main ( int argc , char ** argv ) <nl> return 1 ; <nl> } <nl>  <nl> - frame -> pts = 0 ; <nl> + if ( frame ) <nl> + frame -> pts = 0 ; <nl> for (;;) { <nl> /* Compute current audio and video time . */ <nl> if ( audio_st )
static void mov_text_cleanup_ftab ( MovTextContext * m ) <nl>  <nl> static int mov_text_tx3g ( AVCodecContext * avctx , MovTextContext * m ) <nl> { <nl> - char * tx3g_ptr = avctx -> extradata ; <nl> + uint8_t * tx3g_ptr = avctx -> extradata ; <nl> int i , box_size , font_length ; <nl> int8_t v_align , h_align ; <nl> int style_fontID ;
# include " cabac_functions . h " <nl> # include " hevc . h " <nl>  <nl> -# define CABAC_MAX_BIN 100 <nl> +# define CABAC_MAX_BIN 31 <nl>  <nl> /** <nl> * number of bin by SyntaxElement .
static int get_video_frame ( VideoState * is , AVFrame * frame , int64_t * pts , AVPacke <nl> is -> frame_last_dropped_pos = pkt -> pos ; <nl> is -> frame_last_dropped_pts = dpts ; <nl> is -> frame_drops_early ++; <nl> + av_frame_unref ( frame ); <nl> ret = 0 ; <nl> } <nl> }
static int mp3_read_probe ( AVProbeData * p ) <nl>  <nl> max_frames = 0 ; <nl> buf = buf0 ; <nl> - end = buf + p -> buf_size - sizeof ( uint32_t ); <nl> + end = p -> buf + p -> buf_size - sizeof ( uint32_t ); <nl>  <nl> for (; buf < end ; buf = buf2 + 1 ) { <nl> buf2 = buf ;
int ff_mov_read_stsd_entries ( MOVContext * c , AVIOContext * pb , int entries ) <nl> avio_rb32 ( pb ); /* reserved */ <nl> avio_rb16 ( pb ); /* reserved */ <nl> dref_id = avio_rb16 ( pb ); <nl> + } else if ( size <= 0 ){ <nl> + av_log ( c -> fc , AV_LOG_ERROR , " invalid size % d in stsd \ n ", size ); <nl> + return - 1 ; <nl> } <nl>  <nl> if ( st -> codec -> codec_tag &&
int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , SwsFilter * dstFilter ) <nl> /* precalculate vertical scaler filter coefficients */ <nl> { <nl> const int filterAlign = <nl> - ( HAVE_MMX && cpu_flags & AV_CPU_FLAG_MMX ) && ( flags & SWS_ACCURATE_RND ) ? 2 : <nl> + ( HAVE_MMX && cpu_flags & AV_CPU_FLAG_MMX ) ? 2 : <nl> ( HAVE_ALTIVEC && cpu_flags & AV_CPU_FLAG_ALTIVEC ) ? 8 : <nl> 1 ; <nl> 
static inline void copy ( LZOContext * c , int cnt ) <nl> */ <nl> static inline void copy_backptr ( LZOContext * c , int back , int cnt ) <nl> { <nl> - register const uint8_t * src = & c -> out [- back ]; <nl> register uint8_t * dst = c -> out ; <nl> - if ( src < c -> out_start || src > dst ) { <nl> + if ( dst - c -> out_start < back ) { <nl> c -> error |= AV_LZO_INVALID_BACKPTR ; <nl> return ; <nl> }
static int mov_finalize_stsd_codec ( MOVContext * c , AVIOContext * pb , <nl>  <nl> static int mov_skip_multiple_stsd ( MOVContext * c , AVIOContext * pb , <nl> int codec_tag , int format , <nl> - int size ) <nl> + int64_t size ) <nl> { <nl> int video_codec_id = ff_codec_get_id ( ff_codec_movvideo_tags , format ); <nl> 
static void vda_decoder_callback ( void * vda_hw_ctx , <nl> vda_frame * new_frame ; <nl> vda_frame * queue_walker ; <nl>  <nl> - if (!( new_frame = av_mallocz ( sizeof ( vda_frame )))) <nl> + if (!( new_frame = av_mallocz ( sizeof (* new_frame )))) <nl> return ; <nl> new_frame -> next_frame = NULL ; <nl> new_frame -> cv_buffer = CVPixelBufferRetain ( image_buffer );
static int decode_picture_header ( AVCodecContext * avctx , const uint8_t * buf , cons <nl> \ <nl> if ( q > switch_bits ) { /* exp golomb */ \ <nl> bits = exp_order - switch_bits + ( q << 1 ); \ <nl> - if ( bits > MIN_CACHE_BITS ) \ <nl> + if ( bits > FFMIN ( MIN_CACHE_BITS , 31 )) \ <nl> return AVERROR_INVALIDDATA ; \ <nl> val = SHOW_UBITS ( re , gb , bits ) - ( 1 << exp_order ) + \ <nl> (( switch_bits + 1 ) << rice_order ); \
static int http_read_stream ( URLContext * h , uint8_t * buf , int size ) <nl>  <nl> av_log ( NULL , AV_LOG_TRACE , " Chunked encoding data size : %" PRId64 "'\ n ", <nl> s -> chunksize ); <nl> - <nl> - if (! s -> chunksize ) <nl> + if ( s -> chunksize < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + else if (! s -> chunksize ) <nl> return 0 ; <nl> break ; <nl> }
static int matroska_ebmlnum_uint ( MatroskaDemuxContext * matroska , <nl> { <nl> ByteIOContext pb ; <nl> init_put_byte (& pb , data , size , 0 , NULL , NULL , NULL , NULL ); <nl> - return ebml_read_num ( matroska , & pb , 8 , num ); <nl> + return ebml_read_num ( matroska , & pb , FFMIN ( size , 8 ), num ); <nl> } <nl>  <nl> /*
static int X264_frame ( AVCodecContext * ctx , uint8_t * buf , <nl> } <nl>  <nl> x4 -> out_pic . key_frame = pic_out . b_keyframe ; <nl> - x4 -> out_pic . quality = ( pic_out . i_qpplus1 - 1 ) * FF_QP2LAMBDA ; <nl> + if ( bufsize ) <nl> + x4 -> out_pic . quality = ( pic_out . i_qpplus1 - 1 ) * FF_QP2LAMBDA ; <nl>  <nl> return bufsize ; <nl> }
static int read_header ( AVFormatContext * s ) <nl> return ret ; <nl> } <nl>  <nl> - avio_seek ( pb , vst -> index_entries [ 0 ]. pos , SEEK_SET ); <nl> + if ( vst -> index_entries ) <nl> + avio_seek ( pb , vst -> index_entries [ 0 ]. pos , SEEK_SET ); <nl> + else <nl> + avio_skip ( pb , 4 ); <nl>  <nl> bink -> current_track = - 1 ; <nl> return 0 ;
int ff_combine_frame ( ParseContext * pc , int next , const uint8_t ** buf , int * buf_s <nl> if (! new_buffer ) <nl> return AVERROR ( ENOMEM ); <nl> pc -> buffer = new_buffer ; <nl> + if ( FF_INPUT_BUFFER_PADDING_SIZE > - next ) <nl> memcpy (& pc -> buffer [ pc -> index ], * buf , next + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> pc -> index = 0 ; <nl> * buf = pc -> buffer ;
static int check_n_master ( AVCodecContext * avctx , int n_master , int bs_xover_band <nl> static int sbr_make_f_master ( AACContext * ac , SpectralBandReplication * sbr , <nl> SpectrumParameters * spectrum ) <nl> { <nl> - unsigned int temp , max_qmf_subbands ; <nl> + unsigned int temp , max_qmf_subbands = 0 ; <nl> unsigned int start_min , stop_min ; <nl> int k ; <nl> const int8_t * sbr_offset_ptr ;
void ff_lzw_decode_tail ( LZWState * p ) <nl>  <nl> if ( s -> mode == FF_LZW_GIF ) { <nl> while ( s -> bs > 0 ) { <nl> - if ( s -> pbuf + s -> bs >= s -> ebuf ) { <nl> + if ( s -> bs >= s -> ebuf - s -> pbuf ) { <nl> s -> pbuf = s -> ebuf ; <nl> break ; <nl> } else {
static opj_image_t * mj2_create_image ( AVCodecContext * avctx , opj_cparameters_t * p <nl>  <nl> img = opj_image_create ( numcomps , cmptparm , color_space ); <nl>  <nl> + if (! img ) <nl> + return NULL ; <nl> + <nl> // x0 , y0 is the top left corner of the image <nl> // x1 , y1 is the width , height of the reference grid <nl> img -> x0 = 0 ;
static int decode_block ( MJpegDecodeContext * s , int16_t * block , int component , <nl> av_log ( s -> avctx , AV_LOG_ERROR , " error dc \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> - val = val * quant_matrix [ 0 ] + s -> last_dc [ component ]; <nl> + val = val * ( unsigned ) quant_matrix [ 0 ] + s -> last_dc [ component ]; <nl> val = av_clip_int16 ( val ); <nl> s -> last_dc [ component ] = val ; <nl> block [ 0 ] = val ;
static int vorbis_parse ( AVCodecParserContext * s1 , AVCodecContext * avctx , <nl>  <nl> if (! s -> vp && avctx -> extradata && avctx -> extradata_size ) { <nl> s -> vp = av_vorbis_parse_init ( avctx -> extradata , avctx -> extradata_size ); <nl> - if (! s -> vp ) <nl> - goto end ; <nl> } <nl> + if (! s -> vp ) <nl> + goto end ; <nl>  <nl> if (( duration = av_vorbis_parse_frame ( s -> vp , buf , buf_size )) >= 0 ) <nl> s1 -> duration = duration ;
static void pop_output_configuration ( AACContext * ac ) { <nl> ac -> oc [ 1 ] = ac -> oc [ 0 ]; <nl> ac -> avctx -> channels = ac -> oc [ 1 ]. channels ; <nl> ac -> avctx -> channel_layout = ac -> oc [ 1 ]. channel_layout ; <nl> - } else { <nl> - ac -> avctx -> channels = 0 ; <nl> - ac -> avctx -> channel_layout = 0 ; <nl> } <nl> } <nl> }
# include " mlz . h " <nl>  <nl> av_cold void ff_mlz_init_dict ( void * context , MLZ * mlz ) { <nl> - mlz -> dict = av_malloc_array ( TABLE_SIZE , sizeof (* mlz -> dict )); <nl> + mlz -> dict = av_mallocz_array ( TABLE_SIZE , sizeof (* mlz -> dict )); <nl>  <nl> mlz -> flush_code = FLUSH_CODE ; <nl> mlz -> current_dic_index_max = DIC_INDEX_INIT ;
static int escape124_decode_frame ( AVCodecContext * avctx , <nl> cb_size = s -> num_superblocks << cb_depth ; <nl> } <nl> } <nl> + if ( s -> num_superblocks >= INT_MAX >> cb_depth ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Depth or num_superblocks are too large \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> av_freep (& s -> codebooks [ i ]. blocks ); <nl> s -> codebooks [ i ] = unpack_codebook (& gb , cb_depth , cb_size ); <nl> if (! s -> codebooks [ i ]. blocks )
static int mov_write_header ( AVFormatContext * s ) <nl> AVStream * st = s -> streams [ i ]; <nl> if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO || <nl> st -> codec -> codec_type == AVMEDIA_TYPE_AUDIO ) { <nl> - ff_mov_init_hinting ( s , hint_track , i ); <nl> + if ( ff_mov_init_hinting ( s , hint_track , i ) < 0 ) <nl> + goto error ; <nl> hint_track ++; <nl> } <nl> }
typedef struct SmcContext { <nl> row_ptr += stride * 4 ; \ <nl> } \ <nl> total_blocks --; \ <nl> - if ( total_blocks < 0 ) \ <nl> + if ( total_blocks < 0 + !! n_blocks ) \ <nl> { \ <nl> av_log ( s -> avctx , AV_LOG_INFO , " warning : block counter just went negative ( this should not happen )\ n "); \ <nl> return ; \
static int jpeg2000_read_main_headers ( Jpeg2000DecoderContext * s ) <nl> if ( marker == JPEG2000_EOC ) <nl> break ; <nl>  <nl> - if ( bytestream2_get_bytes_left (& s -> g ) < 2 ) <nl> - return AVERROR_INVALIDDATA ; <nl> len = bytestream2_get_be16u (& s -> g ); <nl> + if ( len < 2 || bytestream2_get_bytes_left (& s -> g ) < len - 2 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> switch ( marker ) { <nl> case JPEG2000_SIZ : <nl> ret = get_siz ( s );
static int lag_read_prob_header ( lag_rac * rac , GetBitContext * gb ) <nl> } <nl>  <nl> scale_factor ++; <nl> - cumulative_target = 1 << scale_factor ; <nl> + if ( scale_factor >= 32U ) <nl> + return AVERROR_INVALIDDATA ; <nl> + cumulative_target = 1U << scale_factor ; <nl>  <nl> if ( scaled_cumul_prob > cumulative_target ) { <nl> av_log ( rac -> avctx , AV_LOG_ERROR ,
static av_cold int adx_encode_init ( AVCodecContext * avctx ) <nl> avctx -> frame_size = BLOCK_SAMPLES ; <nl>  <nl> avctx -> coded_frame = avcodec_alloc_frame (); <nl> + if (! avctx -> coded_frame ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> /* the cutoff can be adjusted , but this seems to work pretty well */ <nl> c -> cutoff = 500 ;
int avpriv_ac3_parse_header ( AC3HeaderInfo ** phdr , const uint8_t * buf , <nl> return AVERROR ( ENOMEM ); <nl> hdr = * phdr ; <nl>  <nl> - init_get_bits8 (& gb , buf , size ); <nl> + err = init_get_bits8 (& gb , buf , size ); <nl> + if ( err < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> err = ff_ac3_parse_header (& gb , hdr ); <nl> if ( err < 0 ) <nl> return AVERROR_INVALIDDATA ;
int ff_mjpeg_decode_sof ( MJpegDecodeContext * s ) <nl> else s -> avctx -> pix_fmt = AV_PIX_FMT_YUV420P16 ; <nl> s -> avctx -> color_range = s -> cs_itu601 ? AVCOL_RANGE_MPEG : AVCOL_RANGE_JPEG ; <nl> if ( pix_fmt_id == 0x42111100 ) { <nl> + if ( s -> bits > 8 ) <nl> + goto unk_pixfmt ; <nl> s -> upscale_h = 6 ; <nl> } else if ( pix_fmt_id == 0x24111100 ) { <nl> + if ( s -> bits > 8 ) <nl> + goto unk_pixfmt ; <nl> s -> upscale_v = 6 ; <nl> } <nl> break ;
static int ogg_write_trailer ( AVFormatContext * s ) <nl> av_free ( oggstream -> header [ 0 ]); <nl> av_free ( oggstream -> header [ 1 ]); <nl> } <nl> + else <nl> + av_free ( oggstream -> header [ 1 ]); <nl> av_freep (& st -> priv_data ); <nl> } <nl> return 0 ;
static int sbr_make_f_master ( AACContext * ac , SpectralBandReplication * sbr , <nl> max_qmf_subbands = 35 ; <nl> } else if ( sbr -> sample_rate >= 48000 ) <nl> max_qmf_subbands = 32 ; <nl> + else <nl> + av_assert0 ( 0 ); <nl>  <nl> if ( sbr -> k [ 2 ] - sbr -> k [ 0 ] > max_qmf_subbands ) { <nl> av_log ( ac -> avctx , AV_LOG_ERROR ,
static void fix_bitshift ( ShortenContext * s , int32_t * buffer ) <nl>  <nl> if ( s -> bitshift != 0 ) <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> - buffer [ s -> nwrap + i ] <<= s -> bitshift ; <nl> + buffer [ i ] <<= s -> bitshift ; <nl> } <nl>  <nl> 
static int hnm_decode_frame ( AVCodecContext * avctx , void * data , <nl> int ret ; <nl> uint16_t chunk_id ; <nl>  <nl> + if ( avpkt -> size < 8 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " packet too small \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (( ret = ff_get_buffer ( avctx , frame , 0 )) < 0 ) <nl> return ret ; <nl> 
static int filter_frame ( AVFilterLink * inlink , AVFrame * insamples ) <nl> break ; <nl> av_assert1 ( input_number < am -> nb_inputs ); <nl> if ( ff_bufqueue_is_full (& am -> in [ input_number ]. queue )) { <nl> - av_log ( ctx , AV_LOG_ERROR , " Buffer queue overflow \ n "); <nl> av_frame_free (& insamples ); <nl> return AVERROR ( ENOMEM ); <nl> }
static av_cold void init_cplscales_table ( COOKContext * q ) <nl> static inline int decode_bytes ( const uint8_t * inbuffer , uint8_t * out , int bytes ) <nl> { <nl> static const uint32_t tab [ 4 ] = { <nl> - AV_BE2NE32C ( 0x37c511f2 ), AV_BE2NE32C ( 0xf237c511 ), <nl> - AV_BE2NE32C ( 0x11f237c5 ), AV_BE2NE32C ( 0xc511f237 ), <nl> + AV_BE2NE32C ( 0x37c511f2U ), AV_BE2NE32C ( 0xf237c511U ), <nl> + AV_BE2NE32C ( 0x11f237c5U ), AV_BE2NE32C ( 0xc511f237U ), <nl> }; <nl> int i , off ; <nl> uint32_t c ;
static int wv_get_value ( WavpackFrameContext * ctx , GetBitContext * gb , <nl> INC_MED ( 1 ); <nl> DEC_MED ( 2 ); <nl> } else { <nl> - base = GET_MED ( 0 ) + GET_MED ( 1 ) + GET_MED ( 2 ) * ( t - 2 ); <nl> + base = GET_MED ( 0 ) + GET_MED ( 1 ) + GET_MED ( 2 ) * ( t - 2U ); <nl> add = GET_MED ( 2 ) - 1 ; <nl> INC_MED ( 0 ); <nl> INC_MED ( 1 );
# define CODE_UNSET - 1 <nl> # define CODE_BIT_INIT 9 <nl> # define DIC_INDEX_INIT 512 // 2 ^ 9 <nl> -# define DIC_INDEX_MAX 32768l // 2 ^ 15 <nl> +# define DIC_INDEX_MAX 32768 // 2 ^ 15 <nl> # define FLUSH_CODE 256 <nl> # define FREEZE_CODE 257 <nl> # define FIRST_CODE 258 <nl> -# define MAX_CODE 32767l <nl> -# define TABLE_SIZE 35023l // TABLE_SIZE must be a prime number <nl> +# define MAX_CODE 32767 <nl> +# define TABLE_SIZE 35023 // TABLE_SIZE must be a prime number <nl>  <nl> /** Dictionary structure for mlz decompression <nl> */
static av_always_inline av_const int32_t av_clipl_int32_c ( int64_t a ) <nl> */ <nl> static av_always_inline av_const int av_clip_intp2_c ( int a , int p ) <nl> { <nl> - if (( a + ( 1 << p )) & ~(( 2 << p ) - 1 )) <nl> + if ((( unsigned ) a + ( 1 << p )) & ~(( 2 << p ) - 1 )) <nl> return ( a >> 31 ) ^ (( 1 << p ) - 1 ); <nl> else <nl> return a ;
static void dmix_sub_c ( int32_t * dst , const int32_t * src , int coeff , ptrdiff_t le <nl> int i ; <nl>  <nl> for ( i = 0 ; i < len ; i ++) <nl> - dst [ i ] -= mul15 ( src [ i ], coeff ); <nl> + dst [ i ] -= ( unsigned ) mul15 ( src [ i ], coeff ); <nl> } <nl>  <nl> static void dmix_add_c ( int32_t * dst , const int32_t * src , int coeff , ptrdiff_t len )
static inline int wv_get_value_integer ( WavpackFrameContext * s , uint32_t * crc , <nl> unsigned bit ; <nl>  <nl> if ( s -> extra_bits ) { <nl> - S <<= s -> extra_bits ; <nl> + S *= 1 << s -> extra_bits ; <nl>  <nl> if ( s -> got_extra_bits && <nl> get_bits_left (& s -> gb_extra_bits ) >= s -> extra_bits ) {
FFPsyChannelGroup * ff_psy_find_group ( FFPsyContext * ctx , int channel ) <nl>  <nl> av_cold void ff_psy_end ( FFPsyContext * ctx ) <nl> { <nl> - if ( ctx -> model -> end ) <nl> + if ( ctx -> model && ctx -> model -> end ) <nl> ctx -> model -> end ( ctx ); <nl> av_freep (& ctx -> bands ); <nl> av_freep (& ctx -> num_bands );
static int thp_read_packet ( AVFormatContext * s , <nl> pkt -> stream_index = thp -> video_stream_index ; <nl> } else { <nl> ret = av_get_packet ( pb , pkt , thp -> audiosize ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( ret != thp -> audiosize ) { <nl> av_free_packet ( pkt ); <nl> return AVERROR ( EIO );
static int v410_encode_frame ( AVCodecContext * avctx , uint8_t * buf , <nl> int i , j ; <nl> int output_size = 0 ; <nl>  <nl> - if ( buf_size < avctx -> width * avctx -> height * 3 ) { <nl> + if ( buf_size < avctx -> width * avctx -> height * 4 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Out buffer is too small .\ n "); <nl> return AVERROR ( ENOMEM ); <nl> }
void ff_set_mpeg4_time ( MpegEncContext * s ) <nl>  <nl> static void mpeg4_encode_gop_header ( MpegEncContext * s ) <nl> { <nl> - int hours , minutes , seconds ; <nl> + int64_t hours , minutes , seconds ; <nl> int64_t time ; <nl>  <nl> put_bits (& s -> pb , 16 , 0 );
static int decode_frame ( AVCodecContext * avctx , <nl> buf_size - offset ); <nl> if ( t < 0 ) { <nl> int j = tm2_stream_order [ i ]; <nl> - memset ( l -> tokens [ j ], 0 , sizeof (** l -> tokens ) * l -> tok_lens [ j ]); <nl> + if ( l -> tok_lens [ j ]) <nl> + memset ( l -> tokens [ j ], 0 , sizeof (** l -> tokens ) * l -> tok_lens [ j ]); <nl> return t ; <nl> } <nl> offset += t ;
static int decode_init_thread_copy ( AVCodecContext * avctx ) <nl> memset ( h -> sps_buffers , 0 , sizeof ( h -> sps_buffers )); <nl> memset ( h -> pps_buffers , 0 , sizeof ( h -> pps_buffers )); <nl>  <nl> + h -> rbsp_buffer [ 0 ] = NULL ; <nl> + h -> rbsp_buffer [ 1 ] = NULL ; <nl> + h -> rbsp_buffer_size [ 0 ] = 0 ; <nl> + h -> rbsp_buffer_size [ 1 ] = 0 ; <nl> h -> context_initialized = 0 ; <nl>  <nl> return 0 ;
int ff_h264_decode_ref_pic_list_reordering ( H264Context * h , H264SliceContext * sl ) <nl>  <nl> long_idx = pic_num_extract ( h , pic_id , & pic_structure ); <nl>  <nl> - if ( long_idx > 31 ) { <nl> + if ( long_idx > 31U ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , <nl> " long_term_pic_idx overflow \ n "); <nl> return AVERROR_INVALIDDATA ;
end : <nl> free_and_end : <nl> av_dict_free (& tmp ); <nl> av_freep (& avctx -> priv_data ); <nl> - if ( avctx -> internal ) <nl> + if ( avctx -> internal ) { <nl> av_freep (& avctx -> internal -> pool ); <nl> + av_frame_free (& avctx -> internal -> to_free ); <nl> + } <nl> av_freep (& avctx -> internal ); <nl> avctx -> codec = NULL ; <nl> goto end ;
static int mxf_read_generic_descriptor ( void * arg , AVIOContext * pb , int tag , int <nl> default : <nl> /* Private uid used by SONY C0023S01 . mxf */ <nl> if ( IS_KLV_KEY ( uid , mxf_sony_mpeg4_extradata )) { <nl> - descriptor -> extradata = av_malloc ( size ); <nl> + descriptor -> extradata = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! descriptor -> extradata ) <nl> return - 1 ; <nl> descriptor -> extradata_size = size ;
static char * get_content_url ( xmlNodePtr * baseurl_nodes , <nl> return NULL ; <nl> } <nl> av_strlcpy ( tmp_str , url , sizeof ( tmp_str )); <nl> - av_free ( url ); <nl> } <nl> if ( rep_bandwidth_val && tmp_str [ 0 ] != '\ 0 ') { <nl> + // free any previously assigned url before reassigning <nl> + av_free ( url ); <nl> url = av_strireplace ( tmp_str , "$ Bandwidth $", ( const char *) rep_bandwidth_val ); <nl> if (! url ) { <nl> return NULL ;
static int cdxl_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_free_packet ( pkt ); <nl> return ret ; <nl> } <nl> + av_shrink_packet ( pkt , CDXL_HEADER_SIZE + ret ); <nl> pkt -> stream_index = cdxl -> video_stream_index ; <nl> pkt -> flags |= AV_PKT_FLAG_KEY ; <nl> pkt -> pos = pos ;
static av_always_inline av_const float roundf ( float x ) <nl> } <nl> # endif /* HAVE_ROUNDF */ <nl>  <nl> +# if ! HAVE_TRUNC <nl> + static av_always_inline av_const double trunc ( double x ) <nl> +{ <nl> + return ( x > 0 ) ? floor ( x ) : ceil ( x ); <nl> +} <nl> +# endif /* HAVE_TRUNC */ <nl> + <nl> # if ! HAVE_TRUNCF <nl> static av_always_inline av_const float truncf ( float x ) <nl> {
static int mxf_read_primer_pack ( void * arg , AVIOContext * pb , int tag , int size , U <nl> avpriv_request_sample ( pb , " Primer pack item length % d ", item_len ); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> - if ( item_num > 65536 ) { <nl> + if ( item_num > 65536 || item_num < 0 ) { <nl> av_log ( mxf -> fc , AV_LOG_ERROR , " item_num % d is too large \ n ", item_num ); <nl> return AVERROR_INVALIDDATA ; <nl> }
int ff_rm_read_mdpr_codecdata ( AVFormatContext * s , AVIOContext * pb , <nl> skip : <nl> /* skip codec info */ <nl> size = avio_tell ( pb ) - codec_pos ; <nl> - avio_skip ( pb , codec_data_size - size ); <nl> + if ( codec_data_size >= size ) { <nl> + avio_skip ( pb , codec_data_size - size ); <nl> + } else { <nl> + av_log ( s , AV_LOG_WARNING , " codec_data_size % u < size % d \ n ", codec_data_size , size ); <nl> + } <nl>  <nl> return 0 ; <nl> }
void ff_parse_specific_params ( AVStream * st , int * au_rate , <nl>  <nl> void ff_riff_write_info_tag ( AVIOContext * pb , const char * tag , const char * str ) <nl> { <nl> - int len = strlen ( str ); <nl> - if ( len > 0 ) { <nl> + size_t len = strlen ( str ); <nl> + if ( len > 0 && len < UINT32_MAX ) { <nl> len ++; <nl> ffio_wfourcc ( pb , tag ); <nl> avio_wl32 ( pb , len );
static int mxf_write_header ( AVFormatContext * s ) <nl> mxf -> edit_unit_byte_count += klv_fill_size ( mxf -> edit_unit_byte_count ); <nl>  <nl> sc -> signal_standard = 1 ; <nl> + sc -> color_siting = 0 ; <nl> } <nl> if ( mxf -> signal_standard >= 0 ) <nl> sc -> signal_standard = mxf -> signal_standard ;
AVCodec ff_ffv1_decoder = { <nl> . update_thread_context = ONLY_IF_THREADS_ENABLED ( update_thread_context ), <nl> . capabilities = AV_CODEC_CAP_DR1 /*| AV_CODEC_CAP_DRAW_HORIZ_BAND */ | <nl> AV_CODEC_CAP_FRAME_THREADS | AV_CODEC_CAP_SLICE_THREADS , <nl> + . caps_internal = FF_CODEC_CAP_INIT_CLEANUP <nl> };
static const struct URLProtocol * url_find_protocol ( const char * filename ) <nl> * ptr = '\ 0 '; <nl>  <nl> protocols = ffurl_get_protocols ( NULL , NULL ); <nl> + if (! protocols ) <nl> + return NULL ; <nl> for ( i = 0 ; protocols [ i ]; i ++) { <nl> const URLProtocol * up = protocols [ i ]; <nl> if (! strcmp ( proto_str , up -> name )) {
static int read_quant_tables ( RangeCoder * c , <nl> int context_count = 1 ; <nl>  <nl> for ( i = 0 ; i < 5 ; i ++) { <nl> - context_count *= read_quant_table ( c , quant_table [ i ], context_count ); <nl> + int ret = read_quant_table ( c , quant_table [ i ], context_count ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + context_count *= ret ; <nl> if ( context_count > 32768U ) { <nl> return AVERROR_INVALIDDATA ; <nl> }
static void decode_channel_map ( uint8_t layout_map [][ 3 ], <nl> case AAC_CHANNEL_LFE : <nl> syn_ele = TYPE_LFE ; <nl> break ; <nl> + default : <nl> + av_assert0 ( 0 ); <nl> } <nl> layout_map [ 0 ][ 0 ] = syn_ele ; <nl> layout_map [ 0 ][ 1 ] = get_bits ( gb , 4 );
static void estimate_timings_from_bit_rate ( AVFormatContext * ic ) <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> st = ic -> streams [ i ]; <nl> if ( st -> codec -> bit_rate > 0 ) { <nl> - if ( INT_MAX - st -> codec -> bit_rate > bit_rate ) { <nl> + if ( INT_MAX - st -> codec -> bit_rate < bit_rate ) { <nl> bit_rate = 0 ; <nl> break ; <nl> }
int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { <nl> AVCodecContext * ctx = avcodec_alloc_context3 ( NULL ); <nl> if (! ctx ) <nl> error (" Failed memory allocation "); <nl> + <nl> + ctx -> max_pixels = 4096 * 4096 ; // To reduce false positive OOM and hangs <nl> + <nl> int res = avcodec_open2 ( ctx , c , NULL ); <nl> if ( res < 0 ) <nl> return res ;
static int encode_apng ( AVCodecContext * avctx , AVPacket * pkt , <nl> int ret ; <nl> int enc_row_size ; <nl> size_t max_packet_size ; <nl> - APNGFctlChunk fctl_chunk ; <nl> + APNGFctlChunk fctl_chunk = { 0 }; <nl>  <nl> if ( pict && avctx -> codec_id == AV_CODEC_ID_APNG && s -> color_type == PNG_COLOR_TYPE_PALETTE ) { <nl> uint32_t checksum = ~ av_crc ( av_crc_get_table ( AV_CRC_32_IEEE_LE ), ~ 0U , pict -> data [ 1 ], 256 * sizeof ( uint32_t ));
static int wma_decode_superframe ( AVCodecContext * avctx , <nl> return 0 ; <nl> } <nl> if ( buf_size < s -> block_align ) <nl> - return 0 ; <nl> + return AVERROR ( EINVAL ); <nl> buf_size = s -> block_align ; <nl>  <nl> samples = data ;
 <nl> typedef struct Hnm4VideoContext { <nl> uint8_t version ; <nl> - uint16_t width ; <nl> - uint16_t height ; <nl> + int width ; <nl> + int height ; <nl> uint8_t * current ; <nl> uint8_t * previous ; <nl> uint8_t * buffer1 ;
static int64_t read_ts ( char ** line , int * duration ) <nl> int64_t start , end ; <nl>  <nl> if ( sscanf (* line , "%" SCNd64 ",%" SCNd64 , & start , & end ) == 2 ) { <nl> - * line += strcspn (* line , "\"") + 1 ; <nl> + * line += strcspn (* line , "\""); <nl> + * line += !!** line ; <nl> * duration = end - start ; <nl> return start ; <nl> }
sigterm_handler ( int sig ) <nl> received_nb_signals ++; <nl> term_exit_sigsafe (); <nl> if ( received_nb_signals > 3 ) <nl> - exit_program ( 123 ); <nl> + exit ( 123 ); <nl> } <nl>  <nl> void term_init ( void )
void ff_aac_update_ltp ( AACEncContext * s , SingleChannelElement * sce ) <nl> lag = i ; <nl> } <nl> } <nl> - lag = av_clip ( lag , 0 , 2048 ); /* 11 bits => 2 ^ 11 = 2048 */ <nl> + lag = av_clip ( lag , 0 , 2047 ); /* 11 bits => 2 ^ 11 = 0 -> 2047 */ <nl>  <nl> if (! lag ) { <nl> sce -> ics . ltp . lag = lag ;
static int pmp_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( pmp -> cur_stream == 0 ) { <nl> int num_packets ; <nl> pmp -> audio_packets = avio_r8 ( pb ); <nl> + if (! pmp -> audio_packets ) { <nl> + av_log_ask_for_sample ( s , " 0 audio packets \ n "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> num_packets = ( pmp -> num_streams - 1 ) * pmp -> audio_packets + 1 ; <nl> avio_skip ( pb , 8 ); <nl> pmp -> current_packet = 0 ;
static int msmpeg4v34_decode_mb ( MpegEncContext * s , int16_t block [ 6 ][ 64 ]) <nl> uint8_t * coded_val ; <nl> uint32_t * const mb_type_ptr = & s -> current_picture . mb_type [ s -> mb_x + s -> mb_y * s -> mb_stride ]; <nl>  <nl> + if ( get_bits_left (& s -> gb ) <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( s -> pict_type == AV_PICTURE_TYPE_P ) { <nl> if ( s -> use_skip_mb_code ) { <nl> if ( get_bits1 (& s -> gb )) {
int64_t ff_gen_search ( AVFormatContext * s , int stream_index , int64_t target_ts , <nl> } <nl>  <nl> if ( ts_max == AV_NOPTS_VALUE ){ <nl> - int step = 1024 ; <nl> + int64_t step = 1024 ; <nl> filesize = avio_size ( s -> pb ); <nl> pos_max = filesize - 1 ; <nl> do {
int swri_realloc_audio ( AudioData * a , int count ){ <nl> av_assert0 ( a -> bps ); <nl> av_assert0 ( a -> ch_count ); <nl>  <nl> - a -> data = av_mallocz ( countb * a -> ch_count ); <nl> + a -> data = av_mallocz_array ( countb , a -> ch_count ); <nl> if (! a -> data ) <nl> return AVERROR ( ENOMEM ); <nl> for ( i = 0 ; i < a -> ch_count ; i ++){
static int tm2_read_stream ( TM2Context * ctx , const uint8_t * buf , int stream_id , i <nl> buf += 4 ; cur += 4 ; <nl> buf += 4 ; cur += 4 ; /* unused by decoder */ <nl>  <nl> + if ( skip < cur ) <nl> + return - 1 ; <nl> init_get_bits (& ctx -> gb , buf , ( skip - cur ) * 8 ); <nl> if ( tm2_build_huff_table ( ctx , & codes ) == - 1 ) <nl> return - 1 ;
int ff_rtsp_open_transport_ctx ( AVFormatContext * s , RTSPStream * rtsp_st ) <nl>  <nl> if (! rtsp_st -> transport_priv ) { <nl> return AVERROR ( ENOMEM ); <nl> - } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP ) { <nl> + } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP && <nl> + s -> iformat ) { <nl> RTPDemuxContext * rtpctx = rtsp_st -> transport_priv ; <nl> rtpctx -> ssrc = rtsp_st -> ssrc ; <nl> if ( rtsp_st -> dynamic_handler ) {
const uint8_t * avpriv_mpv_find_start_code ( const uint8_t * restrict p , <nl> av_cold int ff_dct_common_init ( MpegEncContext * s ) <nl> { <nl> ff_dsputil_init (& s -> dsp , s -> avctx ); <nl> - ff_videodsp_init (& s -> vdsp , 8 ); <nl> + ff_videodsp_init (& s -> vdsp , s -> avctx -> bits_per_raw_sample ); <nl>  <nl> s -> dct_unquantize_h263_intra = dct_unquantize_h263_intra_c ; <nl> s -> dct_unquantize_h263_inter = dct_unquantize_h263_inter_c ;
void ff_set_mpeg4_time ( MpegEncContext * s ) <nl>  <nl> static void mpeg4_encode_gop_header ( MpegEncContext * s ) <nl> { <nl> - int hours , minutes , seconds ; <nl> + int64_t hours , minutes , seconds ; <nl> int64_t time ; <nl>  <nl> put_bits (& s -> pb , 16 , 0 );
static void revert_cdlms ( WmallDecodeCtx * s , int ch , <nl> s -> channel_residues [ ch ][ icoef ] = input ; <nl> } <nl> } <nl> + emms_c (); <nl> } <nl>  <nl> static void revert_inter_ch_decorr ( WmallDecodeCtx * s , int tile_size )
static int poll_filters ( void ) <nl> } <nl> break ; <nl> } <nl> + frame_pts = AV_NOPTS_VALUE ; <nl> if ( ost -> enc -> type == AVMEDIA_TYPE_VIDEO ) <nl> filtered_frame -> pts = frame_pts = av_rescale_q ( picref -> pts , ist_pts_tb , AV_TIME_BASE_Q ); <nl> else if ( picref -> pts != AV_NOPTS_VALUE )
static int decode_pivot ( MSS1Context * ctx , ArithCoder * acoder , int base ) <nl> val = arith_get_number ( acoder , ( base + 1 ) / 2 - 2 ) + 3 ; <nl> } <nl>  <nl> - if ( val == base ) { <nl> + if (( unsigned ) val >= base ) { <nl> ctx -> corrupted = 1 ; <nl> return 0 ; <nl> }
static int h264_slice_header_parse ( const H264Context * h , H264SliceContext * sl , <nl> } <nl>  <nl> sl -> last_qscale_diff = 0 ; <nl> - tmp = pps -> init_qp + get_se_golomb (& sl -> gb ); <nl> + tmp = pps -> init_qp + ( unsigned ) get_se_golomb (& sl -> gb ); <nl> if ( tmp > 51 + 6 * ( sps -> bit_depth_luma - 8 )) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " QP % u out of range \ n ", tmp ); <nl> return AVERROR_INVALIDDATA ;
static int decode_dvd_subtitles ( DVDSubContext * ctx , AVSubtitle * sub_header , <nl> w = x2 - x1 + 1 ; <nl> if ( w < 0 ) <nl> w = 0 ; <nl> - h = y2 - y1 ; <nl> + h = y2 - y1 + 1 ; <nl> if ( h < 0 ) <nl> h = 0 ; <nl> if ( w > 0 && h > 0 ) {
static inline int set_options ( AVFilterContext * ctx , const char * args ) <nl>  <nl> hue -> hue_expr = NULL ; <nl> hue -> hue_deg_expr = NULL ; <nl> + hue -> saturation_expr = NULL ; <nl>  <nl> if (( ret = av_set_options_string ( hue , args , "=", ":")) < 0 ) <nl> return ret ;
int main ( int argc , char * argv []) <nl> end : <nl> avformat_close_input (& fmt_ctx ); <nl> /* note : the internal buffer could have changed , and be != avio_ctx_buffer */ <nl> - av_freep (& avio_ctx -> buffer ); <nl> - av_freep (& avio_ctx ); <nl> + if ( avio_ctx ) { <nl> + av_freep (& avio_ctx -> buffer ); <nl> + av_freep (& avio_ctx ); <nl> + } <nl> av_file_unmap ( buffer , buffer_size ); <nl>  <nl> if ( ret < 0 ) {
static av_cold int ulti_decode_init ( AVCodecContext * avctx ) <nl> s -> width = avctx -> width ; <nl> s -> height = avctx -> height ; <nl> s -> blocks = ( s -> width / 8 ) * ( s -> height / 8 ); <nl> + if ( s -> blocks == 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> avctx -> pix_fmt = AV_PIX_FMT_YUV410P ; <nl> s -> ulti_codebook = ulti_codebook ; <nl> 
static int decode_frame_png ( AVCodecContext * avctx , <nl> } <nl>  <nl> if (( ret = av_frame_ref ( data , s -> picture . f )) < 0 ) <nl> - return ret ; <nl> + goto the_end ; <nl>  <nl> * got_frame = 1 ; <nl> 
static int libopus_encode ( AVCodecContext * avctx , AVPacket * avpkt , <nl> int ret ; <nl>  <nl> if ( frame ) { <nl> - ff_af_queue_add (& opus -> afq , frame ); <nl> + ret = ff_af_queue_add (& opus -> afq , frame ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( frame -> nb_samples < opus -> opts . packet_size ) { <nl> audio = opus -> samples ; <nl> memcpy ( audio , frame -> data [ 0 ], frame -> nb_samples * sample_size );
av_cold int ff_dvvideo_init ( AVCodecContext * avctx ) <nl> ff_dv_rl_vlc [ i ]. run = run ; <nl> } <nl> ff_free_vlc (& dv_vlc ); <nl> - <nl> - dv_vlc_map_tableinit (); <nl> } <nl>  <nl> /* Generic DSP setup */ <nl> static av_cold int dvvideo_init_encoder ( AVCodecContext * avctx ) <nl> return AVERROR ( EINVAL ); <nl> } <nl>  <nl> + dv_vlc_map_tableinit (); <nl> + <nl> return ff_dvvideo_init ( avctx ); <nl> } <nl> 
static inline int read_huff_channels ( MLPDecodeContext * m , GetBitContext * gbp , <nl> result = ( result << lsb_bits ) + get_bits ( gbp , lsb_bits ); <nl>  <nl> result += cp -> sign_huff_offset ; <nl> - result <<= quant_step_size ; <nl> + result *= 1 << quant_step_size ; <nl>  <nl> m -> sample_buffer [ pos + s -> blockpos ][ channel ] = result ; <nl> }
static int64_t mxf_essence_container_end ( MXFContext * mxf , int body_sid ) <nl> static int mxf_edit_unit_absolute_offset ( MXFContext * mxf , MXFIndexTable * index_table , int64_t edit_unit , int64_t * edit_unit_out , int64_t * offset_out , int nag ) <nl> { <nl> int i ; <nl> - int offset_temp = 0 ; <nl> + int64_t offset_temp = 0 ; <nl>  <nl> for ( i = 0 ; i < index_table -> nb_segments ; i ++) { <nl> MXFIndexTableSegment * s = index_table -> segments [ i ];
retry : <nl> uint8_t * side_data = av_packet_new_side_data ( pkt , <nl> AV_PKT_DATA_METADATA_UPDATE , <nl> os -> new_metadata_size ); <nl> + if ( side_data == NULL ) { <nl> + av_free_packet ( pkt ); <nl> + av_free ( pkt ); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> memcpy ( side_data , os -> new_metadata , os -> new_metadata_size ); <nl> av_freep (& os -> new_metadata ); <nl> os -> new_metadata_size = 0 ;
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> } <nl> } <nl> total_size += 8 ; <nl> - if ( a . size == 1 ) { /* 64 bit extended size */ <nl> + if ( a . size == 1 && total_size + 8 <= atom . size ) { /* 64 bit extended size */ <nl> a . size = avio_rb64 ( pb ) - 8 ; <nl> total_size += 8 ; <nl> }
static int decode_pic_hdr ( IVI45DecContext * ctx , AVCodecContext * avctx ) <nl> /* skip picture header extension if any */ <nl> while ( get_bits1 (& ctx -> gb )) { <nl> ff_dlog ( avctx , " Pic hdr extension encountered !\ n "); <nl> + if ( get_bits_left (& ctx -> gb ) < 10 ) <nl> + return AVERROR_INVALIDDATA ; <nl> skip_bits (& ctx -> gb , 8 ); <nl> } <nl> 
static int hls_write_header ( AVFormatContext * s ) <nl> int ret , i ; <nl> char * p ; <nl> const char * pattern = "% d . ts "; <nl> - int basename_size = strlen ( s -> filename ) + strlen ( pattern ); <nl> + int basename_size = strlen ( s -> filename ) + strlen ( pattern ) + 1 ; <nl>  <nl> hls -> number = 0 ; <nl> 
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> if (! reslevel -> band ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> + if ( reslevel -> num_precincts_x * ( uint64_t ) reslevel -> num_precincts_y * reslevel -> nbands > avctx -> max_pixels / sizeof (* reslevel -> band -> prec )) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> for ( bandno = 0 ; bandno < reslevel -> nbands ; bandno ++, gbandno ++) { <nl> ret = init_band ( avctx , reslevel , <nl> comp , codsty , qntsty ,
int av_find_stream_info ( AVFormatContext * ic ) <nl> } <nl>  <nl> pkt = add_to_pktbuf (& ic -> packet_buffer , & pkt1 ); <nl> - if ( av_dup_packet ( pkt ) < 0 ) <nl> + if ( av_dup_packet ( pkt ) < 0 ) { <nl> + av_free ( duration_error ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> read_size += pkt -> size ; <nl> 
static int mov_write_header ( AVFormatContext * s ) <nl> else if (! TAG_IS_AVCI ( track -> tag )){ <nl> track -> vos_len = st -> codec -> extradata_size ; <nl> track -> vos_data = av_malloc ( track -> vos_len ); <nl> - if (! track -> vos_data ) <nl> + if (! track -> vos_data ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> goto error ; <nl> + } <nl> memcpy ( track -> vos_data , st -> codec -> extradata , track -> vos_len ); <nl> } <nl> }
static int mov_write_tkhd_tag ( AVIOContext * pb , MOVMuxContext * mov , <nl>  <nl> display_matrix = ( uint32_t *) av_stream_get_side_data ( st , AV_PKT_DATA_DISPLAYMATRIX , <nl> & display_matrix_size ); <nl> - if ( display_matrix_size < 9 * sizeof (* display_matrix )) <nl> + if ( display_matrix && display_matrix_size < 9 * sizeof (* display_matrix )) <nl> display_matrix = NULL ; <nl> } <nl> 
int vp56_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> VP56Context * s = avctx -> priv_data ; <nl> AVFrame * const p = s -> framep [ VP56_FRAME_CURRENT ]; <nl> int remaining_buf_size = buf_size ; <nl> - int is_alpha , alpha_offset ; <nl> + int is_alpha , av_uninit ( alpha_offset ); <nl>  <nl> if ( s -> has_alpha ) { <nl> alpha_offset = bytestream_get_be24 (& buf );
static int swf_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> tag = get_swf_tag ( pb , & len ); <nl> if ( tag < 0 ) <nl> return tag ; <nl> + if ( len < 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " len % d is invalid \ n ", len ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( tag == TAG_VIDEOSTREAM ) { <nl> int ch_id = avio_rl16 ( pb ); <nl> len -= 2 ;
ogg_gptopts ( AVFormatContext * s , int i , uint64_t gp , int64_t * dts ) <nl> if ( dts ) <nl> * dts = pts ; <nl> } <nl> + if ( pts > INT64_MAX && pts != AV_NOPTS_VALUE ) { <nl> + // The return type is unsigned , we thus cannot return negative pts <nl> + av_log ( s , AV_LOG_ERROR , " invalid pts %" PRId64 "\ n ", pts ); <nl> + pts = AV_NOPTS_VALUE ; <nl> + } <nl>  <nl> return pts ; <nl> }
static int probe_buf_write ( void * opaque , uint8_t * buf , int buf_size ) <nl> int main ( int argc , char ** argv ) <nl> { <nl> int ret ; <nl> - uint8_t * buffer = av_malloc ( AVP_BUFFSIZE ); <nl> + uint8_t * buffer = av_mallocz ( AVP_BUFFSIZE ); <nl>  <nl> if (! buffer ) <nl> exit ( 1 );
static void vc1_mc_4mv_chroma4 ( VC1Context * v ) <nl> uvmy_field [ i ] = ( uvmy_field [ i ] & 3 ) << 1 ; <nl>  <nl> if ( fieldmv && !( uvsrc_y & 1 )) <nl> - v_edge_pos --; <nl> + v_edge_pos = ( s -> v_edge_pos >> 1 ) - 1 ; <nl> + <nl> if ( fieldmv && ( uvsrc_y & 1 ) && uvsrc_y < 2 ) <nl> uvsrc_y --; <nl> if (( v -> mv_mode == MV_PMODE_INTENSITY_COMP )
static int mov_read_header ( AVFormatContext * s ) <nl> } <nl> if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO && sc -> nb_frames_for_fps > 0 && sc -> duration_for_fps > 0 ) <nl> av_reduce (& st -> avg_frame_rate . num , & st -> avg_frame_rate . den , <nl> - sc -> time_scale * sc -> nb_frames_for_fps , sc -> duration_for_fps , INT_MAX ); <nl> + sc -> time_scale *( int64_t ) sc -> nb_frames_for_fps , sc -> duration_for_fps , INT_MAX ); <nl> } <nl>  <nl> if ( mov -> trex_data ) {
static av_cold void uninit ( AVFilterContext * ctx ) <nl> FrameRateContext * s = ctx -> priv ; <nl> int i ; <nl>  <nl> - for ( i = s -> frst + 1 ; i < s -> last ; i ++) { <nl> + for ( i = s -> frst ; i < s -> last ; i ++) { <nl> if ( s -> srce [ i ] && ( s -> srce [ i ] != s -> srce [ i + 1 ])) <nl> av_frame_free (& s -> srce [ i ]); <nl> }
static void output_packet ( OutputFile * of , AVPacket * pkt , OutputStream * ost ) <nl> if ( ost -> nb_bitstream_filters ) { <nl> int idx ; <nl>  <nl> + av_packet_split_side_data ( pkt ); <nl> ret = av_bsf_send_packet ( ost -> bsf_ctx [ 0 ], pkt ); <nl> if ( ret < 0 ) <nl> goto finish ;
# include " libavutil / pixdesc . h " <nl> # include " avcodec . h " <nl> # include " internal . h " <nl> + <nl> +# if defined ( _MSC_VER ) <nl> +# define X264_API_IMPORTS 1 <nl> +# endif <nl> + <nl> # include < x264 . h > <nl> # include < float . h > <nl> # include < math . h >
static int bmp_decode_frame ( AVCodecContext * avctx , <nl>  <nl> hsize = bytestream_get_le32 (& buf ); /* header size */ <nl> ihsize = bytestream_get_le32 (& buf ); /* more header size */ <nl> - if ( ihsize + 14 > hsize ) { <nl> + if ( ihsize + 14LL > hsize ) { <nl> av_log ( avctx , AV_LOG_ERROR , " invalid header size % u \ n ", hsize ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int config_input ( AVFilterLink * inlink ) <nl> const AVPixFmtDescriptor * desc = av_pix_fmt_desc_get ( inlink -> format ); <nl> int i ; <nl>  <nl> + uninit ( inlink -> dst ); <nl> + <nl> s -> hsub = desc -> log2_chroma_w ; <nl> s -> vsub = desc -> log2_chroma_h ; <nl> s -> depth = desc -> comp [ 0 ]. depth_minus1 + 1 ;
static void decode_lpc ( int32_t * coeffs , int mode , int length ) <nl> unsigned a1 = * coeffs ++; <nl> for ( i = 0 ; i < length - 1 >> 1 ; i ++) { <nl> * coeffs += a1 ; <nl> - coeffs [ 1 ] += * coeffs ; <nl> + coeffs [ 1 ] += ( unsigned )* coeffs ; <nl> a1 = coeffs [ 1 ]; <nl> coeffs += 2 ; <nl> }
static int vqf_read_header ( AVFormatContext * s ) <nl> rate_flag = AV_RB32 ( comm_chunk + 8 ); <nl> avio_skip ( s -> pb , len - 12 ); <nl>  <nl> + if ( st -> codec -> channels <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> st -> codec -> bit_rate = read_bitrate * 1000 ; <nl> break ; <nl> case MKTAG (' D ',' S ',' I ',' Z '): // size of compressed data
ogg_get_length ( AVFormatContext * s ) <nl> url_fseek (& s -> pb , end , SEEK_SET ); <nl>  <nl> while (! ogg_read_page ( s , & i )){ <nl> - if ( ogg -> streams [ i ]. granule != - 1 && ogg -> streams [ i ]. granule != 0 ) <nl> + if ( ogg -> streams [ i ]. granule != - 1 && ogg -> streams [ i ]. granule != 0 && <nl> + ogg -> streams [ i ]. codec ) <nl> idx = i ; <nl> } <nl> 
static int mp2_write_trailer ( struct AVFormatContext * s ) <nl> return 0 ; <nl> } <nl>  <nl> - static int query_codec ( enum CodecID id , int std_compliance ) <nl> + static int query_codec ( enum AVCodecID id , int std_compliance ) <nl> { <nl> CodecMime * cm = ff_id3v2_mime_tags ; <nl> - while ( cm -> id != CODEC_ID_NONE ) { <nl> + while ( cm -> id != AV_CODEC_ID_NONE ) { <nl> if ( id == cm -> id ) <nl> return MKTAG (' A ', ' P ', ' I ', ' C '); <nl> cm ++;
static int dvdsub_parse ( AVCodecParserContext * s , <nl>  <nl> if ( pc -> packet_index == 0 ) { <nl> if ( buf_size < 2 ) <nl> - return 0 ; <nl> + return buf_size ; <nl> pc -> packet_len = AV_RB16 ( buf ); <nl> if ( pc -> packet_len == 0 ) /* HD - DVD subpicture packet */ <nl> pc -> packet_len = AV_RB32 ( buf + 2 );
static int yop_read_header ( AVFormatContext * s ) <nl>  <nl> audio_stream = avformat_new_stream ( s , NULL ); <nl> video_stream = avformat_new_stream ( s , NULL ); <nl> + if (! audio_stream || ! video_stream ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> // Extra data that will be passed to the decoder <nl> video_stream -> codec -> extradata_size = 8 ;
static const AVCodecDescriptor codec_descriptors [] = { <nl> . type = AVMEDIA_TYPE_VIDEO , <nl> . name = " fraps ", <nl> . long_name = NULL_IF_CONFIG_SMALL (" Fraps "), <nl> - . props = AV_CODEC_PROP_LOSSLESS , <nl> + . props = AV_CODEC_PROP_INTRA_ONLY | AV_CODEC_PROP_LOSSLESS , <nl> }, <nl> { <nl> . id = AV_CODEC_ID_TRUEMOTION2 ,
static int decode_residual_block ( AVSContext * h , GetBitContext * gb , <nl> const dec_2dvlc_t * r , int esc_golomb_order , <nl> int qp , uint8_t * dst , int stride ) { <nl> int i , level_code , esc_code , level , run , mask ; <nl> - DCTELEM level_buf [ 64 ]; <nl> - uint8_t run_buf [ 64 ]; <nl> + DCTELEM level_buf [ 65 ]; <nl> + uint8_t run_buf [ 65 ]; <nl> DCTELEM * block = h -> block ; <nl>  <nl> for ( i = 0 ; i < 65 ; i ++) {
static int read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> } <nl> } <nl>  <nl> + if ( s -> pb -> eof_reached ) <nl> + return AVERROR_EOF ; <nl> + <nl> return AVERROR ( EIO ); <nl> } <nl> 
int main ( int argc , char ** argv ){ <nl>  <nl> selfTest ( src , stride , W , H ); <nl>  <nl> - return 123 ; <nl> + return 0 ; <nl> }
static int read_channel_params ( MLPDecodeContext * m , unsigned int substr , <nl>  <nl> if ( cp -> huff_lsbs > 24 ) { <nl> av_log ( m -> avctx , AV_LOG_ERROR , " Invalid huff_lsbs .\ n "); <nl> + cp -> huff_lsbs = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
typedef struct PanContext { <nl> static int parse_channel_name ( char ** arg , int * rchannel , int * rnamed ) <nl> { <nl> char buf [ 8 ]; <nl> - int len , i , channel_id ; <nl> + int len , i , channel_id = 0 ; <nl> int64_t layout , layout0 ; <nl>  <nl> if ( sscanf (* arg , " % 7 [ A - Z ] % n ", buf , & len )) {
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl> break ; <nl> } <nl>  <nl> - if ( last ){ <nl> + if ( last && s -> current_picture_ptr ){ <nl> if ( r -> loop_filter ) <nl> r -> loop_filter ( r , s -> mb_height - 1 ); <nl> ff_er_frame_end ( s );
static int rtp_parse_packet_internal ( RTPDemuxContext * s , AVPacket * pkt , <nl> if ( ret < 0 ) <nl> return AVERROR ( EAGAIN ); <nl> if ( ret < len ) { <nl> - s -> read_buf_size = len - ret ; <nl> + s -> read_buf_size = FFMIN ( len - ret , sizeof ( s -> buf )); <nl> memcpy ( s -> buf , buf + ret , s -> read_buf_size ); <nl> s -> read_buf_index = 0 ; <nl> return 1 ;
static int decode_thread ( void * arg ) <nl> if ( ret < 0 ) { <nl> if ( ret == AVERROR_EOF || url_feof ( ic -> pb )) <nl> eof = 1 ; <nl> - if ( ic -> pb -> error ) <nl> + if ( ic -> pb && ic -> pb -> error ) <nl> break ; <nl> SDL_Delay ( 100 ); /* wait for user event */ <nl> continue ;
static int decode_frame ( AVCodecContext * avctx , <nl> int prev_y = 0 , prev_u = 0 , prev_v = 0 ; <nl> uint8_t * rbuf ; <nl>  <nl> - if ( buf_size <= 8 ) { <nl> + if ( buf_size < 8 + avctx -> height * ( avctx -> width / 2 )/ 8 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Packet size % d is too small \ n ", buf_size ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int resolve_content_path ( AVFormatContext * s , const char * url , int * max_ur <nl> if (!( node = baseurl_nodes [ rootId ])) { <nl> continue ; <nl> } <nl> - if ( ishttp ( xmlNodeGetContent ( node ))) { <nl> + text = xmlNodeGetContent ( node ); <nl> + if ( ishttp ( text )) { <nl> + xmlFree ( text ); <nl> break ; <nl> } <nl> + xmlFree ( text ); <nl> } <nl>  <nl> node = baseurl_nodes [ rootId ];
int ff_vda_destroy_decoder ( struct vda_context * vda_ctx ) <nl> static int vda_h264_uninit ( AVCodecContext * avctx ) <nl> { <nl> VDAContext * vda = avctx -> internal -> hwaccel_priv_data ; <nl> - av_freep (& vda -> bitstream ); <nl> - if ( vda -> frame ) <nl> - CVPixelBufferRelease ( vda -> frame ); <nl> + if ( vda ) { <nl> + av_freep (& vda -> bitstream ); <nl> + if ( vda -> frame ) <nl> + CVPixelBufferRelease ( vda -> frame ); <nl> + } <nl> return 0 ; <nl> } <nl> 
char * av_strdup ( const char * s ) <nl> char * ptr = NULL ; <nl> if ( s ) { <nl> int len = strlen ( s ) + 1 ; <nl> - ptr = av_malloc ( len ); <nl> + ptr = av_realloc ( NULL , len ); <nl> if ( ptr ) <nl> memcpy ( ptr , s , len ); <nl> }
static int adx_decode_frame ( AVCodecContext * avctx , void * data , <nl> buf_size -= BLOCK_SIZE ; <nl> buf += BLOCK_SIZE ; <nl> } <nl> - samples_offset += BLOCK_SAMPLES ; <nl> + if (! c -> eof ) <nl> + samples_offset += BLOCK_SAMPLES ; <nl> } <nl>  <nl> + frame -> nb_samples = samples_offset ; <nl> * got_frame_ptr = 1 ; <nl>  <nl> return buf - avpkt -> data ;
static int mkv_write_attachments ( AVFormatContext * s ) <nl>  <nl> mkv -> attachments = av_mallocz ( sizeof (* mkv -> attachments )); <nl> if (! mkv -> attachments ) <nl> - return ret ; <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> av_lfg_init (& c , av_get_random_seed ()); <nl> 
static int vdpau_hevc_start_frame ( AVCodecContext * avctx , <nl> const HEVCFrame * frame = & h -> DPB [ i ]; <nl> if ( frame != h -> ref && ( frame -> flags & ( HEVC_FRAME_FLAG_LONG_REF | <nl> HEVC_FRAME_FLAG_SHORT_REF ))) { <nl> - if ( j > 16 ) { <nl> + if ( j > 15 ) { <nl> av_log ( avctx , AV_LOG_WARNING , <nl> " VDPAU only supports up to 16 references in the DPB . " <nl> " This frame may not be decoded correctly .\ n ");
static int matroska_read_seek ( AVFormatContext * s , int stream_index , <nl> avio_seek ( s -> pb , st -> index_entries [ st -> nb_index_entries - 1 ]. pos , SEEK_SET ); <nl> matroska -> current_id = 0 ; <nl> while (( index = av_index_search_timestamp ( st , timestamp , flags )) < 0 ) { <nl> + matroska -> prev_pkt = NULL ; <nl> matroska_clear_queue ( matroska ); <nl> if ( matroska_parse_cluster ( matroska ) < 0 ) <nl> break ;
int64_t avio_seek ( AVIOContext * s , int64_t offset , int whence ) <nl> offset1 = pos + ( s -> buf_ptr - s -> buffer ); <nl> if ( offset == 0 ) <nl> return offset1 ; <nl> + if ( offset > INT64_MAX - offset1 ) <nl> + return AVERROR ( EINVAL ); <nl> offset += offset1 ; <nl> } <nl> if ( offset < 0 )
static int cfhd_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> output = s -> plane [ plane ]. subband [ 0 ]; <nl> for ( i = 0 ; i < lowpass_height * 2 ; i ++) { <nl> for ( j = 0 ; j < lowpass_width * 2 ; j ++) <nl> - output [ j ] <<= 2 ; <nl> + output [ j ] *= 4 ; <nl>  <nl> output += lowpass_width * 2 ; <nl> }
static int find_image_range ( int * pfirst_index , int * plast_index , <nl> if ( avio_check ( buf , AVIO_FLAG_READ ) > 0 ) <nl> break ; <nl> } <nl> - if ( first_index == 5 ) <nl> + if ( first_index == start_index + 5 ) <nl> goto fail ; <nl>  <nl> /* find the last image */
int ff_h264_decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> } <nl> } <nl>  <nl> - if ( h == h0 && h -> dequant_coeff_pps != pps_id ) { <nl> + if ( first_slice && h -> dequant_coeff_pps != pps_id ) { <nl> h -> dequant_coeff_pps = pps_id ; <nl> h264_init_dequant_tables ( h ); <nl> }
static void read_tree ( GetBitContext * gb , Tree * tree ) <nl> tree -> syms [ i ] = get_bits ( gb , 4 ); <nl> tmp1 [ tree -> syms [ i ]] = 1 ; <nl> } <nl> - for ( i = 0 ; i < 16 ; i ++) <nl> + for ( i = 0 ; i < 16 && len < 16 - 1 ; i ++) <nl> if (! tmp1 [ i ]) <nl> tree -> syms [++ len ] = i ; <nl> } else {
static int dfa_probe ( AVProbeData * p ) <nl> if ( p -> buf_size < 4 || AV_RL32 ( p -> buf ) != MKTAG (' D ', ' F ', ' I ', ' A ')) <nl> return 0 ; <nl>  <nl> + if ( AV_RL32 ( p -> buf + 16 ) != 0x80 ) <nl> + return AVPROBE_SCORE_MAX / 4 ; <nl> + <nl> return AVPROBE_SCORE_MAX ; <nl> } <nl> 
typedef struct AVStream { <nl> int codec_info_nb_frames ; <nl> # endif <nl> /** encoding : PTS generation when outputing stream */ <nl> - AVFrac pts ; <nl> + struct AVFrac pts ; <nl>  <nl> /** <nl> * this is the fundamental unit of time ( in seconds ) in terms
int ff_mpv_reallocate_putbitbuffer ( MpegEncContext * s , size_t threshold , size_t s <nl> uint8_t * new_buffer = NULL ; <nl> int new_buffer_size = 0 ; <nl>  <nl> + if (( s -> avctx -> internal -> byte_buffer_size + size_increase ) >= INT_MAX / 8 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " Cannot reallocate putbit buffer \ n "); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> + <nl> av_fast_padded_malloc (& new_buffer , & new_buffer_size , <nl> s -> avctx -> internal -> byte_buffer_size + size_increase ); <nl> if (! new_buffer )
static int jp2_find_codestream ( Jpeg2000DecoderContext * s ) <nl> atom2_end = bytestream2_tell (& s -> g ) + atom2_size - 8 ; <nl> if ( atom2_size < 8 || atom2_end > atom_end || atom2_end < atom2_size ) <nl> break ; <nl> + atom2_size -= 8 ; <nl> if ( atom2 == JP2_CODESTREAM ) { <nl> return 1 ; <nl> } else if ( atom2 == MKBETAG (' c ',' o ',' l ',' r ') && atom2_size >= 7 ) {
SchroFrame * ff_create_schro_frame ( AVCodecContext * avccontext , <nl> uv_height = y_height >> ( SCHRO_FRAME_FORMAT_V_SHIFT ( schro_frame_fmt )); <nl>  <nl> p_pic = av_mallocz ( sizeof ( AVPicture )); <nl> - avpicture_alloc ( p_pic , avccontext -> pix_fmt , y_width , y_height ); <nl> + if (! p_pic || avpicture_alloc ( p_pic , avccontext -> pix_fmt , y_width , y_height ) < 0 ) { <nl> + av_free ( p_pic ); <nl> + return NULL ; <nl> + } <nl>  <nl> p_frame = schro_frame_new (); <nl> p_frame -> format = schro_frame_fmt ;
static int get_dimension ( GetBitContext * gb , const int * dim ) <nl> val = dim [ get_bits1 ( gb ) - val ]; <nl> if (! val ){ <nl> do { <nl> + if ( get_bits_left ( gb ) < 8 ) <nl> + return AVERROR_INVALIDDATA ; <nl> t = get_bits ( gb , 8 ); <nl> val += t << 2 ; <nl> } while ( t == 0xFF );
static int cmv_decode_frame ( AVCodecContext * avctx , <nl> CmvContext * s = avctx -> priv_data ; <nl> const uint8_t * buf_end = buf + buf_size ; <nl>  <nl> + if ( buf_end - buf < EA_PREAMBLE_SIZE ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( AV_RL32 ( buf )== MVIh_TAG || AV_RB32 ( buf )== MVIh_TAG ) { <nl> cmv_process_header ( s , buf + EA_PREAMBLE_SIZE , buf_end ); <nl> return buf_size ;
AVStream * avformat_new_stream ( AVFormatContext * s , const AVCodec * c ) <nl> } <nl>  <nl> st -> codec = avcodec_alloc_context3 ( c ); <nl> + if (! st -> codec ) { <nl> + av_free ( st -> info ); <nl> + av_free ( st ); <nl> + return NULL ; <nl> + } <nl> if ( s -> iformat ) { <nl> /* no default bitrate if decoding */ <nl> st -> codec -> bit_rate = 0 ;
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> { <nl> libx265Context * ctx = avctx -> priv_data ; <nl> x265_picture x265pic ; <nl> - x265_picture x265pic_out = { { 0 } }; <nl> + x265_picture x265pic_out = { 0 }; <nl> x265_nal * nal ; <nl> uint8_t * dst ; <nl> int payload = 0 ;
void avformat_free_context ( AVFormatContext * s ) <nl> av_dict_free (& s -> metadata ); <nl> av_dict_free (& s -> internal -> id3v2_meta ); <nl> av_freep (& s -> streams ); <nl> - av_freep (& s -> internal ); <nl> flush_packet_queue ( s ); <nl> + av_freep (& s -> internal ); <nl> av_free ( s ); <nl> } <nl> 
int vc1_parse_frame_header_adv ( VC1Context * v , GetBitContext * gb ) <nl> v -> rnd = get_bits1 ( gb ); <nl> if ( v -> interlace ) <nl> v -> uvsamp = get_bits1 ( gb ); <nl> + if (! ff_vc1_bfraction_vlc . table ) <nl> + return 0 ; // parsing only , vlc tables havnt been allocated <nl> if ( v -> field_mode ) { <nl> if (! v -> refdist_flag ) <nl> v -> refdist = 0 ;
static int parse_playlist ( HLSContext * c , const char * url , <nl> uint8_t iv [ 16 ] = ""; <nl> int has_iv = 0 ; <nl> char key [ MAX_URL_SIZE ] = ""; <nl> - char line [ 1024 ]; <nl> + char line [ MAX_URL_SIZE ]; <nl> const char * ptr ; <nl> int close_in = 0 ; <nl> 
static inline int msmpeg4_decode_block ( MpegEncContext * s , DCTELEM * block , <nl> if ( i > 62 ){ <nl> i -= 192 ; <nl> if ( i &(~ 63 )){ <nl> - if ( s -> error_resilience < 0 ){ <nl> + if (( i + 192 == 64 && level / qmul ==- 1 ) || s -> error_resilience < 0 ){ <nl> fprintf ( stderr , " ignoring overflow at % d % d \ n ", s -> mb_x , s -> mb_y ); <nl> break ; <nl> } else {
static void gif_copy_img_rect ( const uint32_t * src , uint32_t * dst , <nl> const uint32_t * src_px , * src_pr , <nl> * src_py = src + y_start , <nl> * dst_py = dst + y_start ; <nl> - const uint32_t * src_pb = src_py + ( t + h ) * linesize ; <nl> + const uint32_t * src_pb = src_py + t * linesize ; <nl> uint32_t * dst_px ; <nl>  <nl> for (; src_py < src_pb ; src_py += linesize , dst_py += linesize ) {
static int unpack_bitstream ( G723_1_Context * p , const uint8_t * buf , <nl> /** <nl> * Bitexact implementation of sqrt ( val / 2 ). <nl> */ <nl> - static int16_t square_root ( int val ) <nl> + static int16_t square_root ( unsigned val ) <nl> { <nl> + av_assert2 (!( val & 0x80000000 )); <nl> + <nl> return ( ff_sqrt ( val << 1 ) >> 1 ) & (~ 1 ); <nl> } <nl> 
static int zerocodec_decode_frame ( AVCodecContext * avctx , void * data , <nl> pic -> key_frame = 1 ; <nl> pic -> pict_type = AV_PICTURE_TYPE_I ; <nl> } else { <nl> + if ( prev == NULL ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " No previous frame !\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> pic -> key_frame = 0 ; <nl> pic -> pict_type = AV_PICTURE_TYPE_P ; <nl> }
static int vp8_lossy_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if (!* got_frame ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> update_canvas_size ( avctx , avctx -> width , avctx -> height ); <nl>  <nl> if ( s -> has_alpha ) {
ogm_header ( AVFormatContext * s , int idx ) <nl>  <nl> time_unit = bytestream2_get_le64 (& p ); <nl> spu = bytestream2_get_le64 (& p ); <nl> + if (! time_unit || ! spu ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid timing values .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> bytestream2_skip (& p , 4 ); /* default_len */ <nl> bytestream2_skip (& p , 8 ); /* buffersize + bits_per_sample */ <nl> 
static int read_packet ( AVFormatContext * s , <nl> return 0 ; <nl>  <nl> case MM_TYPE_AUDIO : <nl> + if ( s -> nb_streams != 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , <nl> + " Unexpected audio packet , skipping \ n "); <nl> + avio_skip ( pb , length ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( av_get_packet ( s -> pb , pkt , length )< 0 ) <nl> return AVERROR ( ENOMEM ); <nl> pkt -> size = length ;
int ff_mjpeg_decode_sof ( MJpegDecodeContext * s ) <nl>  <nl> if ( s -> v_max == 1 && s -> h_max == 1 && s -> lossless == 1 && nb_components == 3 ) <nl> s -> rgb = 1 ; <nl> + else if (! s -> lossless ) <nl> + s -> rgb = 0 ; <nl>  <nl> /* if different size , realloc / alloc picture */ <nl> if ( width != s -> width || height != s -> height
static int paf_video_decode ( AVCodecContext * avctx , void * data , <nl> bytestream2_init (& c -> gb , pkt -> data , pkt -> size ); <nl>  <nl> code = bytestream2_get_byte (& c -> gb ); <nl> - if (( code & 0xF ) > 4 ) { <nl> + if (( code & 0xF ) > 4 || ( code & 0xF ) == 3 ) { <nl> avpriv_request_sample ( avctx , " unknown / invalid code "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int dnxhd_decode_header ( DNXHDContext * ctx , AVFrame * frame , <nl> frame -> top_field_first = first_field ^ ctx -> cur_field ; <nl> av_log ( ctx -> avctx , AV_LOG_DEBUG , <nl> " interlaced % d , cur field % d \ n ", buf [ 5 ] & 3 , ctx -> cur_field ); <nl> + } else { <nl> + ctx -> cur_field = 0 ; <nl> } <nl>  <nl> ctx -> height = AV_RB16 ( buf + 0x18 );
static av_cold int vc2_encode_init ( AVCodecContext * avctx ) <nl> s -> num_x = s -> plane [ 0 ]. dwt_width / s -> slice_width ; <nl> s -> num_y = s -> plane [ 0 ]. dwt_height / s -> slice_height ; <nl>  <nl> - s -> slice_args = av_malloc ( s -> num_x * s -> num_y * sizeof ( SliceArgs )); <nl> + s -> slice_args = av_calloc ( s -> num_x * s -> num_y , sizeof ( SliceArgs )); <nl> if (! s -> slice_args ) <nl> goto alloc_fail ; <nl> 
static void FUNC ( dequant )( int16_t * coeffs , int16_t log2_size ) <nl> } else { <nl> for ( y = 0 ; y < size ; y ++) { <nl> for ( x = 0 ; x < size ; x ++) { <nl> - * coeffs = * coeffs << - shift ; <nl> + * coeffs = *( uint16_t *) coeffs << - shift ; <nl> coeffs ++; <nl> } <nl> }
static int hevc_init ( AVCodecParserContext * s ) <nl> { <nl> HEVCContext * h = &(( HEVCParseContext *) s -> priv_data )-> h ; <nl> h -> HEVClc = av_mallocz ( sizeof ( HEVCLocalContext )); <nl> + if (! h -> HEVClc ) <nl> + return AVERROR ( ENOMEM ); <nl> h -> skipped_bytes_pos_size = INT_MAX ; <nl>  <nl> return 0 ;
static int smvjpeg_decode_frame ( AVCodecContext * avctx , void * data , int * data_siz <nl>  <nl> cur_frame = avpkt -> pts % s -> frames_per_jpeg ; <nl>  <nl> + /* cur_frame is later used to calculate the buffer offset , so it mustn ' t be negative */ <nl> + if ( cur_frame < 0 ) <nl> + cur_frame += s -> frames_per_jpeg ; <nl> + <nl> /* Are we at the start of a block ? */ <nl> if (! cur_frame ) { <nl> av_frame_unref ( mjpeg_data );
static int commit_bitstream_and_slice_buffer ( AVCodecContext * avctx , <nl> const H264Picture * current_picture = h -> cur_pic_ptr ; <nl> struct dxva2_picture_context * ctx_pic = current_picture -> hwaccel_picture_private ; <nl> DXVA_Slice_H264_Short * slice = NULL ; <nl> - void * dxva_data_ptr ; <nl> + void * dxva_data_ptr = NULL ; <nl> uint8_t * dxva_data , * current , * end ; <nl> - unsigned dxva_size ; <nl> + unsigned dxva_size = 0 ; <nl> void * slice_data ; <nl> unsigned slice_size ; <nl> unsigned padding ;
static int request_frame ( AVFilterLink * outlink ) <nl> TestSourceContext * test = outlink -> src -> priv ; <nl> AVFilterBufferRef * picref ; <nl>  <nl> - if ( test -> max_pts >= 0 && test -> pts > test -> max_pts ) <nl> + if ( test -> max_pts >= 0 && test -> pts >= test -> max_pts ) <nl> return AVERROR_EOF ; <nl> picref = avfilter_get_video_buffer ( outlink , AV_PERM_WRITE , <nl> test -> w , test -> h );
static int webm_dash_manifest_cues ( AVFormatContext * s , int64_t init_range ) <nl> "%" PRId64 , s -> streams [ 0 ]-> index_entries [ i ]. timestamp ); <nl> if ( ret <= 0 || ( ret == 20 && i == s -> streams [ 0 ]-> nb_index_entries - 1 )) { <nl> av_log ( s , AV_LOG_ERROR , " timestamp too long .\ n "); <nl> + av_free ( buf ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> end += ret ;
void checkasm_stack_clobber ( uint64_t clobber , ...); <nl> }\ <nl> } while ( 0 ) <nl> # else <nl> -# define bench_new (...) <nl> +# define bench_new (...) while ( 0 ) <nl> # endif <nl>  <nl> # endif
AVBitStreamFilterContext * av_bitstream_filter_init ( const char * name ){ <nl> if (! strcmp ( name , bsf -> name )){ <nl> AVBitStreamFilterContext * bsfc = av_mallocz ( sizeof ( AVBitStreamFilterContext )); <nl> bsfc -> filter = bsf ; <nl> - bsfc -> priv_data = av_mallocz ( bsf -> priv_data_size ); <nl> + bsfc -> priv_data = bsf -> priv_data_size ? av_mallocz ( bsf -> priv_data_size ) : NULL ; <nl> return bsfc ; <nl> } <nl> bsf = bsf -> next ;
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> s -> subsampling [ i ] = ff_tget (& s -> gb , type , s -> le ); <nl> if ( s -> subsampling [ i ] <= 0 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " subsampling % d is invalid \ n ", s -> subsampling [ i ]); <nl> + s -> subsampling [ i ] = 1 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> }
static int cfhd_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> s -> level ++; <nl> av_log ( avctx , AV_LOG_DEBUG , " Subband number %" PRIu16 "\ n ", data ); <nl> s -> subband_num = data ; <nl> - if ( s -> level > DWT_LEVELS ) { <nl> + if ( s -> level >= DWT_LEVELS ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Invalid level \ n "); <nl> ret = AVERROR ( EINVAL ); <nl> break ;
static int dxv_decompress_dxt5 ( AVCodecContext * avctx ) <nl> AV_WL32 ( ctx -> tex_data + 4 * pos , prev ); <nl> pos ++; <nl> } else { <nl> + if ( bytestream2_get_bytes_left ( gbc ) < 1 ) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( state == 0 ) { <nl> value = bytestream2_get_le32 ( gbc ); <nl> state = 16 ;
static int parse_palette ( AVCodecContext * avctx , GetByteContext * gbc , <nl> bytestream2_skip ( gbc , 1 ); <nl> b = bytestream2_get_byte ( gbc ); <nl> bytestream2_skip ( gbc , 1 ); <nl> - pal [ idx ] = ( r << 16 ) | ( g << 8 ) | b ; <nl> + pal [ idx ] = ( 0xFFU << 24 ) | ( r << 16 ) | ( g << 8 ) | b ; <nl> } <nl> return 0 ; <nl> }
static int sp5x_decode_frame ( AVCodecContext * avctx , <nl> recoded [ j ++] = 0xFF ; <nl> recoded [ j ++] = 0xD9 ; <nl>  <nl> - avctx -> flags &= ~ CODEC_FLAG_EMU_EDGE ; <nl> av_init_packet (& avpkt_recoded ); <nl> avpkt_recoded . data = recoded ; <nl> avpkt_recoded . size = j ; <nl> AVCodec ff_amv_decoder = { <nl> NULL , <nl> ff_mjpeg_decode_end , <nl> sp5x_decode_frame , <nl> - CODEC_CAP_DR1 , <nl> + 0 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" AMV Video "), <nl> };
static void decode_postinit ( H264Context * h , int setup_finished ){ <nl>  <nl> if ( s -> avctx -> strict_std_compliance >= FF_COMPLIANCE_STRICT <nl> && ! h -> sps . bitstream_restriction_flag ){ <nl> - s -> avctx -> has_b_frames = MAX_DELAYED_PIC_COUNT ; <nl> + s -> avctx -> has_b_frames = MAX_DELAYED_PIC_COUNT - 1 ; <nl> s -> low_delay = 0 ; <nl> } <nl> 
int ff_ac3_bit_alloc_calc_mask ( AC3BitAllocParameters * s , int16_t * band_psd , <nl> int band_start , band_end , begin , end1 ; <nl> int lowcomp , fastleak , slowleak ; <nl>  <nl> + if ( end <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> /* excitation function */ <nl> band_start = ff_ac3_bin_to_band_tab [ start ]; <nl> band_end = ff_ac3_bin_to_band_tab [ end - 1 ] + 1 ;
static int http_open_cnx ( URLContext * h ) <nl>  <nl> if (! strcmp ( proto , " https ")) { <nl> lower_proto = " tls "; <nl> + use_proxy = 0 ; <nl> if ( port < 0 ) <nl> port = 443 ; <nl> }
static int mov_read_close ( AVFormatContext * s ) <nl> MOVContext * mov = ( MOVContext *) s -> priv_data ; <nl> for ( i = 0 ; i < mov -> total_streams ; i ++) <nl> mov_free_stream_context ( mov -> streams [ i ]); <nl> - for ( i = 0 ; i < s -> nb_streams ; i ++) <nl> - av_freep (& s -> streams [ i ]); <nl> /* free color tabs */ <nl> for ( i = 0 ; i < mov -> ctab_size ; i ++) <nl> av_freep (& mov -> ctab [ i ]);
static int hls_slice_header ( HEVCContext * s ) <nl>  <nl> if ( s -> pps -> slice_header_extension_present_flag ) { <nl> unsigned int length = get_ue_golomb_long ( gb ); <nl> + if ( length * 8LL > get_bits_left ( gb )) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too many slice_header_extension_data_bytes \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> for ( i = 0 ; i < length ; i ++) <nl> skip_bits ( gb , 8 ); // slice_header_extension_data_byte <nl> }
void ff_mpeg_set_erpic ( ERPicture * dst , Picture * src ) <nl> { <nl> int i ; <nl>  <nl> - if (! src ) <nl> + if (! src ) { <nl> + dst -> f = NULL ; <nl> + dst -> tf = NULL ; <nl> return ; <nl> + } <nl>  <nl> dst -> f = src -> f ; <nl> dst -> tf = & src -> tf ;
static int get_metadata_size ( const uint8_t * buf , int buf_size ) <nl>  <nl> buf += 4 ; <nl> do { <nl> + if ( buf_end - buf < 4 ) <nl> + return 0 ; <nl> ff_flac_parse_block_header ( buf , & metadata_last , NULL , & metadata_size ); <nl> buf += 4 ; <nl> - if ( buf + metadata_size > buf_end ) { <nl> + if ( buf_end - buf < metadata_size ) { <nl> /* need more data in order to read the complete header */ <nl> return 0 ; <nl> }
static void read_tree ( GetBitContext * gb , Tree * tree ) <nl> tree -> syms [ i ] = get_bits ( gb , 4 ); <nl> tmp1 [ tree -> syms [ i ]] = 1 ; <nl> } <nl> - for ( i = 0 ; i < 16 ; i ++) <nl> + for ( i = 0 ; i < 16 && len < 16 - 1 ; i ++) <nl> if (! tmp1 [ i ]) <nl> tree -> syms [++ len ] = i ; <nl> } else {
hvn_nvs_cmd ( struct hvn_softc * sc , void * cmd , size_t cmdsize , uint64_t tid , <nl> return ( rv ); <nl> } <nl>  <nl> + if ( timo == 0 ) <nl> + return ( 0 ); <nl> + <nl> do { <nl> if ( cold ) <nl> delay ( 1000 );
int unix_listen_opts ( QemuOpts * opts , Error ** errp ) <nl> qemu_opt_set ( opts , " path ", un . sun_path , & error_abort ); <nl> } <nl>  <nl> - if (( access ( un . sun_path , F_OK ) == 0 ) && <nl> - unlink ( un . sun_path ) < 0 ) { <nl> + if ( unlink ( un . sun_path ) < 0 && errno != ENOENT ) { <nl> error_setg_errno ( errp , errno , <nl> " Failed to unlink socket % s ", un . sun_path ); <nl> goto err ;
static void qvirtio_scsi_pci_free ( QVirtIOSCSI * vs ) <nl> qvirtqueue_cleanup ( vs -> dev -> bus , vs -> vq [ i ], vs -> qs -> alloc ); <nl> } <nl> qvirtio_pci_device_disable ( container_of ( vs -> dev , QVirtioPCIDevice , vdev )); <nl> - g_free ( vs -> dev ); <nl> + qvirtio_pci_device_free (( QVirtioPCIDevice *) vs -> dev ); <nl> qvirtio_scsi_stop ( vs -> qs ); <nl> g_free ( vs ); <nl> }
int qemu_acl_remove ( qemu_acl * acl , <nl> i ++; <nl> if ( strcmp ( entry -> match , match ) == 0 ) { <nl> QTAILQ_REMOVE (& acl -> entries , entry , next ); <nl> + acl -> nentries --; <nl> + g_free ( entry -> match ); <nl> + g_free ( entry ); <nl> return i ; <nl> } <nl> }
static target_ulong disas_insn ( DisasContext * s , target_ulong pc_start ) <nl> tval += s -> pc - s -> cs_base ; <nl> if ( s -> dflag == 0 ) <nl> tval &= 0xffff ; <nl> + else if (! CODE64 ( s )) <nl> + tval &= 0xffffffff ; <nl> gen_jmp ( s , tval ); <nl> break ; <nl> case 0xea : /* ljmp im */
static int elf_core_dump ( int signr , const CPUArchState * env ) <nl> phdr . p_align = ELF_EXEC_PAGESIZE ; <nl>  <nl> bswap_phdr (& phdr , 1 ); <nl> - dump_write ( fd , & phdr , sizeof ( phdr )); <nl> + if ( dump_write ( fd , & phdr , sizeof ( phdr )) != 0 ) { <nl> + goto out ; <nl> + } <nl> } <nl>  <nl> /*
static void * qpa_thread_out ( void * arg ) <nl> return NULL ; <nl> } <nl>  <nl> + pa -> live = 0 ; <nl> pa -> rpos = rpos ; <nl> - pa -> live -= decr ; <nl> pa -> decr += decr ; <nl> } <nl> 
static void ehci_detach ( USBPort * port ) <nl> ehci_queues_rip_device ( s , port -> dev , 0 ); <nl> ehci_queues_rip_device ( s , port -> dev , 1 ); <nl>  <nl> - * portsc &= ~( PORTSC_CONNECT | PORTSC_PED ); <nl> + * portsc &= ~( PORTSC_CONNECT | PORTSC_PED | PORTSC_SUSPEND ); <nl> * portsc |= PORTSC_CSC ; <nl>  <nl> ehci_raise_irq ( s , USBSTS_PCD );
int virtio_load ( VirtIODevice * vdev , QEMUFile * f ) <nl> qemu_get_8s ( f , & vdev -> status ); <nl> qemu_get_8s ( f , & vdev -> isr ); <nl> qemu_get_be16s ( f , & vdev -> queue_sel ); <nl> + if ( vdev -> queue_sel >= VIRTIO_PCI_QUEUE_MAX ) { <nl> + return - 1 ; <nl> + } <nl> qemu_get_be32s ( f , & features ); <nl>  <nl> if ( virtio_set_features ( vdev , features ) < 0 ) {
static void esp_pci_dma_memory_rw ( PCIESPState * pci , uint8_t * buf , int len , <nl> /* update status registers */ <nl> pci -> dma_regs [ DMA_WBC ] -= len ; <nl> pci -> dma_regs [ DMA_WAC ] += len ; <nl> + if ( pci -> dma_regs [ DMA_WBC ] == 0 ) <nl> + pci -> dma_regs [ DMA_STAT ] |= DMA_STAT_DONE ; <nl> } <nl>  <nl> static void esp_pci_dma_memory_read ( void * opaque , uint8_t * buf , int len )
void qmp_block_resize ( bool has_device , const char * device , <nl> goto out ; <nl> } <nl>  <nl> - /* complete all in - flight operations before resizing the device */ <nl> - bdrv_drain_all (); <nl> - <nl> + bdrv_drained_begin ( bs ); <nl> ret = blk_truncate ( blk , size , errp ); <nl> + bdrv_drained_end ( bs ); <nl>  <nl> out : <nl> blk_unref ( blk );
int pci_piix3_xen_ide_unplug ( DeviceState * dev ) <nl> { <nl> PCIIDEState * pci_ide ; <nl> DriveInfo * di ; <nl> - int i = 0 ; <nl> + int i ; <nl>  <nl> pci_ide = PCI_IDE ( dev ); <nl>  <nl> - for (; i < 3 ; i ++) { <nl> + for ( i = 0 ; i < 4 ; i ++) { <nl> di = drive_get_by_index ( IF_IDE , i ); <nl> if ( di != NULL && ! di -> media_cd ) { <nl> BlockBackend * blk = blk_by_legacy_dinfo ( di );
static void pmac_ide_atapi_transfer_cb ( void * opaque , int ret ) <nl> } <nl>  <nl> /* Calculate current offset */ <nl> - offset = ( int64_t )( s -> lba << 11 ) + s -> io_buffer_index ; <nl> + offset = (( int64_t ) s -> lba << 11 ) + s -> io_buffer_index ; <nl>  <nl> pmac_dma_read ( s -> blk , offset , io -> len , pmac_ide_atapi_transfer_cb , io ); <nl> return ;
static void dump_qobject ( fprintf_function func_fprintf , void * f , <nl> case QTYPE_QERROR : { <nl> QString * value = qerror_human (( QError *) obj ); <nl> func_fprintf ( f , "% s ", qstring_get_str ( value )); <nl> + QDECREF ( value ); <nl> break ; <nl> } <nl> case QTYPE_NONE :
static void spr_write_excp_prefix ( void * opaque , int sprn , int gprn ) <nl> tcg_gen_and_tl ( t0 , t0 , cpu_gpr [ gprn ]); <nl> tcg_gen_st_tl ( t0 , cpu_env , offsetof ( CPUState , excp_prefix )); <nl> gen_store_spr ( sprn , t0 ); <nl> + tcg_temp_free ( t0 ); <nl> } <nl>  <nl> static void spr_write_excp_vector ( void * opaque , int sprn , int gprn )
static uint64_t do_cvttq ( CPUAlphaState * env , uint64_t a , int roundmode ) <nl> frac = a & 0xfffffffffffffull ; <nl>  <nl> if ( exp == 0 ) { <nl> - if ( unlikely ( frac != 0 )) { <nl> + if ( unlikely ( frac != 0 ) && ! env -> fp_status . flush_inputs_to_zero ) { <nl> goto do_underflow ; <nl> } <nl> } else if ( exp == 0x7ff ) {
static void ncq_cb ( void * opaque , int ret ) <nl> NCQTransferState * ncq_tfs = ( NCQTransferState *) opaque ; <nl> IDEState * ide_state = & ncq_tfs -> drive -> port . ifs [ 0 ]; <nl>  <nl> + ncq_tfs -> aiocb = NULL ; <nl> if ( ret == - ECANCELED ) { <nl> return ; <nl> }
static int qcow2_amend_options ( BlockDriverState * bs , QemuOpts * opts , <nl> } <nl>  <nl> if ( new_size ) { <nl> - ret = bdrv_truncate ( bs , new_size ); <nl> + BlockBackend * blk = blk_new (); <nl> + blk_insert_bs ( blk , bs ); <nl> + ret = blk_truncate ( blk , new_size ); <nl> + blk_unref ( blk ); <nl> + <nl> if ( ret < 0 ) { <nl> return ret ; <nl> }
static int cpu_post_load ( void * opaque , int version_id ) <nl>  <nl> # if defined ( TARGET_PPC64 ) <nl> if ( cpu -> compat_pvr ) { <nl> + uint32_t compat_pvr = cpu -> compat_pvr ; <nl> Error * local_err = NULL ; <nl>  <nl> - ppc_set_compat ( cpu , cpu -> compat_pvr , & local_err ); <nl> + cpu -> compat_pvr = 0 ; <nl> + ppc_set_compat ( cpu , compat_pvr , & local_err ); <nl> if ( local_err ) { <nl> error_report_err ( local_err ); <nl> return - 1 ;
static target_ulong disas_insn ( CPUX86State * env , DisasContext * s , <nl> } <nl> /* fallthru */ <nl> case 0xf9 ... 0xff : /* sfence */ <nl> + if (!( s -> cpuid_features & CPUID_SSE ) <nl> + || ( prefixes & PREFIX_LOCK )) { <nl> + goto illegal_op ; <nl> + } <nl> + break ; <nl> case 0xe8 ... 0xef : /* lfence */ <nl> case 0xf0 ... 0xf7 : /* mfence */ <nl> if (!( s -> cpuid_features & CPUID_SSE2 )
static void build_guest_fsinfo_for_virtual_device ( char const * syspath , <nl> dirpath = g_strdup_printf ("% s / slaves ", syspath ); <nl> dir = opendir ( dirpath ); <nl> if (! dir ) { <nl> - error_setg_errno ( errp , errno , " opendir (\"% s \")", dirpath ); <nl> + if ( errno != ENOENT ) { <nl> + error_setg_errno ( errp , errno , " opendir (\"% s \")", dirpath ); <nl> + } <nl> g_free ( dirpath ); <nl> return ; <nl> }
static void ahci_reg_init ( AHCIState * s ) <nl> s -> control_regs . cap = ( s -> ports - 1 ) | <nl> ( AHCI_NUM_COMMAND_SLOTS << 8 ) | <nl> ( AHCI_SUPPORTED_SPEED_GEN1 << AHCI_SUPPORTED_SPEED ) | <nl> - HOST_CAP_NCQ | HOST_CAP_AHCI ; <nl> + HOST_CAP_NCQ | HOST_CAP_AHCI | HOST_CAP_64 ; <nl>  <nl> s -> control_regs . impl = ( 1 << s -> ports ) - 1 ; <nl> 
void vfio_region_finalize ( VFIORegion * region ) <nl> g_free ( region -> mmaps ); <nl>  <nl> trace_vfio_region_finalize ( region -> vbasedev -> name , region -> nr ); <nl> + <nl> + region -> mem = NULL ; <nl> + region -> mmaps = NULL ; <nl> + region -> nr_mmaps = 0 ; <nl> + region -> size = 0 ; <nl> + region -> flags = 0 ; <nl> + region -> nr = 0 ; <nl> } <nl>  <nl> void vfio_region_mmaps_set_enabled ( VFIORegion * region , bool enabled )
static void cuda_receive_packet ( CUDAState * s , <nl> } <nl> break ; <nl> default : <nl> + obuf [ 0 ] = ERROR_PACKET ; <nl> + obuf [ 1 ] = 0x2 ; <nl> + obuf [ 2 ] = CUDA_PACKET ; <nl> + obuf [ 3 ] = data [ 0 ]; <nl> + cuda_send_packet_to_host ( s , obuf , 4 ); <nl> break ; <nl> } <nl> }
static target_long monitor_get_ccr ( const struct MonitorDef * md , int val ) <nl>  <nl> u = 0 ; <nl> for ( i = 0 ; i < 8 ; i ++) <nl> - u |= env -> crf [ i ] << ( 32 - ( 4 * i )); <nl> + u |= env -> crf [ i ] << ( 32 - ( 4 * ( i + 1 ))); <nl>  <nl> return u ; <nl> }
static void handle_ti ( ESPState * s ) <nl> { <nl> uint32_t dmalen , minlen ; <nl>  <nl> + if ( s -> dma && ! s -> dma_enabled ) { <nl> + s -> dma_cb = handle_ti ; <nl> + return ; <nl> + } <nl> + <nl> dmalen = s -> rregs [ ESP_TCLO ] | ( s -> rregs [ ESP_TCMID ] << 8 ); <nl> if ( dmalen == 0 ) { <nl> dmalen = 0x10000 ;
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> break ; <nl> } <nl> # endif <nl> + case PR_GET_SECCOMP : <nl> + case PR_SET_SECCOMP : <nl> + /* Disable seccomp to prevent the target disabling syscalls we <nl> + * need . */ <nl> + ret = - TARGET_EINVAL ; <nl> + break ; <nl> default : <nl> /* Most prctl options have no pointer arguments */ <nl> ret = get_errno ( prctl ( arg1 , arg2 , arg3 , arg4 , arg5 ));
uint16_t pvpanic_port ( void ) <nl> if (! o ) { <nl> return 0 ; <nl> } <nl> - return object_property_get_int ( o , PVPANIC_IOPORT_PROP , NULL ); <nl> + return object_property_get_uint ( o , PVPANIC_IOPORT_PROP , NULL ); <nl> } <nl>  <nl> static Property pvpanic_isa_properties [] = {
static void net_slirp_cleanup ( NetClientState * nc ) <nl> SlirpState * s = DO_UPCAST ( SlirpState , nc , nc ); <nl>  <nl> slirp_cleanup ( s -> slirp ); <nl> - qemu_remove_exit_notifier (& s -> exit_notifier ); <nl> + if ( s -> exit_notifier . notify ) { <nl> + qemu_remove_exit_notifier (& s -> exit_notifier ); <nl> + } <nl> slirp_smb_cleanup ( s ); <nl> QTAILQ_REMOVE (& slirp_stacks , s , entry ); <nl> }
typedef struct RAMBlock { <nl> static inline void * ramblock_ptr ( RAMBlock * block , ram_addr_t offset ) <nl> { <nl> assert ( offset < block -> length ); <nl> + assert ( block -> host ); <nl> return ( char *) block -> host + offset ; <nl> } <nl> 
void fork_end ( int child ) <nl> Discard information about the parent threads . */ <nl> CPU_FOREACH_SAFE ( cpu , next_cpu ) { <nl> if ( cpu != thread_cpu ) { <nl> - QTAILQ_REMOVE (& cpus , thread_cpu , node ); <nl> + QTAILQ_REMOVE (& cpus , cpu , node ); <nl> } <nl> } <nl> pending_cpus = 0 ;
static void dump_map_entry ( OutputFormat output_format , MapEntry * e , <nl> ( e -> flags & BDRV_BLOCK_ZERO ) ? " true " : " false ", <nl> ( e -> flags & BDRV_BLOCK_DATA ) ? " true " : " false "); <nl> if ( e -> flags & BDRV_BLOCK_OFFSET_VALID ) { <nl> - printf (", ' offset ': %" PRId64 "", e -> offset ); <nl> + printf (", \" offset \": %" PRId64 "", e -> offset ); <nl> } <nl> putchar ('}'); <nl> 
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } else { <nl> dev -> attached = 1 ; <nl> } <nl> + if ( dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> + dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> + return - EINVAL ; <nl> + } <nl> return 0 ; <nl> } <nl> 
int qdev_device_help ( QemuOpts * opts ) <nl> return 1 ; <nl> } <nl>  <nl> - if (! qemu_opt_get ( opts , "?")) { <nl> + if (! driver || ! qemu_opt_get ( opts , "?")) { <nl> return 0 ; <nl> } <nl> 
static int megasas_dcmd_ld_get_info ( MegasasState * s , MegasasCmd * cmd ) <nl>  <nl> static int megasas_dcmd_cfg_read ( MegasasState * s , MegasasCmd * cmd ) <nl> { <nl> - uint8_t data [ 4096 ]; <nl> + uint8_t data [ 4096 ] = { 0 }; <nl> struct mfi_config_data * info ; <nl> int num_pd_disks = 0 , array_offset , ld_offset ; <nl> BusChild * kid ;
void qmp_drive_mirror ( const char * device , const char * target , <nl> if (! source && sync == MIRROR_SYNC_MODE_TOP ) { <nl> sync = MIRROR_SYNC_MODE_FULL ; <nl> } <nl> + if ( sync == MIRROR_SYNC_MODE_NONE ) { <nl> + source = bs ; <nl> + } <nl>  <nl> size = bdrv_getlength ( bs ); <nl> if ( size < 0 ) {
static gboolean ga_channel_listen_accept ( GIOChannel * channel , <nl> ret = ga_channel_client_add ( c , client_fd ); <nl> if ( ret ) { <nl> g_warning (" error setting up connection "); <nl> + close ( client_fd ); <nl> goto out ; <nl> } <nl> accepted = true ;
void qdev_init_nofail ( DeviceState * dev ) <nl>  <nl> assert (! dev -> realized ); <nl>  <nl> + object_ref ( OBJECT ( dev )); <nl> object_property_set_bool ( OBJECT ( dev ), true , " realized ", & err ); <nl> if ( err ) { <nl> error_reportf_err ( err , " Initialization of device % s failed : ", <nl> object_get_typename ( OBJECT ( dev ))); <nl> exit ( 1 ); <nl> } <nl> + object_unref ( OBJECT ( dev )); <nl> } <nl>  <nl> void qdev_machine_creation_done ( void )
int qemu_chr_be_can_write ( CharDriverState * s ) <nl>  <nl> void qemu_chr_be_write ( CharDriverState * s , uint8_t * buf , int len ) <nl> { <nl> - s -> chr_read ( s -> handler_opaque , buf , len ); <nl> + if ( s -> chr_read ) { <nl> + s -> chr_read ( s -> handler_opaque , buf , len ); <nl> + } <nl> } <nl>  <nl> int qemu_chr_fe_get_msgfd ( CharDriverState * s )
void ppc_slb_invalidate_all ( CPUPPCState * env ) <nl>  <nl> do_invalidate = 0 ; <nl> sr_base = env -> spr [ SPR_ASR ]; <nl> - for ( n = 0 ; n < env -> slb_nr ; n ++) { <nl> + /* XXX : Warning : slbia never invalidates the first segment */ <nl> + for ( n = 1 ; n < env -> slb_nr ; n ++) { <nl> tmp64 = ldq_phys ( sr_base ); <nl> if ( slb_is_valid ( tmp64 )) { <nl> slb_invalidate (& tmp64 );
vvfat_co_pwritev ( BlockDriverState * bs , uint64_t offset , uint64_t bytes , <nl> static int64_t coroutine_fn vvfat_co_get_block_status ( BlockDriverState * bs , <nl> int64_t sector_num , int nb_sectors , int * n , BlockDriverState ** file ) <nl> { <nl> - BDRVVVFATState * s = bs -> opaque ; <nl> - * n = s -> sector_count - sector_num ; <nl> + * n = bs -> total_sectors - sector_num ; <nl> if (* n > nb_sectors ) { <nl> * n = nb_sectors ; <nl> } else if (* n < 0 ) {
static void pc_fw_add_pflash_drv ( void ) <nl> filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl>  <nl> opts = drive_add ( IF_PFLASH , - 1 , filename , " readonly = on "); <nl> + <nl> + g_free ( filename ); <nl> + <nl> if ( opts == NULL ) { <nl> return ; <nl> }
static QemuOptsList nbd_runtime_opts = { <nl> . type = QEMU_OPT_STRING , <nl> . help = " ID of the TLS credentials to use ", <nl> }, <nl> + { /* end of list */ } <nl> }, <nl> }; <nl> 
void bdrv_detach_dev ( BlockDriverState * bs , void * dev ) <nl> bs -> dev = NULL ; <nl> bs -> dev_ops = NULL ; <nl> bs -> dev_opaque = NULL ; <nl> + bs -> buffer_alignment = 512 ; <nl> } <nl>  <nl> /* TODO change to return DeviceState * when all users are qdevified */
static void gen_add_A0_ds_seg ( DisasContext * s ) <nl> if ( s -> override >= 0 ) { <nl> override = s -> override ; <nl> must_add_seg = 1 ; <nl> - } else { <nl> - override = R_DS ; <nl> } <nl> if ( must_add_seg ) { <nl> # ifdef TARGET_X86_64
void pcie_aer_root_init ( PCIDevice * dev ) <nl> PCI_ERR_ROOT_CMD_EN_MASK ); <nl> pci_set_long ( dev -> w1cmask + pos + PCI_ERR_ROOT_STATUS , <nl> PCI_ERR_ROOT_STATUS_REPORT_MASK ); <nl> + /* PCI_ERR_ROOT_IRQ is RO but devices change it using a <nl> + * device - specific method . <nl> + */ <nl> + pci_set_long ( dev -> cmask + pos + PCI_ERR_ROOT_STATUS , <nl> + ~ PCI_ERR_ROOT_IRQ ); <nl> } <nl>  <nl> void pcie_aer_root_reset ( PCIDevice * dev )
void test_clone ( void ) <nl> CLONE_VM | CLONE_FS | CLONE_FILES | SIGCHLD , " hello2 ")); <nl>  <nl> while ( waitpid ( pid1 , & status1 , 0 ) != pid1 ); <nl> + free ( stack1 ); <nl> while ( waitpid ( pid2 , & status2 , 0 ) != pid2 ); <nl> + free ( stack2 ); <nl> if ( thread1_res != 5 || <nl> thread2_res != 6 ) <nl> error (" clone ");
static void zynq_xadc_write ( void * opaque , hwaddr offset , uint64_t val , <nl> break ; <nl> } <nl>  <nl> - if ( xadc_reg > ZYNQ_XADC_NUM_ADC_REGS && xadc_cmd != CMD_NOP ) { <nl> + if ( xadc_reg >= ZYNQ_XADC_NUM_ADC_REGS && xadc_cmd != CMD_NOP ) { <nl> qemu_log_mask ( LOG_GUEST_ERROR , " read / write op to invalid xadc " <nl> " reg 0x % x \ n ", xadc_reg ); <nl> break ;
static inline int kvmppc_remove_spapr_tce ( void * table , int pfd , <nl>  <nl> static inline int kvmppc_reset_htab ( int shift_hint ) <nl> { <nl> - return - 1 ; <nl> + return 0 ; <nl> } <nl>  <nl> static inline uint64_t kvmppc_rma_size ( uint64_t current_size ,
static inline int vfp_exceptbits_from_host ( int host_bits ) <nl> target_bits |= 2 ; <nl> if ( host_bits & float_flag_overflow ) <nl> target_bits |= 4 ; <nl> - if ( host_bits & float_flag_underflow ) <nl> + if ( host_bits & ( float_flag_underflow | float_flag_output_denormal )) <nl> target_bits |= 8 ; <nl> if ( host_bits & float_flag_inexact ) <nl> target_bits |= 0x10 ;
void s390_machine_reset ( void ) <nl> { <nl> S390CPU * ipl_cpu = S390_CPU ( qemu_get_cpu ( 0 )); <nl>  <nl> - qemu_devices_reset (); <nl> s390_cmma_reset (); <nl> + qemu_devices_reset (); <nl> s390_crypto_reset (); <nl>  <nl> /* all cpus are stopped - configure and start the ipl cpu only */
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> # endif <nl> # ifdef TARGET_NR_pause /* not on alpha */ <nl> case TARGET_NR_pause : <nl> - ret = get_errno ( pause ()); <nl> + if (! block_signals ()) { <nl> + sigsuspend (&(( TaskState *) cpu -> opaque )-> signal_mask ); <nl> + } <nl> + ret = - TARGET_EINTR ; <nl> break ; <nl> # endif <nl> # ifdef TARGET_NR_utime
void vhost_dev_stop ( struct vhost_dev * hdev , VirtIODevice * vdev ) <nl>  <nl> hdev -> started = false ; <nl> qemu_free ( hdev -> log ); <nl> + hdev -> log = NULL ; <nl> hdev -> log_size = 0 ; <nl> }
static void build_guest_fsinfo_for_real_device ( char const * syspath , <nl> break ; <nl> } <nl>  <nl> + g_free ( driver ); <nl> if ( sscanf ( p , "/% x :% x :% x .% x % n ", <nl> pci , pci + 1 , pci + 2 , pci + 3 , & pcilen ) == 4 ) { <nl> p += pcilen ;
void virtqueue_discard ( VirtQueue * vq , const VirtQueueElement * elem , <nl> unsigned int len ) <nl> { <nl> vq -> last_avail_idx --; <nl> + vq -> inuse --; <nl> virtqueue_unmap_sg ( vq , elem , len ); <nl> } <nl> 
static int bdrv_check_byte_request ( BlockDriverState * bs , int64_t offset , <nl> { <nl> int64_t len ; <nl>  <nl> + if ( size > INT_MAX ) { <nl> + return - EIO ; <nl> + } <nl> + <nl> if (! bdrv_is_inserted ( bs )) <nl> return - ENOMEDIUM ; <nl> 
static int bdrv_check_byte_request ( BlockDriverState * bs , int64_t offset , <nl> static int bdrv_check_request ( BlockDriverState * bs , int64_t sector_num , <nl> int nb_sectors ) <nl> { <nl> + if ( nb_sectors > INT_MAX / BDRV_SECTOR_SIZE ) { <nl> + return - EIO ; <nl> + } <nl> + <nl> return bdrv_check_byte_request ( bs , sector_num * BDRV_SECTOR_SIZE , <nl> nb_sectors * BDRV_SECTOR_SIZE ); <nl> }
vpc_co_pwritev ( BlockDriverState * bs , uint64_t offset , uint64_t bytes , <nl> int64_t image_offset ; <nl> int64_t n_bytes ; <nl> int64_t bytes_done = 0 ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> VHDFooter * footer = ( VHDFooter *) s -> footer_buf ; <nl> QEMUIOVector local_qiov ; <nl> 
void helper_svm_check_intercept_param ( uint32_t type , uint64_t param ) <nl> switch (( uint32_t ) ECX ) { <nl> case 0 ... 0x1fff : <nl> t0 = ( ECX * 2 ) % 8 ; <nl> - t1 = ECX / 8 ; <nl> + t1 = ( ECX * 2 ) / 8 ; <nl> break ; <nl> case 0xc0000000 ... 0xc0001fff : <nl> t0 = ( 8192 + ECX - 0xc0000000 ) * 2 ;
static int blkverify_open ( BlockDriverState * bs , QDict * options , int flags , <nl>  <nl> ret = 0 ; <nl> fail : <nl> + if ( ret < 0 ) { <nl> + bdrv_unref_child ( bs , bs -> file ); <nl> + } <nl> qemu_opts_del ( opts ); <nl> return ret ; <nl> }
static int disas_neon_data_insn ( CPUState * env , DisasContext * s , uint32_t insn ) <nl> } <nl> tmp3 = neon_load_reg ( rm , 1 ); <nl> gen_helper_neon_tbl ( tmp3 , tmp3 , tmp , tmp4 , tmp5 ); <nl> - dead_tmp ( tmp5 ); <nl> - dead_tmp ( tmp4 ); <nl> + tcg_temp_free_i32 ( tmp5 ); <nl> + tcg_temp_free_i32 ( tmp4 ); <nl> neon_store_reg ( rd , 0 , tmp2 ); <nl> neon_store_reg ( rd , 1 , tmp3 ); <nl> dead_tmp ( tmp );
static ssize_t nc_sendv_compat ( NetClientState * nc , const struct iovec * iov , <nl> offset = iov [ 0 ]. iov_len ; <nl> } else { <nl> buffer = buf ; <nl> - offset = iov_to_buf ( iov , iovcnt , 0 , buffer , sizeof ( buffer )); <nl> + offset = iov_to_buf ( iov , iovcnt , 0 , buf , sizeof ( buf )); <nl> } <nl>  <nl> if ( flags & QEMU_NET_PACKET_FLAG_RAW && nc -> info -> receive_raw ) {
static void mirror_iteration_done ( MirrorOp * op , int ret ) <nl> bitmap_set ( s -> cow_bitmap , chunk_num , nb_chunks ); <nl> } <nl>  <nl> + qemu_iovec_destroy (& op -> qiov ); <nl> g_slice_free ( MirrorOp , op ); <nl> qemu_coroutine_enter ( s -> common . co , NULL ); <nl> }
static always_inline void gen_bcond ( DisasContext * ctx , int type ) <nl> # endif <nl> gen_op_btest_T1 ( ctx -> nip ); <nl> no_test : <nl> - if ( ctx -> singlestep_enabled & GDBSTUB_SINGLE_STEP ) { <nl> - gen_update_nip ( ctx , ctx -> nip ); <nl> - gen_op_debug (); <nl> - } <nl> tcg_gen_exit_tb ( 0 ); <nl> } <nl> }
int bdrv_pwrite_sync ( BlockDriverState * bs , int64_t offset , <nl> return ret ; <nl> } <nl>  <nl> - /* No flush needed for cache modes that already do it */ <nl> - if ( bs -> enable_write_cache ) { <nl> - bdrv_flush ( bs ); <nl> + ret = bdrv_flush ( bs ); <nl> + if ( ret < 0 ) { <nl> + return ret ; <nl> } <nl>  <nl> return 0 ;
static void spapr_dr_connector_class_init ( ObjectClass * k , void * data ) <nl> drck -> attach = attach ; <nl> drck -> detach = detach ; <nl> drck -> release_pending = release_pending ; <nl> + /* <nl> + * Reason : it crashes FIXME find and document the real reason <nl> + */ <nl> + dk -> cannot_instantiate_with_device_add_yet = true ; <nl> } <nl>  <nl> static const TypeInfo spapr_dr_connector_info = {
static void shpc_interrupt_update ( PCIDevice * d ) <nl> for ( slot = 0 ; slot < shpc -> nslots ; ++ slot ) { <nl> uint8_t event = shpc -> config [ SHPC_SLOT_EVENT_LATCH ( slot )]; <nl> uint8_t disable = shpc -> config [ SHPC_SLOT_EVENT_SERR_INT_DIS ( d , slot )]; <nl> - uint32_t mask = 1 << SHPC_IDX_TO_LOGICAL ( slot ); <nl> + uint32_t mask = 1U << SHPC_IDX_TO_LOGICAL ( slot ); <nl> if ( event & ~ disable ) { <nl> int_locator |= mask ; <nl> }
static abi_ulong mmap_find_vma_reserved ( abi_ulong start , abi_ulong size ) <nl> if ( prot ) { <nl> end_addr = addr ; <nl> } <nl> - if ( addr + size == end_addr ) { <nl> + if ( addr && addr + size == end_addr ) { <nl> break ; <nl> } <nl> addr -= qemu_host_page_size ;
guint qemu_chr_fe_add_watch ( CharBackend * be , GIOCondition cond , <nl> } <nl>  <nl> g_source_set_callback ( src , ( GSourceFunc ) func , user_data , NULL ); <nl> - tag = g_source_attach ( src , NULL ); <nl> + tag = g_source_attach ( src , s -> gcontext ); <nl> g_source_unref ( src ); <nl>  <nl> return tag ;
static const QEMUFileOps rdma_write_ops = { <nl>  <nl> static void * qemu_fopen_rdma ( RDMAContext * rdma , const char * mode ) <nl> { <nl> - QEMUFileRDMA * r = g_malloc0 ( sizeof ( QEMUFileRDMA )); <nl> + QEMUFileRDMA * r ; <nl>  <nl> if ( qemu_file_mode_is_not_valid ( mode )) { <nl> return NULL ; <nl> } <nl>  <nl> + r = g_malloc0 ( sizeof ( QEMUFileRDMA )); <nl> r -> rdma = rdma ; <nl>  <nl> if ( mode [ 0 ] == ' w ') {
static void spapr_finalize_fdt ( sPAPREnvironment * spapr , <nl>  <nl> cpu_physical_memory_write ( fdt_addr , fdt , fdt_totalsize ( fdt )); <nl>  <nl> + g_free ( bootlist ); <nl> g_free ( fdt ); <nl> } <nl> 
static int ehci_process_itd ( EHCIState * ehci , <nl> if ( off + len > 4096 ) { <nl> /* transfer crosses page border */ <nl> if ( pg == 6 ) { <nl> + qemu_sglist_destroy (& ehci -> isgl ); <nl> return - 1 ; /* avoid page pg + 1 */ <nl> } <nl> ptr2 = ( itd -> bufptr [ pg + 1 ] & ITD_BUFPTR_MASK );
DisplaySurface * qemu_create_displaysurface_guestmem ( int width , int height , <nl> linesize = width * PIXMAN_FORMAT_BPP ( format ) / 8 ; <nl> } <nl>  <nl> - size = linesize * height ; <nl> + size = ( hwaddr ) linesize * height ; <nl> data = cpu_physical_memory_map ( addr , & size , 0 ); <nl> - if ( size != linesize * height ) { <nl> + if ( size != ( hwaddr ) linesize * height ) { <nl> cpu_physical_memory_unmap ( data , size , 0 , 0 ); <nl> return NULL ; <nl> }
void replay_add_event ( ReplayAsyncEventKind event_kind , <nl>  <nl> void replay_bh_schedule_event ( QEMUBH * bh ) <nl> { <nl> - if ( replay_mode != REPLAY_MODE_NONE ) { <nl> + if ( replay_mode != REPLAY_MODE_NONE && events_enabled ) { <nl> uint64_t id = replay_get_current_step (); <nl> replay_add_event ( REPLAY_ASYNC_EVENT_BH , bh , NULL , id ); <nl> } else {
static bool run_poll_handlers_once ( AioContext * ctx ) <nl>  <nl> QLIST_FOREACH_RCU ( node , & ctx -> aio_handlers , node ) { <nl> if (! node -> deleted && node -> io_poll && <nl> - node -> io_poll ( node -> opaque )) { <nl> + aio_node_check ( ctx , node -> is_external ) && <nl> + node -> io_poll ( node -> opaque )) { <nl> progress = true ; <nl> } <nl> 
int cpu_exec ( CPUArchState * env ) <nl> * local variables as longjmp is marked ' noreturn '. */ <nl> cpu = current_cpu ; <nl> env = cpu -> env_ptr ; <nl> +# if !( defined ( CONFIG_USER_ONLY ) && \ <nl> + ( defined ( TARGET_M68K ) || defined ( TARGET_PPC ) || defined ( TARGET_S390X ))) <nl> + cc = CPU_GET_CLASS ( cpu ); <nl> +# endif <nl> } <nl> } /* for (;;) */ <nl> 
static inline void powerpc_excp ( CPUPPCState * env , int excp_model , int excp ) <nl> if ( asrr1 != - 1 ) <nl> env -> spr [ asrr1 ] = env -> spr [ srr1 ]; <nl> /* If we disactivated any translation , flush TLBs */ <nl> - if ( new_msr & (( 1 << MSR_IR ) | ( 1 << MSR_DR ))) <nl> + if ( msr & (( 1 << MSR_IR ) | ( 1 << MSR_DR ))) <nl> tlb_flush ( env , 1 ); <nl>  <nl> if ( msr_ile ) {
static int exynos4210_combiner_init ( SysBusDevice * sbd ) <nl> qdev_init_gpio_in ( dev , exynos4210_combiner_handler , IIC_NIRQ ); <nl>  <nl> /* Connect SysBusDev irqs to device specific irqs */ <nl> - for ( i = 0 ; i < IIC_NIRQ ; i ++) { <nl> + for ( i = 0 ; i < IIC_NGRP ; i ++) { <nl> sysbus_init_irq ( sbd , & s -> output_irq [ i ]); <nl> } <nl> 
static gboolean qio_channel_websock_handshake_io ( QIOChannel * ioc , <nl> return TRUE ; <nl> } <nl>  <nl> - object_ref ( OBJECT ( task )); <nl> trace_qio_channel_websock_handshake_reply ( ioc ); <nl> qio_channel_add_watch ( <nl> wioc -> master , <nl> G_IO_OUT , <nl> qio_channel_websock_handshake_send , <nl> task , <nl> - ( GDestroyNotify ) object_unref ); <nl> + NULL ); <nl> return FALSE ; <nl> } <nl> 
static int local_symlink ( FsContext * fs_ctx , const char * oldpath , <nl> return fd ; <nl> } <nl> /* Write the oldpath ( target ) to the file . */ <nl> - oldpath_size = strlen ( oldpath ) + 1 ; <nl> + oldpath_size = strlen ( oldpath ); <nl> do { <nl> write_size = write ( fd , ( void *) oldpath , oldpath_size ); <nl> } while ( write_size == - 1 && errno == EINTR );
static void test_dummy_createcmdl ( void ) <nl> g_assert ( err == NULL ); <nl> error_free ( err ); <nl>  <nl> + object_unref ( OBJECT ( dobj )); <nl> + <nl> /* <nl> * cmdline - parsing via qemu_opts_parse () results in a QemuOpts entry <nl> * corresponding to the Object ' s ID to be added to the QemuOptsList
static int vhost_kernel_memslots_limit ( struct vhost_dev * dev ) <nl> & s , NULL , NULL )) { <nl> uint64_t val = g_ascii_strtoull ( s , NULL , 10 ); <nl> if (!(( val == G_MAXUINT64 || ! val ) && errno )) { <nl> + g_free ( s ); <nl> return val ; <nl> } <nl> error_report (" ignoring invalid max_mem_regions value in vhost module :" <nl> " % s ", s ); <nl> } <nl> + g_free ( s ); <nl> return limit ; <nl> } <nl> 
static BlockDriverAIOCB * rbd_start_aio ( BlockDriverState * bs , <nl> } <nl>  <nl> if ( r < 0 ) { <nl> - goto failed ; <nl> + goto failed_completion ; <nl> } <nl>  <nl> return & acb -> common ; <nl>  <nl> + failed_completion : <nl> + rbd_aio_release ( c ); <nl> failed : <nl> g_free ( rcb ); <nl> + qemu_vfree ( acb -> bounce ); <nl> qemu_aio_release ( acb ); <nl> return NULL ; <nl> }
SDState * sd_init ( BlockDriverState * bs , bool is_spi ) <nl> { <nl> SDState * sd ; <nl>  <nl> - if ( bdrv_is_read_only ( bs )) { <nl> + if ( bs && bdrv_is_read_only ( bs )) { <nl> fprintf ( stderr , " sd_init : Cannot use read - only drive \ n "); <nl> return NULL ; <nl> }
static void tcg_out_qemu_st ( TCGContext * s , const TCGArg * args , int opc ) <nl> # else <nl> tcg_out_mov ( s , TCG_TYPE_I32 , 3 , addr_reg2 ); <nl> tcg_out_mov ( s , TCG_TYPE_I32 , 4 , addr_reg ); <nl> -# ifdef TCG_TARGET_CALL_ALIGN_ARGS <nl> ir = 5 ; <nl> -# else <nl> - ir = 4 ; <nl> -# endif <nl> # endif <nl>  <nl> switch ( opc ) {
void tlb_fill ( CPUState * env1 , target_ulong addr , int is_write , int mmu_idx , <nl> int ret ; <nl>  <nl> saved_env = env ; <nl> + env = env1 ; <nl> ret = cpu_arm_handle_mmu_fault ( env , addr , is_write , mmu_idx ); <nl> if ( unlikely ( ret )) { <nl> if ( retaddr ) {
static int read_directory ( BDRVVVFATState * s , int mapping_index ) <nl> if ( st . st_size > 0x7fffffff ) { <nl> fprintf ( stderr , " File % s is larger than 2GB \ n ", buffer ); <nl> free ( buffer ); <nl> + closedir ( dir ); <nl> return - 2 ; <nl> } <nl> direntry -> size = cpu_to_le32 ( S_ISDIR ( st . st_mode )? 0 : st . st_size );
static int vmdk_init_tables ( BlockDriverState * bs , VmdkExtent * extent , <nl> Error ** errp ) <nl> { <nl> int ret ; <nl> - int l1_size , i ; <nl> + size_t l1_size ; <nl> + int i ; <nl>  <nl> /* read the L1 table */ <nl> l1_size = extent -> l1_size * sizeof ( uint32_t );
int cpu_get_pic_interrupt ( CPUX86State * env ) <nl> /* Make sure everything is in a consistent state for calling fork (). */ <nl> void fork_start ( void ) <nl> { <nl> - cpu_list_lock (); <nl> - qemu_mutex_lock (& tb_ctx . tb_lock ); <nl> mmap_fork_start (); <nl> + qemu_mutex_lock (& tb_ctx . tb_lock ); <nl> + cpu_list_lock (); <nl> } <nl>  <nl> void fork_end ( int child )
static uint64_t alloc_cluster_offset ( BlockDriverState * bs , <nl> /* how many free clusters ? */ <nl>  <nl> while ( i < nb_clusters ) { <nl> - cluster_offset = l2_table [ l2_index + i ]; <nl> + cluster_offset = be64_to_cpu ( l2_table [ l2_index + i ]); <nl> if ( cluster_offset != 0 ) <nl> break ; <nl> i ++;
static inline void gdb_continue ( GDBState * s ) <nl> # ifdef CONFIG_USER_ONLY <nl> s -> running_state = 1 ; <nl> # else <nl> - if ( runstate_check ( RUN_STATE_DEBUG )) { <nl> + if (! runstate_needs_reset ()) { <nl> vm_start (); <nl> } <nl> # endif
static int cpudef_setfield ( const char * name , const char * str , void * opaque ) <nl> int err = 0 ; <nl>  <nl> if (! strcmp ( name , " name ")) { <nl> + g_free (( void *) def -> name ); <nl> def -> name = g_strdup ( str ); <nl> } else if (! strcmp ( name , " model_id ")) { <nl> strncpy ( def -> model_id , str , sizeof ( def -> model_id ));
static int cpu_post_load ( void * opaque , int version_id ) <nl> ppc_set_compat ( cpu , cpu -> compat_pvr , & local_err ); <nl> if ( local_err ) { <nl> error_report_err ( local_err ); <nl> - error_free ( local_err ); <nl> return - 1 ; <nl> } <nl> } else
typedef struct VirtQueueElement <nl> struct iovec out_sg [ VIRTQUEUE_MAX_SIZE ]; <nl> } VirtQueueElement ; <nl>  <nl> -# define VIRTIO_QUEUE_MAX 64 <nl> +# define VIRTIO_QUEUE_MAX 1024 <nl>  <nl> # define VIRTIO_NO_VECTOR 0xffff <nl> 
static void wdt_diag288_class_init ( ObjectClass * klass , void * data ) <nl> dc -> realize = wdt_diag288_realize ; <nl> dc -> unrealize = wdt_diag288_unrealize ; <nl> dc -> reset = wdt_diag288_reset ; <nl> + dc -> hotpluggable = false ; <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> vmsd = & vmstate_diag288 ; <nl> diag288 -> handle_timer = wdt_diag288_handle_timer ;
static void * oss_audio_init ( void ) <nl>  <nl> if ( access ( conf -> devpath_in , R_OK | W_OK ) < 0 || <nl> access ( conf -> devpath_out , R_OK | W_OK ) < 0 ) { <nl> + g_free ( conf ); <nl> return NULL ; <nl> } <nl> return conf ;
static void vhost_scsi_stop ( VHostSCSI * s ) <nl> VirtioBusClass * k = VIRTIO_BUS_GET_CLASS ( qbus ); <nl> int ret = 0 ; <nl>  <nl> - if (! k -> set_guest_notifiers ) { <nl> + if ( k -> set_guest_notifiers ) { <nl> ret = k -> set_guest_notifiers ( qbus -> parent , s -> dev . nvqs , false ); <nl> if ( ret < 0 ) { <nl> error_report (" vhost guest notifier cleanup failed : % d \ n ", ret );
ram_addr_t qemu_ram_alloc_from_ptr ( ram_addr_t size , void * host , <nl>  <nl> ram_list . phys_dirty = g_realloc ( ram_list . phys_dirty , <nl> last_ram_offset () >> TARGET_PAGE_BITS ); <nl> + memset ( ram_list . phys_dirty + ( new_block -> offset >> TARGET_PAGE_BITS ), <nl> + 0 , size >> TARGET_PAGE_BITS ); <nl> cpu_physical_memory_set_dirty_range ( new_block -> offset , size , 0xff ); <nl>  <nl> if ( kvm_enabled ())
static void baum_chr_open ( Chardev * chr , <nl> error_setg ( errp , " brlapi__openConnection : % s ", <nl> brlapi_strerror ( brlapi_error_location ())); <nl> g_free ( handle ); <nl> + baum -> brlapi = NULL ; <nl> return ; <nl> } <nl> baum -> deferred_init = 0 ;
m_free ( struct mbuf * m ) <nl> * Either free () it or put it on the free list <nl> */ <nl> if ( m -> m_flags & M_DOFREE ) { <nl> - free ( m ); <nl> m -> slirp -> mbuf_alloced --; <nl> + free ( m ); <nl> } else if (( m -> m_flags & M_FREELIST ) == 0 ) { <nl> insque ( m ,& m -> slirp -> m_freelist ); <nl> m -> m_flags = M_FREELIST ; /* Clobber other flags */
static const RunStateTransition runstate_transitions_def [] = { <nl>  <nl> { RUN_STATE_PAUSED , RUN_STATE_RUNNING }, <nl> { RUN_STATE_PAUSED , RUN_STATE_FINISH_MIGRATE }, <nl> + { RUN_STATE_PAUSED , RUN_STATE_POSTMIGRATE }, <nl> { RUN_STATE_PAUSED , RUN_STATE_PRELAUNCH }, <nl> { RUN_STATE_PAUSED , RUN_STATE_COLO }, <nl>  <nl> static const RunStateTransition runstate_transitions_def [] = { <nl> { RUN_STATE_PRELAUNCH , RUN_STATE_INMIGRATE }, <nl>  <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_RUNNING }, <nl> + { RUN_STATE_FINISH_MIGRATE , RUN_STATE_PAUSED }, <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_POSTMIGRATE }, <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_PRELAUNCH }, <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_COLO },
void helper_pmon ( int function ) <nl> break ; <nl> case 158 : <nl> { <nl> - unsigned char * fmt = ( void *)( unsigned long ) env -> active_tc . gpr [ 4 ]; <nl> + unsigned char * fmt = ( void *)( uintptr_t ) env -> active_tc . gpr [ 4 ]; <nl> printf ("% s ", fmt ); <nl> } <nl> break ;
static VncState * vnc_state ; /* needed for info vnc */ <nl>  <nl> void do_info_vnc ( void ) <nl> { <nl> - if ( vnc_state == NULL ) <nl> + if ( vnc_state == NULL || vnc_state -> display == NULL ) <nl> term_printf (" VNC server disabled \ n "); <nl> else { <nl> term_printf (" VNC server active on : ");
static void qxl_enter_vga_mode ( PCIQXLDevice * d ) <nl> trace_qxl_enter_vga_mode ( d -> id ); <nl> qemu_spice_create_host_primary (& d -> ssd ); <nl> d -> mode = QXL_MODE_VGA ; <nl> - memset (& d -> ssd . dirty , 0 , sizeof ( d -> ssd . dirty )); <nl> + dpy_gfx_resize ( d -> ssd . ds ); <nl> vga_dirty_log_start (& d -> vga ); <nl> } <nl> 
static int ata_passthrough_12_xfer_size ( SCSIDevice * dev , uint8_t * buf ) <nl> switch ( length ) { <nl> case 0 : <nl> case 3 : /* USB - specific . */ <nl> + default : <nl> xfer = 0 ; <nl> break ; <nl> case 1 : <nl> static int ata_passthrough_16_xfer_size ( SCSIDevice * dev , uint8_t * buf ) <nl> switch ( length ) { <nl> case 0 : <nl> case 3 : /* USB - specific . */ <nl> + default : <nl> xfer = 0 ; <nl> break ; <nl> case 1 :
static int64_t try_fiemap ( BlockDriverState * bs , off_t start , off_t * data , <nl>  <nl> f . fm . fm_start = start ; <nl> f . fm . fm_length = ( int64_t ) nb_sectors * BDRV_SECTOR_SIZE ; <nl> - f . fm . fm_flags = 0 ; <nl> + f . fm . fm_flags = FIEMAP_FLAG_SYNC ; <nl> f . fm . fm_extent_count = 1 ; <nl> f . fm . fm_reserved = 0 ; <nl> if ( ioctl ( s -> fd , FS_IOC_FIEMAP , & f ) == - 1 ) {
static int pci_qdev_init ( DeviceState * qdev ) <nl> pci_dev -> romfile = g_strdup ( pc -> romfile ); <nl> is_default_rom = true ; <nl> } <nl> - pci_add_option_rom ( pci_dev , is_default_rom ); <nl> + <nl> + rc = pci_add_option_rom ( pci_dev , is_default_rom ); <nl> + if ( rc != 0 ) { <nl> + pci_unregister_device ( DEVICE ( pci_dev )); <nl> + return rc ; <nl> + } <nl>  <nl> return 0 ; <nl> }
static void spapr_memory_pre_plug ( HotplugHandler * hotplug_dev , DeviceState * dev , <nl> if ( mem_dev && ! kvmppc_is_mem_backend_page_size_ok ( mem_dev )) { <nl> error_setg ( errp , " Memory backend has bad page size . " <nl> " Use ' memory - backend - file ' with correct mem - path ."); <nl> - return ; <nl> + goto out ; <nl> } <nl> + <nl> + out : <nl> + g_free ( mem_dev ); <nl> } <nl>  <nl> struct sPAPRDIMMState {
static void handle_windowevent ( SDL_Event * ev ) <nl> { <nl> struct sdl2_console * scon = get_scon_from_window ( ev -> window . windowID ); <nl>  <nl> + if (! scon ) { <nl> + return ; <nl> + } <nl> + <nl> switch ( ev -> window . event ) { <nl> case SDL_WINDOWEVENT_RESIZED : <nl> {
struct BusState { <nl> struct Property { <nl> const char * name ; <nl> PropertyInfo * info ; <nl> - int offset ; <nl> + ptrdiff_t offset ; <nl> uint8_t bitnr ; <nl> qtype_code qtype ; <nl> int64_t defval ;
static void bit_prop_set ( DeviceState * dev , Property * props , bool val ) <nl> uint32_t * p = qdev_get_prop_ptr ( dev , props ); <nl> uint32_t mask = qdev_get_prop_mask ( props ); <nl> if ( val ) <nl> - * p |= ~ mask ; <nl> + * p |= mask ; <nl> else <nl> * p &= ~ mask ; <nl> }
typedef struct USBNetState { <nl>  <nl> static int is_rndis ( USBNetState * s ) <nl> { <nl> - return s -> dev . config -> bConfigurationValue == DEV_RNDIS_CONFIG_VALUE ; <nl> + return s -> dev . config ? <nl> + s -> dev . config -> bConfigurationValue == DEV_RNDIS_CONFIG_VALUE : 0 ; <nl> } <nl>  <nl> static int ndis_query ( USBNetState * s , uint32_t oid ,
static int pty_chr_write ( CharDriverState * chr , const uint8_t * buf , int len ) <nl> if (! s -> connected ) { <nl> /* guest sends data , check for ( re -) connect */ <nl> pty_chr_update_read_handler_locked ( chr ); <nl> - return 0 ; <nl> + if (! s -> connected ) { <nl> + return 0 ; <nl> + } <nl> } <nl> return io_channel_send ( s -> fd , buf , len ); <nl> }
int cpu_exec ( CPUState * env ) <nl> /* reset soft MMU for next block ( it can currently <nl> only be set by a memory fault ) */ <nl> } /* for (;;) */ <nl> + } else { <nl> + /* Reload env after longjmp - the compiler may have smashed all <nl> + * local variables as longjmp is marked ' noreturn '. */ <nl> + env = cpu_single_env ; <nl> } <nl> } /* for (;;) */ <nl> 
static void pcie_cap_slot_hotplug_common ( PCIDevice * hotplug_dev , <nl> /* the slot is electromechanically locked . <nl> * This error is propagated up to qdev and then to HMP / QMP . <nl> */ <nl> - error_setg_errno ( errp , - EBUSY , " slot is electromechanically locked "); <nl> + error_setg_errno ( errp , EBUSY , " slot is electromechanically locked "); <nl> } <nl> } <nl> 
InetSocketAddress * inet_parse ( const char * str , Error ** errp ) <nl> { <nl> InetSocketAddress * addr ; <nl> const char * optstr , * h ; <nl> - char host [ 64 ]; <nl> + char host [ 65 ]; <nl> char port [ 33 ]; <nl> int to ; <nl> int pos ;
void virtio_queue_set_notification ( VirtQueue * vq , int enable ) <nl> } else { <nl> vring_used_flags_set_bit ( vq , VRING_USED_F_NO_NOTIFY ); <nl> } <nl> + if ( enable ) { <nl> + /* Expose avail event / used flags before caller checks the avail idx . */ <nl> + smp_mb (); <nl> + } <nl> } <nl>  <nl> int virtio_queue_ready ( VirtQueue * vq )
float64 HELPER ( recpe_f64 )( float64 input , void * fpstp ) <nl> } else { <nl> return float64_set_sign ( float64_maxnorm , float64_is_neg ( f64 )); <nl> } <nl> - } else if ( f64_exp >= 1023 && fpst -> flush_to_zero ) { <nl> + } else if ( f64_exp >= 2045 && fpst -> flush_to_zero ) { <nl> float_raise ( float_flag_underflow , fpst ); <nl> return float64_set_sign ( float64_zero , float64_is_neg ( f64 )); <nl> }
static void ccid_card_vscard_handle_message ( PassthruState * card , <nl> error_report (" ATR size exceeds spec , ignoring "); <nl> ccid_card_vscard_send_error ( card , scr_msg_header -> reader_id , <nl> VSC_GENERAL_ERROR ); <nl> + break ; <nl> } <nl> memcpy ( card -> atr , data , scr_msg_header -> length ); <nl> card -> atr_length = scr_msg_header -> length ;
void qemu_tcg_configure ( QemuOpts * opts , Error ** errp ) <nl> } else if ( use_icount ) { <nl> error_setg ( errp , " No MTTCG when icount is enabled "); <nl> } else { <nl> +# ifndef TARGET_SUPPORT_MTTCG <nl> + error_report (" Guest not yet converted to MTTCG - " <nl> + " you may get unexpected results "); <nl> +# endif <nl> if (! check_tcg_memory_orders_compatible ()) { <nl> error_report (" Guest expects a stronger memory ordering " <nl> " than the host provides ");
static inline void gen_neon_mull ( TCGv_i64 dest , TCGv a , TCGv b , int size , int u ) <nl> case 4 : <nl> tmp = gen_muls_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> case 5 : <nl> tmp = gen_mulu_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> default : abort (); <nl> }
static int qxl_track_command ( PCIQXLDevice * qxl , struct QXLCommandExt * ext ) <nl> qxl -> guest_cursor = ext -> cmd . data ; <nl> qemu_mutex_unlock (& qxl -> track_lock ); <nl> } <nl> + if ( cmd -> type == QXL_CURSOR_HIDE ) { <nl> + qemu_mutex_lock (& qxl -> track_lock ); <nl> + qxl -> guest_cursor = 0 ; <nl> + qemu_mutex_unlock (& qxl -> track_lock ); <nl> + } <nl> break ; <nl> } <nl> }
static void gen_pusha ( DisasContext * s ) <nl> { <nl> int i ; <nl> gen_op_movl_A0_reg ( R_ESP ); <nl> - gen_op_addl_A0_im (- 8 << s -> dflag ); <nl> + gen_op_addl_A0_im (-( 8 << s -> dflag )); <nl> if (! s -> ss32 ) <nl> tcg_gen_ext16u_tl ( cpu_A0 , cpu_A0 ); <nl> tcg_gen_mov_tl ( cpu_T [ 1 ], cpu_A0 );
static inline void acpi_build_tables_init ( AcpiBuildTables * tables ) <nl> static inline void acpi_build_tables_cleanup ( AcpiBuildTables * tables , bool mfre ) <nl> { <nl> void * linker_data = bios_linker_loader_cleanup ( tables -> linker ); <nl> - if ( mfre ) { <nl> - g_free ( linker_data ); <nl> - } <nl> + g_free ( linker_data ); <nl> g_array_free ( tables -> rsdp , mfre ); <nl> - g_array_free ( tables -> table_data , mfre ); <nl> + g_array_free ( tables -> table_data , true ); <nl> g_array_free ( tables -> tcpalog , mfre ); <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> cmdline = g_strdup_printf ("- device ipmi - bmc - sim , id = bmc0 " <nl> " - device isa - ipmi - kcs , bmc = bmc0 "); <nl> qtest_start ( cmdline ); <nl> + g_free ( cmdline ); <nl> qtest_irq_intercept_in ( global_qtest , " ioapic "); <nl> qtest_add_func ("/ ipmi / local / kcs_base ", test_kcs_base ); <nl> qtest_add_func ("/ ipmi / local / kcs_abort ", test_kcs_abort );
static int do_readlink ( struct iovec * iovec , struct iovec * out_iovec ) <nl> } <nl> buffer = g_malloc ( size ); <nl> v9fs_string_init (& target ); <nl> - retval = readlink ( path . data , buffer , size ); <nl> + retval = readlink ( path . data , buffer , size - 1 ); <nl> if ( retval > 0 ) { <nl> buffer [ retval ] = '\ 0 '; <nl> v9fs_string_sprintf (& target , "% s ", buffer );
int kvm_arch_release_virq_post ( int virq ) <nl> if ( entry -> virq == virq ) { <nl> trace_kvm_x86_remove_msi_route ( virq ); <nl> QLIST_REMOVE ( entry , list ); <nl> + g_free ( entry ); <nl> break ; <nl> } <nl> }
static int execute_command ( BlockDriverState * bdrv , <nl> r -> io_header . flags |= SG_FLAG_DIRECT_IO ; <nl>  <nl> r -> req . aiocb = bdrv_aio_ioctl ( bdrv , SG_IO , & r -> io_header , complete , r ); <nl> + if ( r -> req . aiocb == NULL ) { <nl> + return - EIO ; <nl> + } <nl>  <nl> return 0 ; <nl> }
void framebuffer_update_display ( <nl>  <nl> i = * first_row ; <nl> * first_row = - 1 ; <nl> - src_len = src_width * rows ; <nl> + src_len = ( hwaddr ) src_width * rows ; <nl>  <nl> mem = mem_section -> mr ; <nl> if (! mem ) {
static void spapr_cpu_core_realize ( DeviceState * dev , Error ** errp ) <nl> if ( local_err ) { <nl> goto err ; <nl> } <nl> + object_unref ( obj ); <nl> } <nl> object_child_foreach ( OBJECT ( dev ), spapr_cpu_core_realize_child , & local_err ); <nl> if ( local_err ) {
static void ppc_spapr_init ( MachineState * machine ) <nl>  <nl> /* Set up Interrupt Controller before we create the VCPUs */ <nl> spapr -> icp = xics_system_init ( machine , <nl> - smp_cpus * kvmppc_smt_threads () / smp_threads , <nl> + DIV_ROUND_UP ( smp_cpus * kvmppc_smt_threads (), <nl> + smp_threads ), <nl> XICS_IRQS ); <nl>  <nl> /* init CPUs */
bool address_space_access_valid ( AddressSpace * as , hwaddr addr , int len , bool is_ <nl> if (! memory_access_is_direct ( mr , is_write )) { <nl> l = memory_access_size ( mr , l , addr ); <nl> if (! memory_region_access_valid ( mr , xlat , l , is_write )) { <nl> + rcu_read_unlock (); <nl> return false ; <nl> } <nl> }
static void apic_timer_update ( APICState * s , int64_t current_time ) <nl> d = ( current_time - s -> initial_count_load_time ) >> <nl> s -> count_shift ; <nl> if ( s -> lvt [ APIC_LVT_TIMER ] & APIC_LVT_TIMER_PERIODIC ) { <nl> + if (! s -> initial_count ) <nl> + goto no_timer ; <nl> d = (( d / (( uint64_t ) s -> initial_count + 1 )) + 1 ) * (( uint64_t ) s -> initial_count + 1 ); <nl> } else { <nl> if ( d >= s -> initial_count )
static int pci_add_option_rom ( PCIDevice * pdev , bool is_default_rom ) <nl> if ( size < 0 ) { <nl> error_report ("% s : failed to find romfile \"% s \"", <nl> __FUNCTION__ , pdev -> romfile ); <nl> + qemu_free ( path ); <nl> return - 1 ; <nl> } <nl> if ( size & ( size - 1 )) {
BlockBackend * blk_new_open ( const char * filename , const char * reference , <nl> } <nl>  <nl> blk -> root = bdrv_root_attach_child ( bs , " root ", & child_root , <nl> - perm , BLK_PERM_ALL , blk , & error_abort ); <nl> + perm , BLK_PERM_ALL , blk , errp ); <nl> + if (! blk -> root ) { <nl> + bdrv_unref ( bs ); <nl> + blk_unref ( blk ); <nl> + return NULL ; <nl> + } <nl>  <nl> return blk ; <nl> }
static void vga_update_memory_access ( VGACommonState * s ) <nl> size = 0x8000 ; <nl> break ; <nl> case 3 : <nl> + default : <nl> base = 0xb8000 ; <nl> size = 0x8000 ; <nl> break ;
static void unix_process_msgfd ( CharDriverState * chr , struct msghdr * msg ) <nl> if ( fd < 0 ) <nl> continue ; <nl>  <nl> + /* O_NONBLOCK is preserved across SCM_RIGHTS so reset it */ <nl> + qemu_set_block ( fd ); <nl> + <nl> # ifndef MSG_CMSG_CLOEXEC <nl> qemu_set_cloexec ( fd ); <nl> # endif
static void fill_prefetch_fifo ( struct omap_gpmc_s * s ) <nl> if ( bytes > s -> prefetch . count ) { <nl> bytes = s -> prefetch . count ; <nl> } <nl> + if ( is16bit ) { <nl> + bytes &= ~ 1 ; <nl> + } <nl> + <nl> s -> prefetch . count -= bytes ; <nl> s -> prefetch . fifopointer += bytes ; <nl> fptr = 64 - s -> prefetch . fifopointer ;
static int send_status ( int sockfd , struct iovec * iovec , int status ) <nl> */ <nl> msg_size = proxy_marshal ( iovec , 0 , " ddd ", header . type , <nl> header . size , status ); <nl> + if ( msg_size < 0 ) { <nl> + return msg_size ; <nl> + } <nl> retval = socket_write ( sockfd , iovec -> iov_base , msg_size ); <nl> if ( retval < 0 ) { <nl> return retval ;
R_API void r_core_anal_autoname_all_fcns ( RCore * core ) { <nl> r_flag_rename ( core -> flags , r_flag_get ( core -> flags , fcn -> name ), name ); <nl> free ( fcn -> name ); <nl> fcn -> name = name ; <nl> + } else { <nl> + free ( name ); <nl> } <nl> } <nl> }
static int r_core_search_rop ( RCore * core , ut64 from , ut64 to , int opt , const cha <nl> } <nl> if ( mode == ' j ') <nl> r_cons_printf ("]\ n "); <nl> + free ( buf ); <nl> return R_TRUE ; <nl> } <nl> 
static int cmd_print ( void * data , const char * input ) { <nl> free ( loc_buf ); <nl> } <nl> r_list_foreach ( f -> bbs , locs_it , b ) { <nl> - if (! first ) { <nl> + if ( first ) { <nl> first = false ; <nl> } else { <nl> r_cons_print (",");
R_API int r_debug_syscall ( RDebug * dbg , int num ) { <nl>  <nl> R_API int r_debug_kill ( RDebug * dbg , int pid , int tid , int sig ) { <nl> int ret = R_FALSE ; <nl> + if ( r_debug_is_dead ( dbg )) <nl> + return R_FALSE ; <nl> if ( dbg -> h && dbg -> h -> kill ) <nl> ret = dbg -> h -> kill ( dbg , pid , tid , sig ); <nl> else eprintf (" Backend does not implements kill ()\ n ");
static void r_core_debug_kill ( RCore * core , const char * input ) { <nl> // - trace <nl> // - stop <nl> if ( signum < 1 ) signum = r_debug_signal_resolve ( core -> dbg , name ); <nl> - if ( signal > 0 ) { <nl> + if ( signum > 0 ) { <nl> int sigopt = 0 ; <nl> if ( strchr ( p , ' s ')) sigopt |= R_DBG_SIGNAL_SKIP ; <nl> if ( strchr ( p , ' c ')) sigopt |= R_DBG_SIGNAL_CONT ;
mgt_sigchld ( struct ev * e , int what ) <nl> ev_poker = NULL ; <nl>  <nl> r = wait4 (- 1 , & status , WNOHANG , NULL ); <nl> - if ( r != child_pid ) { <nl> + if ( r != child_pid || r == - 1 ) { <nl> fprintf ( stderr , " Unknown child died pid =% d status = 0x % x \ n ", <nl> r , status ); <nl> return ( 0 );
SES_RefSrcAddr ( struct sess * sp ) <nl> c3 = c ; <nl> continue ; <nl> } <nl> - TAILQ_REMOVE ( ch , c2 , list ); <nl> - free ( c2 ); <nl> + TAILQ_REMOVE ( ch , c , list ); <nl> + free ( c ); <nl> VSL_stats -> n_srcaddr --; <nl> } <nl> if ( c3 == NULL ) {
void reap_them_all ( int signum ) { <nl> } <nl>  <nl> for ( i = 0 ; i < uwsgi . mules_cnt ; i ++) { <nl> + if (! uwsgi . mules ) break ; <nl> if ( uwsgi . mules [ i ]. pid > 0 ) <nl> kill ( uwsgi . mules [ i ]. pid , SIGKILL ); <nl> }
namespace <nl> char * endptr ; <nl> int res = strtol ( index_str , & endptr , 10 ); <nl>  <nl> - if (* endptr != ']' || res > int ( game_config :: max_loop )) <nl> + if (* endptr != ']' || res > int ( game_config :: max_loop ) || endptr == index_str ) <nl> { <nl> throw invalid_variablename_exception (); <nl> }
void room_manager :: load_config ( const config & cfg ) <nl> { <nl> filename_ = cfg [" room_save_file "]; <nl> compress_stored_rooms_ = utils :: string_bool ( cfg [" compress_stored_rooms "], true ); <nl> - new_room_policy_ = pp_from_string ( cfg [" new_room_policy "]); <nl> + PRIVILEGE_POLICY pp = pp_from_string ( cfg [" new_room_policy "]); <nl> + if ( pp != PP_COUNT ) new_room_policy_ = pp ; <nl> } <nl>  <nl> const std :: string & room_manager :: storage_filename () const
void play_multiplayer ( display & disp , game_data & units_data , config cfg , <nl> } else if ( result < int ( choices . size ()/ 3 )* 2 ) { <nl> controller = " ai "; <nl> result -= choices . size ()/ 3 ; <nl> + sides [ res ]-> values [" description "] = ""; <nl> } else { <nl> controller = " network "; <nl> result -= ( choices . size ()/ 3 )* 2 ;
namespace { <nl> bool terrain_matches_filter ( const gamemap & map , const gamemap :: location & loc , const vconfig & cfg , <nl> const gamestatus & game_status , const unit_map & units , const bool flat_tod , <nl> const size_t max_loop ) <nl> -{ <nl> +{ <nl> + if (! map . on_board ( loc )) return false ; <nl> + <nl> // handle radius <nl> const size_t radius = minimum < size_t >( max_loop , <nl> lexical_cast_default < size_t >( cfg [" radius "], 0 ));
int battle_context :: choose_attacker_weapon ( const unit & attacker , <nl> attacker_combatant_ = new combatant (* attacker_stats_ ); <nl> defender_combatant_ = new combatant (* defender_stats_ , prev_def ); <nl> attacker_combatant_ -> fight (* defender_combatant_ ); <nl> + } else { <nl> + if ( attacker_stats_ -> disable ) { <nl> + delete attacker_stats_ ; <nl> + attacker_stats_ = nullptr ; <nl> + continue ; <nl> + } <nl> } <nl> if (! best_att_comb || better_combat (* attacker_combatant_ , * defender_combatant_ , <nl> * best_att_comb , * best_def_comb , harm_weight )) {
int dhcp_server_handle_message ( sd_dhcp_server * server , DHCPMessage * message , <nl> lease -> address = req -> requested_ip ; <nl> lease -> client_id . data = memdup ( req -> client_id . data , <nl> req -> client_id . length ); <nl> - if (! lease -> client_id . data ) <nl> + if (! lease -> client_id . data ) { <nl> + free ( lease ); <nl> return - ENOMEM ; <nl> + } <nl> lease -> client_id . length = req -> client_id . length ; <nl> } else <nl> lease = existing_lease ;
int config_parse_strv ( <nl> if (* sv ) <nl> for ( k = 0 ; (* sv )[ k ]; k ++) <nl> n [ k ] = (* sv )[ k ]; <nl> + else <nl> + k = 0 ; <nl>  <nl> FOREACH_WORD_QUOTED ( w , l , rvalue , state ) <nl> if (!( n [ k ++] = strndup ( w , l )))
int main ( int argc , char ** argv ) { <nl> } <nl> printf ("% s ... ", name ); <nl> fflush ( stdout ); <nl> - ( void ) LLVMFuzzerTestOneInput (( uint8_t *) buf , size ); <nl> + ( void ) LLVMFuzzerTestOneInput (( uint8_t *) buf , size ); <nl> printf (" ok \ n "); <nl> } <nl> return EXIT_SUCCESS ;
static void mount_set_state ( Mount * m , MountState state ) { <nl> state == MOUNT_REMOUNTING_SIGKILL || <nl> state == MOUNT_UNMOUNTING_SIGTERM || <nl> state == MOUNT_UNMOUNTING_SIGKILL || <nl> - state == MOUNT_FAILED ) <nl> - mount_notify_automount ( m , - ENODEV ); <nl> + state == MOUNT_FAILED ) { <nl> + if ( state != old_state ) <nl> + mount_notify_automount ( m , - ENODEV ); <nl> + } <nl>  <nl> if ( state != old_state ) <nl> log_debug ("% s changed % s -> % s ",
static int adm_settle ( struct udev * udev , int argc , char * argv []) { <nl> break ; <nl> } <nl>  <nl> - if ( timeout > 0 && now ( CLOCK_MONOTONIC ) >= deadline ) <nl> + if ( now ( CLOCK_MONOTONIC ) >= deadline ) <nl> break ; <nl>  <nl> /* wake up when queue is empty */
int base_filesystem_create ( const char * root ) { <nl> const char * target = NULL ; <nl> const char * s ; <nl>  <nl> + if ( faccessat ( fd , table [ i ]. dir , F_OK , AT_SYMLINK_NOFOLLOW ) >= 0 ) <nl> + continue ; <nl> + <nl> /* check if one of the targets exists */ <nl> NULSTR_FOREACH ( s , table [ i ]. target ) { <nl> if ( faccessat ( fd , s , F_OK , AT_SYMLINK_NOFOLLOW ) < 0 )
int bus_event_loop_with_idle ( <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> - if ( r == 0 && ! exiting ) { <nl> + if ( r == 0 && ! exiting && idle ) { <nl>  <nl> r = sd_bus_try_close ( bus ); <nl> if ( r == - EBUSY )
void main_deliver_fn ( <nl> log_printf ( instance -> totemsrp_log_level_security , " Received message is too short ... ignoring % d .\ n ", msg_len ); <nl> return ; <nl> } <nl> - <nl> + <nl> + if (( int ) message_header -> type >= totemsrp_message_handlers . count ) { <nl> + log_printf ( instance -> totemsrp_log_level_security , " Type of received message is wrong ... ignoring % d .\ n ", ( int ) message_header -> type ); <nl> + return ; <nl> + } <nl> + <nl> /* <nl> * Handle incoming message <nl> */
DllMain ( HINSTANCE hinstDLL , <nl> static const gchar * <nl> _ev_win32_get_locale_dir ( HMODULE module ) <nl> { <nl> + if ( locale_dir ) <nl> + return locale_dir ; <nl> + <nl> gchar * install_dir = NULL , * utf8_locale_dir ; <nl> gchar * retval = NULL ; <nl> 
ev_view_button_release_event ( GtkWidget * widget , <nl> view -> pressed_button = - 1 ; <nl>  <nl> return TRUE ; <nl> - } <nl> + } <nl> + <nl> + if ( view -> pressed_button == 1 && event -> state & GDK_CONTROL_MASK ) { <nl> + view -> pressed_button = - 1 ; <nl> + return TRUE ; <nl> + } <nl>  <nl> if ( view -> drag_info . in_drag ) { <nl> view -> drag_info . release_timeout_id =
end_element_handler ( GMarkupParseContext * markup_context , <nl> case STATE_INSIDE_ALT : <nl> case STATE_INSIDE_BAG : <nl> case STATE_INSIDE_SEQ : <nl> + if ( context -> property && context -> prop_cur_value < 0 ) <nl> + { <nl> + g_free ( context -> property ); <nl> + context -> property = NULL ; <nl> + } <nl> context -> state = STATE_INSIDE_PROPERTY ; <nl> break ; <nl> 
gimp_levels_tool_config_notify ( GimpFilterTool * filter_tool , <nl> GIMP_FILTER_TOOL_CLASS ( parent_class )-> config_notify ( filter_tool , <nl> config , pspec ); <nl>  <nl> - if (! levels_tool -> channel_menu ) <nl> + if (! levels_tool -> channel_menu || <nl> + ! levels_tool -> histogram_view ) <nl> return ; <nl>  <nl> if (! strcmp ( pspec -> name , " linear "))
gimp_image_map_tool_initialize ( GimpTool * tool , <nl>  <nl> window = gimp_display_shell_get_window ( display_shell ); <nl>  <nl> - image_map_tool -> overlay = gimp_image_window_get_fullscreen ( window ); <nl> + /* disabled for at least GIMP 2 . 8 */ <nl> + image_map_tool -> overlay = FALSE ; <nl>  <nl> if ( image_map_tool -> overlay ) <nl> {
void <nl> _hb_ot_shape_complex_override_features_indic ( hb_ot_map_builder_t * map , <nl> const hb_segment_properties_t * props HB_UNUSED ) <nl> { <nl> + /* Uniscribe does not apply ' kern '. */ <nl> + if ( indic_options (). uniscribe_bug_compatible ) <nl> + map -> add_feature ( HB_TAG (' k ',' e ',' r ',' n '), 0 , true ); <nl> } <nl>  <nl> 
_rl_fix_last_undo_of_type ( type , start , end ) <nl>  <nl> for ( rl = rl_undo_list ; rl ; rl = rl -> next ) <nl> { <nl> - if ( rl -> what == ( uint ) type ) <nl> + if ( rl -> what == ( unsigned int ) type ) <nl> { <nl> rl -> start = start ; <nl> rl -> end = end ;
btr_cur_optimistic_insert ( <nl> buf_block_get_page_no ( block ), max_size , <nl> rec_size + PAGE_DIR_SLOT_SIZE , index -> type ); <nl> # endif <nl> - if ( leaf <nl> - && ! dict_index_is_clust ( index ) <nl> - && ! dict_index_is_ibuf ( index )) { <nl> + if ( leaf && ! dict_index_is_clust ( index )) { <nl> /* Update the free bits of the B - tree page in the <nl> insert buffer bitmap . */ <nl> 
i_s_zip_fill_low ( <nl>  <nl> DBUG_ENTER (" i_s_zip_fill_low "); <nl>  <nl> + /* deny access to non - superusers */ <nl> + if ( check_global_access ( thd , SUPER_ACL )) { <nl> + <nl> + DBUG_RETURN ( 0 ); <nl> + } <nl> + <nl> /* Determine log2 ( PAGE_ZIP_MIN_SIZE / 2 / BUF_BUDDY_LOW ). */ <nl> for ( uint r = PAGE_ZIP_MIN_SIZE / 2 / BUF_BUDDY_LOW ; r >>= 1 ; y ++); <nl> 
toku_minicron_change_period ( struct minicron * p , u_int32_t new_period ) <nl> int <nl> toku_minicron_shutdown ( struct minicron * p ) { <nl> int r = toku_pthread_mutex_lock (& p -> mutex ); assert ( r == 0 ); <nl> + assert (! p -> do_shutdown ); <nl> p -> do_shutdown = TRUE ; <nl> // printf ("% s :% d signalling \ n ", __FILE__ , __LINE__ ); <nl> r = toku_pthread_cond_signal (& p -> condvar ); assert ( r == 0 );
 <nl> # include < File . hpp > <nl>  <nl> +// alt use PATH_MAX <nl> +# ifndef MAXPATHLEN <nl> +# define MAXPATHLEN 1024 <nl> +# endif <nl> + <nl> // <nl> // PUBLIC <nl> //
sp_head :: check_unresolved_goto () <nl> if ( m_backpatch_goto . elements > 0 ) <nl> { <nl> List_iterator_fast < bp_t > li ( m_backpatch_goto ); <nl> - bp_t * bp ; <nl> - while (( bp = li ++)) <nl> + while ( bp_t * bp = li ++) <nl> { <nl> - if (( bp -> instr_type == GOTO )) <nl> + if ( bp -> instr_type == GOTO ) <nl> { <nl> my_error ( ER_SP_LILABEL_MISMATCH , MYF ( 0 ), " GOTO ", bp -> lab -> name . str ); <nl> has_unresolved_label = true ;
__weak_alias ( vis , _vis ) <nl> # define MAXEXTRAS 5 <nl>  <nl>  <nl> - char * MAKEEXTRALIST ( uint flag , const char * orig ) <nl> + char * MAKEEXTRALIST ( unsigned int flag , const char * orig ) <nl> { <nl> const char * o = orig ; <nl> char * e , * extra ;
int decimal_round ( decimal * from , decimal * to , int scale , decimal_round_mode mode <nl> error = E_DEC_TRUNCATED ; <nl> } <nl>  <nl> - if ( scale + from -> intg < 0 ) <nl> + if ( scale + from -> intg <= 0 ) <nl> { <nl> decimal_make_zero ( to ); <nl> return E_DEC_OK ;
generate_v2_networkstatus ( void ) <nl> or_options_t * options = get_options (); <nl> char fingerprint [ FINGERPRINT_LEN + 1 ]; <nl> char ipaddr [ INET_NTOA_BUF_LEN + 1 ]; <nl> - char published [ ISO_TIME_LEN ]; <nl> + char published [ ISO_TIME_LEN + 1 ]; <nl> char digest [ DIGEST_LEN ]; <nl> struct in_addr in ; <nl> uint32_t addr ;
rend_config_services ( const or_options_t * options , int validate_only ) <nl> log_warn ( LD_CONFIG , <nl> " HiddenServiceAllowUnknownPorts should be 0 or 1 , not % s ", <nl> line -> value ); <nl> - smartlist_free ( temp_service_list ); <nl> goto free_and_return ; <nl> } <nl> log_info ( LD_CONFIG ,
entry_guard_register_connect_status ( const char * digest , int succeeded , <nl> entry -> nickname , buf , tbuf ); <nl> entry -> last_attempted = now ; <nl> } <nl> - entry -> can_retry = 0 ; /* We gave it an early chance ; no good . */ <nl> + if ( entry ) <nl> + entry -> can_retry = 0 ; /* We gave it an early chance ; no good . */ <nl> } <nl>  <nl> if ( first_contact ) {
add_an_entry_guard ( routerinfo_t * chosen ) <nl>  <nl> again : <nl> if (-- tries_left <= 0 ) { <nl> - log_warn ( LD_CIRC , " Tried finding a new entry , but failed . Bad news . XXX ."); <nl> + log_warn ( LD_CIRC , " Tried finding a new entry guard , but failed . " <nl> + " Can you reach the Tor network ?"); <nl> return NULL ; <nl> } <nl> if ( chosen )
imalloc ( size_t size ) <nl> ptralloc = 1 ; <nl> size = malloc_pagesize ; <nl> } <nl> - if (( size + malloc_pagesize ) < size ) { /* Check for overflow */ <nl> + if ( size > SIZE_MAX - malloc_pagesize ) { /* Check for overflow */ <nl> result = NULL ; <nl> errno = ENOMEM ; <nl> } else if ( size <= malloc_maxsize )
# include < algorithm > <nl> # include < functional > <nl>  <nl> +// R - value references and std :: move <nl> +# if defined ( __cplusplus >= 201103L ) <nl> +# include < utility > <nl> +# endif <nl> + <nl> # ifdef CRYPTOPP_INCLUDE_VECTOR_CC <nl> // workaround needed on Sun Studio 12u1 Sun C ++ 5 . 10 SunOS_i386 128229 - 02 2009 / 09 / 21 <nl> # include < vector . cc >
typedef unsigned int word32 ; <nl> # if defined ( _MSC_VER ) || defined ( __BORLANDC__ ) <nl> typedef unsigned __int64 word64 ; <nl> # define W64LIT ( x ) x ## ui64 <nl> -# elif ( _LP64 || __LP64__ ) && ! defined ( __SUNPRO_CC ) <nl> +# elif ( _LP64 || __LP64__ ) <nl> typedef unsigned long word64 ; <nl> # define W64LIT ( x ) x ## UL <nl> # else
void jshFlashErasePage ( uint32_t addr ) { <nl> if (! f ) return ; <nl> uint32_t startAddr , pageSize ; <nl> if ( jshFlashGetPage ( addr , & startAddr , & pageSize )) { <nl> + startAddr -= FLASH_START ; <nl> fseek ( f , startAddr , SEEK_SET ); <nl> char * buf = malloc ( pageSize ); <nl> memset ( buf , 0xFF , pageSize );
static void * _agent ( void * x ) <nl> free_buf ( buffer ); <nl> fail_time = 0 ; <nl> } else { <nl> + /* We still need to free a mult_msg even if we <nl> + got a failure . <nl> + */ <nl> + if ( list_msg . my_list ) { <nl> + list_msg . my_list = NULL ; <nl> + free_buf ( buffer ); <nl> + } <nl> + <nl> fail_time = time ( NULL ); <nl> } <nl> slurm_mutex_unlock (& agent_lock );
bool slurm_container_has_pid ( uint32_t cont_id , pid_t pid ) <nl>  <nl> int slurm_container_wait ( uint32_t id ) <nl> { <nl> - if ( _job_waitjid (( jid_t ) id , NULL , 0 ) == ( jid_t )- 1 ) <nl> + int status ; <nl> + if ( _job_waitjid (( jid_t ) id , & status , 0 ) == ( jid_t )- 1 ) <nl> return SLURM_ERROR ; <nl>  <nl> return SLURM_SUCCESS ;
char * slurm_sprint_partition_info ( partition_info_t * part_ptr , <nl> if ( part_ptr -> disable_root_jobs ) <nl> sprintf ( tmp_line , " DisableRootJobs = YES "); <nl> else <nl> - sprintf ( tmp_line , " DisableRootJobs = NO "); <nl> + sprintf ( tmp_line , " DisableRootJobs = NO "); <nl> xstrcat ( out , tmp_line ); <nl>  <nl> if ( part_ptr -> hidden )
geocache_cfg * geocache_configuration_create ( apr_pool_t * pool ) { <nl> grid -> srs = apr_pstrdup ( pool ," epsg : 4326 "); <nl> grid -> unit = GEOCACHE_UNIT_DEGREES ; <nl> grid -> tile_sx = grid -> tile_sy = 256 ; <nl> - grid -> nlevels = 16 ; <nl> + grid -> nlevels = 19 ; <nl> grid -> extent [ 0 ] = wgs84_extent [ 0 ]; <nl> grid -> extent [ 1 ] = wgs84_extent [ 1 ]; <nl> grid -> extent [ 2 ] = wgs84_extent [ 2 ];
int msDumpLayer ( mapObj * map , layerObj * lp , int nVersion , const char * script_url_ <nl> free ( nestedGroups ); <nl> free ( numNestedGroups ); <nl> free ( isUsedInNestedGroup ); <nl> + free ( group_layers ); <nl> } <nl> } <nl> }
read_pipes ( int outfd , gchar ** out_str , int errfd , gchar ** err_str ) <nl> err_closed = ( nread <= 0 ); <nl> } <nl> } <nl> - <nl> - } while ( res == - 1 && errno == EINTR ); <nl> + } while ( res > 0 || ( res == - 1 && errno == EINTR )); <nl>  <nl> g_free ( buffer ); <nl> if ( out_str )
mono_allocate_stack_slots2 ( MonoCompile * cfg , gboolean backward , guint32 * stack_ <nl> if ( cfg -> disable_reuse_stack_slots ) <nl> reuse_slot = FALSE ; <nl>  <nl> + t = mini_get_underlying_type ( cfg , t ); <nl> switch ( t -> type ) { <nl> case MONO_TYPE_GENERICINST : <nl> if (! mono_type_generic_inst_is_valuetype ( t )) {
mono_method_to_ir ( MonoCompile * cfg , MonoMethod * method , MonoBasicBlock * start_b <nl> MonoMethod * cil_method ; <nl>  <nl> if ( method -> wrapper_type != MONO_WRAPPER_NONE ) { <nl> - if ( cfg -> verbose_level > 2 ) <nl> + if ( constrained_call && cfg -> verbose_level > 2 ) <nl> printf (" DM Constrained call to % s \ n ", mono_type_get_full_name ( constrained_call )); <nl> cmethod = ( MonoMethod *) mono_method_get_wrapper_data ( method , token ); <nl> cil_method = cmethod ;
try <nl> rccS . send ( command ); <nl> string receive = rccS . recv (); <nl> cout << receive ; <nl> - exit ( 0 ); <nl> + return 0 ; <nl> } <nl> catch ( AhuException & ae ) <nl> { <nl> cerr <<" Fatal : "<< ae . reason <<"\ n "; <nl> - exit ( 1 ); <nl> + return 1 ; <nl> }
void GeoIPBackend :: initialize () { <nl> } <nl> } <nl>  <nl> - tmp_domains . push_back ( dom ); <nl> + tmp_domains . push_back ( std :: move ( dom )); <nl> } <nl>  <nl> s_domains . clear ();
static GF_Err gf_isom_parse_movie_boxes_internal ( GF_ISOFile * mov , u32 * boxType , <nl> if ( pos < 0 ) pos = 0 ; <nl> gf_list_insert ( mov -> TopBoxes , brand , pos ); <nl> } <nl> + gf_isom_box_del ( a ); <nl> } <nl> break ; <nl> 
static GF_Err ft_set_font ( GF_FontReader * dr , const char * OrigFontName , u32 style <nl> opt = gf_modules_get_option (( GF_BaseInterface *) dr , " FontEngine ", fname ); <nl>  <nl> if ( opt ) { <nl> - gf_free ( fname ); <nl> FT_Face face ; <nl> + gf_free ( fname ); <nl> if ( FT_New_Face ( ftpriv -> library , opt , 0 , & face )) return GF_IO_ERR ; <nl> if (! face ) return GF_IO_ERR ; <nl> gf_list_add ( ftpriv -> loaded_fonts , face );
static int jcop_set_security_env ( sc_card_t * card , <nl> tmp . algorithm_ref |= 0x10 ; <nl> if ( tmp . algorithm_flags & SC_ALGORITHM_RSA_HASH_MD5 ) <nl> tmp . algorithm_ref |= 0x20 ; <nl> - env =& tmp ; <nl> + <nl> + memcpy ( env , & tmp , sizeof ( struct sc_security_env )); <nl> } <nl>  <nl> sc_format_apdu ( card , & apdu , SC_APDU_CASE_3_SHORT , 0x22 , 0xC1 , 0 );
jpki_finish ( sc_card_t * card ) <nl> struct jpki_private_data * drvdata = JPKI_DRVDATA ( card ); <nl>  <nl> LOG_FUNC_CALLED ( card -> ctx ); <nl> - <nl> + if ( drvdata -> mf ) { <nl> + free ( drvdata -> mf ); <nl> + drvdata -> mf = NULL ; <nl> + } <nl> if ( drvdata ) { <nl> free ( drvdata ); <nl> card -> drv_data = NULL ;
static int part10_build_modify_pin_block ( struct sc_reader * reader , u8 * buf , siz <nl> pin_modify -> bInsertionOffsetNew = 0x00 ; <nl> } <nl>  <nl> - if (! data -> pin1 . min_length || ! data -> pin1 . max_length ) <nl> + if (!( data -> flags & SC_PIN_CMD_IMPLICIT_CHANGE ) <nl> + && (! data -> pin1 . min_length || ! data -> pin1 . max_length )) <nl> return SC_ERROR_INVALID_ARGUMENTS ; <nl>  <nl> tmp16 = ( data -> pin1 . min_length << 8 ) + data -> pin1 . max_length ;
sc_keycache_forget_key ( const sc_path_t * path , int type , int ref ) <nl> while (( s = * prev ) != NULL ) { <nl> if ( __match_entry ( s , type , ref , path , 1 )) { <nl> * prev = s -> next ; <nl> + if ( s -> named_pin != - 1 && s -> ref == - 1 ) <nl> + named_pin [ s -> named_pin ] = NULL ; <nl> memset ( s , 0 , sizeof (* s )); <nl> free ( s ); <nl> } else {
sc_parse_ef_atr_content ( struct sc_card * card , unsigned char * buf , size_t buflen ) <nl>  <nl> category = * buf ; <nl>  <nl> + memset (& ef_atr , 0 , sizeof ( struct sc_ef_atr )); <nl> /* IAS / ECC specific : skip second ' zero ' byte */ <nl> if (*(++ buf ) == 0x00 ) <nl> ++ buf ;
static int tcos_decipher ( sc_card_t * card , const u8 * crgram , size_t crgram_len , <nl> apdu . data = sbuf ; <nl> apdu . lc = apdu . datalen = crgram_len + 1 ; <nl> sbuf [ 0 ] = tcos3 ? 0x00 : (( data -> pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1 ) ? 0x81 : 0x02 ); <nl> + if ( sizeof sbuf - 1 < crgram_len ) <nl> + return SC_ERROR_INVALID_ARGUMENTS ; <nl> memcpy ( sbuf + 1 , crgram , crgram_len ); <nl>  <nl> r = sc_transmit_apdu ( card , & apdu );
CK_RV attr_extract ( CK_ATTRIBUTE_PTR pAttr , void * ptr , size_t * sizep ) <nl> size = sizeof ( CK_KEY_TYPE ); break ; <nl> case CKA_PRIVATE : <nl> size = sizeof ( CK_BBOOL ); break ; <nl> + case CKA_CERTIFICATE_TYPE : <nl> + size = sizeof ( CKA_CERTIFICATE_TYPE ); break ; <nl> default : <nl> return CKR_FUNCTION_FAILED ; <nl> }
pltcl_returnnext ( ClientData cdata , Tcl_Interp * interp , <nl> Datum retval ; <nl> bool isNull = false ; <nl>  <nl> + /* for paranoia ' s sake , check that tupdesc has exactly one column */ <nl> + if ( call_state -> ret_tupdesc -> natts != 1 ) <nl> + elog ( ERROR , " wrong result type supplied in return_next "); <nl> + <nl> retval = InputFunctionCall (& prodesc -> result_in_func , <nl> utf_u2e (( char *) Tcl_GetString ( objv [ 1 ])), <nl> prodesc -> result_typioparam ,
void LibEventTransport :: sendImpl ( const void * data , int size , int code , <nl> } else { <nl> if ( m_method != HEAD ) { <nl> evbuffer_add ( m_request -> output_buffer , data , size ); <nl> + } else { <nl> + char buf [ 11 ]; <nl> + snprintf ( buf , sizeof ( buf ), "% d ", size ); <nl> + addHeaderImpl (" Content - Length ", buf ); <nl> } <nl> m_server -> onResponse ( m_workerId , m_request , code ); <nl> m_sendEnded = true ;
namespace XrdCl <nl>  <nl> XrdSysPwd pwdHandler ; <nl> passwd * pwd = pwdHandler . Get ( getuid () ); <nl> + if ( ! pwd ) return ; <nl> std :: string userPlugIns = pwd -> pw_dir ; <nl> userPlugIns += "/. xrootd / client . plugins . d "; <nl> ProcessConfigDir ( userPlugIns );
namespace XrdCl <nl> pFileUrl ( 0 ), <nl> pDataServer ( 0 ), <nl> pLoadBalancer ( 0 ), <nl> + pStateRedirect ( 0 ), <nl> pFileHandle ( 0 ), <nl> pOpenMode ( 0 ), <nl> pOpenFlags ( 0 ),
static void mygroup_delete ( mygroup_t * mg ) <nl> } <nl>  <nl> metadata_delete_all ( mg ); <nl> + BlockHeapFree ( mygroup_heap , mg ); <nl> } <nl>  <nl> mygroup_t * mygroup_add ( const char * name ) <nl> mygroup_t * mygroup_find ( const char * name ) <nl> static void groupacs_des ( groupacs_t * ga ) <nl> { <nl> metadata_delete_all ( ga ); <nl> - /* XXX nothing */ <nl> + BlockHeapFree ( groupacs_heap , ga ); <nl> } <nl>  <nl> groupacs_t * groupacs_add ( mygroup_t * mg , myuser_t * mu , unsigned int flags )
PHP_FUNCTION ( swoole_server_sendfile ) <nl> memcpy ( buffer , filename , send_data . info . len ); <nl> buffer [ send_data . info . len ] = 0 ; <nl> send_data . info . len ++; <nl> + send_data . length = 0 ; <nl>  <nl> send_data . data = buffer ; <nl> SW_CHECK_RETURN ( serv -> factory . finish (& serv -> factory , & send_data ));
NativeWindowWin :: NativeWindowWin ( content :: WebContents * web_contents , <nl> OnViewWasResized (); <nl>  <nl> if ( g_exe_icon == NULL ) <nl> - g_exe_icon = :: LoadImage ( GetModuleHandle ( NULL ), L " IDR_MAINFRAME ", <nl> + g_exe_icon = :: LoadImage ( GetModuleHandle ( NULL ), MAKEINTRESOURCE ( 1 ), <nl> IMAGE_ICON , 0 , 0 , 0 ); <nl> :: SendMessage ( window_ -> GetNativeWindow (), <nl> WM_SETICON ,
void ospf6_spf_reason_string ( unsigned int reason , char * buf , int size ) <nl> if (! buf ) <nl> return ; <nl>  <nl> - for ( bit = 0 ; bit <= ( sizeof ( ospf6_spf_reason_str ) / sizeof ( char *)); bit ++) <nl> + for ( bit = 0 ; bit < array_size ( ospf6_spf_reason_str ); bit ++) <nl> { <nl> if (( reason & ( 1 << bit )) && ( len < size )) <nl> {
int regexp_nsub ( struct regexp * r ) { <nl> } <nl>  <nl> void regexp_release ( struct regexp * regexp ) { <nl> - if ( regexp -> re != NULL ) { <nl> + if ( regexp != NULL && regexp -> re != NULL ) { <nl> regfree ( regexp -> re ); <nl> FREE ( regexp -> re ); <nl> }
int transform_save ( struct augeas * aug , struct tree * xfm , <nl> goto done ; <nl> } <nl>  <nl> + text = append_newline ( text , strlen ( text )); <nl> + <nl> // FIXME : We might have to create intermediary directories <nl> // to be able to write augnew , but we have no idea what permissions <nl> // etc . they should get . Just the process default ?
namespace tnt <nl> explicit HttpReply ( std :: ostream & s , bool sendStatusLine = true ); <nl>  <nl> void setContentType ( const char * t ) { setHeader ( httpheader :: contentType , t ); } <nl> + void setContentType ( const std :: string & t ) { setHeader ( httpheader :: contentType , t ); } <nl> const char * getContentType () const { return getHeader ( httpheader :: contentType ); } <nl>  <nl> void setHeadRequest ( bool sw = true ) { headRequest = sw ; }
ecall ( mrb_state * mrb , int i ) <nl> mrb_value * self = mrb -> c -> stack ; <nl> struct RObject * exc ; <nl>  <nl> + if ( i < 0 ) return ; <nl> p = mrb -> c -> ensure [ i ]; <nl> if (! p ) return ; <nl> if ( mrb -> c -> ci -> eidx > i )
genop_peep ( codegen_scope * s , mrb_code i , int val ) <nl> return 0 ; <nl> } <nl> } <nl> + if ( c0 == OP_LOADNIL ) { <nl> + if ( GETARG_B ( i ) == GETARG_A ( i0 )) { <nl> + s -> pc --; <nl> + return 0 ; <nl> + } <nl> + } <nl> break ; <nl> case OP_JMPIF : <nl> case OP_JMPNOT :
whack_handle ( int whackctlfd ) <nl> close ( whackfd ); <nl> return ; <nl> } <nl> + memset (& msg , 0 , sizeof ( msg )); <nl> n = read ( whackfd , & msg , sizeof ( msg )); <nl> if ( n <= 0 ) <nl> {
static bool setup_half_ipsec_sa ( struct state * st , bool inbound ) <nl> case IKEv2_ENCR_AES_GCM_8 : <nl> case IKEv2_ENCR_AES_GCM_12 : <nl> case IKEv2_ENCR_AES_GCM_16 : <nl> + /* keymat contains 4 bytes of salt */ <nl> + enc_key_len += AES_GCM_SALT_BYTES ; <nl> + break ; <nl> case IKEv2_ENCR_AES_CCM_8 : <nl> case IKEv2_ENCR_AES_CCM_12 : <nl> case IKEv2_ENCR_AES_CCM_16 : <nl> - /* keymat contains 4 bytes of salt */ <nl> + /* keymat contains 3 bytes of salt */ <nl> enc_key_len += AES_CCM_SALT_BYTES ; <nl> break ; <nl> }
cherokee_validator_ldap_check ( cherokee_validator_ldap_t * ldap , <nl> /* Sanity checks <nl> */ <nl> if (( conn -> validator == NULL ) || <nl> - cherokee_buffer_is_empty (& conn -> validator -> user )) <nl> + cherokee_buffer_is_empty (& conn -> validator -> user ) || <nl> + cherokee_buffer_is_empty (& conn -> validator -> passwd )) <nl> return ret_error ; <nl>  <nl> size = cherokee_buffer_cnt_cspn (& conn -> validator -> user , 0 , "*()");
cherokee_buffer_cnt_cspn ( cherokee_buffer_t * buf , cuint_t offset , const char * st <nl> crc_t <nl> cherokee_buffer_crc32 ( cherokee_buffer_t * buf ) <nl> { <nl> + if ( cherokee_buffer_is_empty ( buf )) <nl> + return 0 ; <nl> + <nl> return crc32_sz ( buf -> buf , buf -> len ); <nl> } <nl> 
ExprResolveBoolean ( struct xkb_context * ctx , const ExprDef * expr , <nl>  <nl> case EXPR_INVERT : <nl> case EXPR_NOT : <nl> - ok = ExprResolveBoolean ( ctx , expr , set_rtrn ); <nl> + ok = ExprResolveBoolean ( ctx , expr -> unary . child , set_rtrn ); <nl> if ( ok ) <nl> * set_rtrn = !* set_rtrn ; <nl> return ok ;
get_controls ( struct xkb_keymap * keymap , xcb_connection_t * conn , <nl> xcb_xkb_get_controls_reply ( conn , cookie , NULL ); <nl>  <nl> FAIL_IF_BAD_REPLY ( reply , " XkbGetControls "); <nl> + FAIL_UNLESS ( reply -> numGroups > 0 && reply -> numGroups <= 4 ); <nl>  <nl> keymap -> enabled_ctrls = translate_controls_mask ( reply -> enabledControls ); <nl> keymap -> num_groups = reply -> numGroups ;
bool <nl> map_file ( FILE * file , char ** string_out , size_t * size_out ) <nl> { <nl> struct stat stat_buf ; <nl> - const int fd = fileno ( file ); <nl> + int fd ; <nl> char * string ; <nl>  <nl> /* Make sure to keep the errno on failure ! */ <nl> + fd = fileno ( file ); <nl> + if ( fd < 0 ) <nl> + return false ; <nl>  <nl> if ( fstat ( fd , & stat_buf ) != 0 ) <nl> return false ;
HRESULT ChakraRTInterface :: ParseConfigFlags () <nl> else <nl> { <nl> hr = WideStringToNarrowDynamic ( fileNameWide , & m_argInfo -> filename ); <nl> + SysFreeString ( fileNameWide ); <nl> if ( FAILED ( hr )) <nl> { <nl> Assert ( hr == E_OUTOFMEMORY );
time_t raptor_parse_date ( char * p , time_t * now ); <nl> int raptor_stringbuffer_append_turtle_string ( raptor_stringbuffer * stringbuffer , unsigned char * text , size_t len , int delim , raptor_simple_message_handler error_handler , void * error_data ); <nl>  <nl>  <nl> +/* raptor_xsd . c */ <nl> + raptor_identifier * raptor_new_identifier_from_double ( double d ); <nl> + <nl> /* end of RAPTOR_INTERNAL */ <nl> # endif <nl> 
MagickExport Image * WaveletDenoiseImage ( const Image * image , <nl> difference ; <nl>  <nl> difference = pixels [ low_pass ]- pixels [ high_pass ]; <nl> - pixels [ i ]+= copysignf ( fmaxf ( fabsf ( difference )- magnitude , 0 . 0f ), <nl> - difference ); <nl> + pixels [ i ]+= copysignf ( fmaxf ( fabsf ( difference )- magnitude - <nl> + softness * magnitude , 0 . 0f ), difference ); <nl> } <nl> } <nl> /*
evhttp_send_reply_start ( struct evhttp_request * req , int code , <nl> evhttp_add_header ( req -> output_headers , " Transfer - Encoding ", <nl> " chunked "); <nl> req -> chunked = 1 ; <nl> + } else { <nl> + req -> chunked = 0 ; <nl> } <nl> evhttp_make_header ( req -> evcon , req ); <nl> evhttp_write_buffer ( req -> evcon , NULL , NULL );
server_request_free_answers ( struct server_request * req ) <nl> free ( victim -> name ); <nl> if ( victim -> data ) <nl> free ( victim -> data ); <nl> - /* XXXX free ( victim ?) - NM */ <nl> + free ( victim ); <nl> victim = next ; <nl> } <nl> * list = NULL ;
static int get_port_by_socket ( int fd ) <nl> socklen_t len = sizeof ( struct sockaddr_in ); <nl> struct sockaddr_in m_addr ; <nl>  <nl> - getpeername ( fd , ( struct sockaddr *) & m_addr , & len ); <nl> + int ret = getpeername ( fd , ( struct sockaddr *) & m_addr , & len ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> return ( int ) m_addr . sin_port ; <nl> } <nl> 
recorder_record_frame ( ShellRecorder * recorder ) <nl>  <nl> size = recorder -> area . width * recorder -> area . height * 4 ; <nl>  <nl> - data = g_malloc ( recorder -> area . width * 4 * recorder -> area . height ); <nl> + data = g_malloc ( size ); <nl> cogl_framebuffer_read_pixels ( cogl_get_draw_framebuffer (), <nl> recorder -> area . x , <nl> recorder -> area . y ,
NPP_GetValue ( NPP instance , <nl>  <nl> *( NPObject **) value = funcs . createobject ( instance , & plugin_class ); <nl> break ; <nl> + <nl> + case NPPVpluginNeedsXEmbed : <nl> + *( bool *) value = TRUE ; <nl> + break ; <nl> + <nl> default : <nl> ; <nl> }
void StreamTcpReassembleMemuseCounter ( ThreadVars * tv , TcpReassemblyThreadCtx * rt <nl> * \ retval 0 if not in bounds <nl> */ <nl> int StreamTcpReassembleCheckMemcap ( uint32_t size ) { <nl> - if ( stream_config . reassembly_memcap == 0 || size + SC_ATOMIC_GET ( ra_memuse ) <= stream_config . reassembly_memcap ) <nl> + if ( stream_config . reassembly_memcap == 0 || <nl> + ( uint64_t )(( uint64_t ) size + SC_ATOMIC_GET ( ra_memuse )) <= stream_config . reassembly_memcap ) <nl> return 1 ; <nl> return 0 ; <nl> }
static int SSHParseBanner ( SshState * state , SshHeader * header , const uint8_t * inp <nl> uint32_t line_len = input_len ; <nl>  <nl> /* is it the version line ? */ <nl> - if ( SCMemcmp (" SSH -", line_ptr , 4 ) != 0 ) { <nl> + if ( line_len >= 4 && SCMemcmp (" SSH -", line_ptr , 4 ) != 0 ) { <nl> SCReturnInt (- 1 ); <nl> } <nl> 
int DetectEngineInspectHttpStatCode ( DetectEngineCtx * de_ctx , <nl> } <nl>  <nl> # ifdef DEBUG <nl> - SigMatch * sm = s -> sm_lists [ DETECT_SM_LIST_HSMDMATCH ]; <nl> + SigMatch * sm = s -> sm_lists [ DETECT_SM_LIST_HSCDMATCH ]; <nl> DetectContentData * co = ( DetectContentData *) sm -> ctx ; <nl> SCLogDebug (" co -> id %" PRIu32 , co -> id ); <nl> # endif
int SCCudaHlGetCudaModule ( CUmodule * p_module , const char * ptx_image , int handle ) <nl> if ( unlikely ( image == NULL )) { <nl> exit ( EXIT_FAILURE ); <nl> } <nl> - memset ( image , 0x0 , sizeof ( image )); <nl> + memset ( image , 0x0 , strlen ( ptx_image )+ 15 ); <nl>  <nl> int major = INT_MAX ; <nl> int minor = INT_MAX ;
int SigAddressPrepareStage4 ( DetectEngineCtx * de_ctx ) <nl>  <nl> SigGroupHeadBuildNonPrefilterArray ( de_ctx , sgh ); <nl>  <nl> + SigGroupHeadInitDataFree ( sgh -> init ); <nl> + sgh -> init = NULL ; <nl> + <nl> sgh -> id = idx ; <nl> cnt ++; <nl> }
ivec_reserve ( rust_task * task , type_desc * ty , rust_ivec * v , size_t n_elems ) <nl> } else { <nl> // On heap ; resize . <nl> heap_part = ( rust_ivec_heap *) task -> realloc ( v -> payload . ptr , <nl> - new_alloc ); <nl> + new_alloc + sizeof ( size_t )); <nl> v -> payload . ptr = heap_part ; <nl> } <nl> 
libc5 - based Linux systems . Only include it on system that are known to <nl> require it ! */ <nl> # if defined ( _AIX ) || defined ( __NOVELL_LIBC__ ) || defined ( __NetBSD__ ) || \ <nl> - defined ( __minix ) || defined ( __SYMBIAN32__ ) || defined ( __INTEGRITY ) <nl> + defined ( __minix ) || defined ( __SYMBIAN32__ ) || defined ( __INTEGRITY ) || \ <nl> + defined ( ANDROID ) <nl> # include < sys / select . h > <nl> # endif <nl> 
CURLcode Curl_perform ( struct SessionHandle * data ) <nl> * an error , use the strerror () string or if things are so bad that not <nl> * even that is good , set a bad string that mentions the error code . <nl> */ <nl> - char * str = curl_easy_strerror ( res ); <nl> + const char * str = curl_easy_strerror ( res ); <nl> if (! str ) <nl> failf ( data , " unspecified error % d ", ( int ) res ); <nl> else
int main ( void ) <nl> /* the DEBUGFUNCTION has no effect until we enable VERBOSE */ <nl> curl_easy_setopt ( curl , CURLOPT_VERBOSE , 1L ); <nl>  <nl> + /* example . com is redirected , so we tell libcurl to follow redirection */ <nl> + curl_easy_setopt ( curl , CURLOPT_FOLLOWLOCATION , 1L ); <nl> + <nl> curl_easy_setopt ( curl , CURLOPT_URL , " http :// example . com /"); <nl> res = curl_easy_perform ( curl ); <nl> /* Check for errors */
void CQuickSpriteSystem :: Add ( float * pointdata , color4ub_t color , vec2_t fog ) <nl> } <nl>  <nl> curcoord = mVerts [ mNextVert ]; <nl> - memcpy ( curcoord , pointdata , 4 * sizeof ( vec4_t )); <nl> + memcpy ( curcoord , pointdata , sizeof ( vec4_t )); <nl>  <nl> // Set up color <nl> curcolor = & mColors [ mNextVert ];
st_insert2 ( st_table * tab , st_data_t key , st_data_t value , <nl> if ( tab -> bins == NULL ) { <nl> bin = find_entry ( tab , hash_value , key ); <nl> new_p = bin == UNDEFINED_ENTRY_IND ; <nl> + if ( new_p ) <nl> + tab -> num_entries ++; <nl> bin_ind = UNDEFINED_BIN_IND ; <nl> } <nl> else {
int CloseUnreal ( HWND hWnd ) <nl> return 0 ; <nl> else <nl> { <nl> - DestroyWindow ( hWnd ); <nl> - exit ( 0 ); <nl> + DestroyWindow ( hWnd ); <nl> + TerminateProcess ( GetCurrentProcess (), 0 ); <nl> + exit ( 0 ); /* in case previous fails ( possible ?) */ <nl> } <nl> } <nl> 
runtime · equal ( Type * t , ...) <nl> uintptr ret ; <nl>  <nl> x = ( byte *)(& t + 1 ); <nl> - y = x + ROUND ( t -> size , t -> align ); <nl> + y = x + t -> size ; <nl> ret = ( uintptr )( y + t -> size ); <nl> ret = ROUND ( ret , Structrnd ); <nl> t -> alg -> equal (( bool *) ret , t -> size , x , y );
int dsa_builtin_paramgen2 ( DSA * ret , size_t L , size_t N , <nl> } else { <nl> p = BN_CTX_get ( ctx ); <nl> q = BN_CTX_get ( ctx ); <nl> + if ( q == NULL ) <nl> + goto err ; <nl> } <nl>  <nl> if (! BN_lshift ( test , BN_value_one (), L - 1 ))
int RSA_padding_check_PKCS1_OAEP ( unsigned char * to , int tlen , <nl> } <nl>  <nl> lzero = num - flen ; <nl> + if ( lzero < 0 ) <nl> + { <nl> + RSAerr ( RSA_F_RSA_PADDING_CHECK_PKCS1_OAEP , RSA_R_OAEP_DECODING_ERROR ); <nl> + return (- 1 ); <nl> + } <nl> maskeddb = from - lzero + SHA_DIGEST_LENGTH ; <nl>  <nl> MGF1 ( seed , SHA_DIGEST_LENGTH , maskeddb , dblen );
int OCSP_parse_url ( char * url , char ** phost , char ** pport , char ** ppath , int * pss <nl>  <nl>  <nl> err : <nl> + if ( buf ) OPENSSL_free ( buf ); <nl> if (* ppath ) OPENSSL_free (* ppath ); <nl> if (* pport ) OPENSSL_free (* pport ); <nl> if (* phost ) OPENSSL_free (* phost );
static int tree_init ( X509_POLICY_TREE ** ptree , STACK_OF ( X509 ) * certs , <nl> X509_check_purpose ( x , - 1 , 0 ); <nl>  <nl> /* If cache is NULL , likely ENOMEM : return immediately */ <nl> - if (( cache = policy_cache_set ( x )) == NULL ) <nl> + if ( policy_cache_set ( x ) == NULL ) <nl> return X509_PCY_TREE_INTERNAL ; <nl> } <nl> 
particular purpose .\ n "); <nl> # ifdef DMALLOC <nl> dmalloc_report (); <nl> # endif <nl> + Clp_DeleteParser ( clp ); <nl> return ( error_count ? EXIT_ERR : EXIT_OK ); <nl> }
static void cil_reset_class ( struct cil_class * class ) <nl>  <nl> static void cil_reset_perm ( struct cil_perm * perm ) <nl> { <nl> - cil_reset_classperms_list ( perm -> classperms ); <nl> + cil_list_destroy (& perm -> classperms , CIL_FALSE ); <nl> } <nl>  <nl> static inline void cil_reset_classperms ( struct cil_classperms * cp )
addListner ( instanceConf_t * inst ) <nl> lcnfLast = newlcnfinfo ; <nl> } <nl> } <nl> + } else { <nl> + errmsg . LogError ( 0 , NO_ERRCODE , " imudp : Could not create udp listener ," <nl> + " ignoring port % s bind - address % s .", <nl> + port , bindAddr ); <nl> } <nl>  <nl> finalize_it :
static EC_KEY * ec_key_new ( ErlNifEnv * env , ERL_NIF_TERM curve_arg ) <nl> } else <nl> goto out_err ; <nl>  <nl> + if (! group ) <nl> + goto out_err ; <nl> + <nl> if ( enif_inspect_binary ( env , prime [ 2 ], & seed )) { <nl> EC_GROUP_set_seed ( group , seed . data , seed . size ); <nl> }
static int cmd_handle_untagged ( struct ImapData * idata ) <nl> mutt_debug ( 2 , " Handling untagged NO \ n "); <nl>  <nl> /* Display the warning message from the server */ <nl> - mutt_error ("% s ", s + 3 ); <nl> + mutt_error ("% s ", s + 2 ); <nl> } <nl>  <nl> return 0 ;
static void * klondike_get_replies ( void * userdata ) <nl> case KLN_CMD_ABORT : <nl> // We can ' t do / check this until it ' s initialised <nl> if ( klninfo -> initialised ) { <nl> + isc = 0 ; <nl> dev = kitem -> kline . ws . dev ; <nl> wr_lock (&( klninfo -> stat_lock )); <nl> klninfo -> jobque [ dev ]. workqc = ( int )( kitem -> kline . ws . workqc );
bool initiate_stratum ( struct pool * pool ) <nl> if ( pool -> sessionid ) <nl> applog ( LOG_DEBUG , " Pool % d stratum session id : % s ", pool -> pool_no , pool -> sessionid ); <nl> else <nl> - applog ( LOG_DEBUG , " Pool % d stratum session id does not exist "); <nl> + applog ( LOG_DEBUG , " Pool % d stratum session id does not exist ", pool -> pool_no ); <nl>  <nl> ret = true ; <nl> out :
int main ( int argc , char ** argv ) <nl>  <nl> io_sources . Register ( thread_mgr , true ); <nl>  <nl> - if ( io_sources . Size () > 0 || have_pending_timers ) <nl> + if ( io_sources . Size () > 0 || have_pending_timers || BifConst :: exit_only_after_terminate ) <nl> { <nl> if ( profiling_logger ) <nl> profiling_logger -> Log ();
void qtcDefaultSettings ( Options * opts ) <nl> opts -> shadeMenubarOnlyWhenActive = false ; <nl> opts -> thin = THIN_BUTTONS ; <nl> opts -> tbarBtns = TBTN_STANDARD ; <nl> +# ifdef _WIN32 <nl> + opts -> scrollbarType = SCROLLBAR_WINDOWS ; <nl> +# elif defined __APPLE__ <nl> + opts -> scrollbarType = SCROLLBAR_NONE ; <nl> +# else <nl> opts -> scrollbarType = SCROLLBAR_KDE ; <nl> +# endif <nl> opts -> buttonEffect = EFFECT_SHADOW ; <nl> opts -> focus = FOCUS_GLOW ; <nl> opts -> lvButton = false ;
static problem_data_t * load_problem_data_if_not_yet ( problem_data_t * problem_data <nl> struct dump_dir * dd = dd_opendir ( dump_dir_name , /* flags :*/ 0 ); <nl> if (! dd ) <nl> { <nl> - problem_data_free ( problem_data ); <nl> return NULL ; <nl> } <nl> problem_data = create_problem_data_from_dump_dir ( dd );
tls_ctx_personalise_random ( struct tls_root_ctx * ctx ) <nl> const md_kt_t * sha256_kt = md_kt_get (" SHA256 "); <nl> mbedtls_x509_crt * cert = ctx -> crt_chain ; <nl>  <nl> - if ( 0 != md_full ( sha256_kt , cert -> tbs . p , cert -> tbs . len , sha256_hash )) <nl> + if (! md_full ( sha256_kt , cert -> tbs . p , cert -> tbs . len , sha256_hash )) <nl> { <nl> msg ( M_WARN , " WARNING : failed to personalise random "); <nl> }
ieee802_11_print ( netdissect_options * ndo , <nl> hdrlen = roundup2 ( hdrlen , 4 ); <nl> if ( ndo -> ndo_Hflag && FC_TYPE ( fc ) == T_DATA && <nl> DATA_FRAME_IS_QOS ( FC_SUBTYPE ( fc ))) { <nl> + if (! ND_TTEST_1 ( p + hdrlen )) { <nl> + nd_print_trunc ( ndo ); <nl> + return hdrlen ; <nl> + } <nl> meshdrlen = extract_mesh_header_length ( p + hdrlen ); <nl> hdrlen += meshdrlen ; <nl> } else
icmp6_nodeinfo_print ( netdissect_options * ndo , u_int icmp6len , const u_char * bp , <nl> } else <nl> dnsname_print ( ndo , cp , ep ); <nl> if (( EXTRACT_16BITS (& ni6 -> ni_flags ) & 0x01 ) != 0 ) <nl> - ND_PRINT (( ndo ," [ TTL =% u ]", *( uint32_t *)( ni6 + 1 ))); <nl> + ND_PRINT (( ndo ," [ TTL =% u ]", EXTRACT_32BITS ( ni6 + 1 ))); <nl> break ; <nl> case NI_QTYPE_NODEADDR : <nl> if ( needcomma )
bool find_server ( PgSocket * client ) <nl> sbuf_pause (& client -> sbuf ); <nl> res = false ; /* don ' t process client data yet */ <nl> server -> setting_vars = 1 ; <nl> + server -> ready = 0 ; <nl> } else <nl> res = true ; <nl> } else {
static int send_ra ( int sock , struct Interface * iface , struct in6_addr const * des <nl> flog ( LOG_WARNING , " sendmsg : % s ", strerror ( errno )); <nl> else <nl> dlog ( LOG_DEBUG , 3 , " sendmsg : % s ", strerror ( errno )); <nl> + return - 1 ; <nl> } <nl>  <nl> return 0 ;
static int inflate ( struct mszipd_stream * zip ) { <nl>  <nl> /* read in block type */ <nl> READ_BITS ( block_type , 2 ); <nl> - D ((" block_type =% u last_block =% u ", block_type , last_block )) <nl>  <nl> if ( block_type == 0 ) { <nl> /* uncompressed block */
getfont ( PyObject * self_ , PyObject * args , PyObject * kw ) <nl> } <nl>  <nl> if ( filename && font_bytes_size <= 0 ) { <nl> + self -> font_bytes = NULL ; <nl> error = FT_New_Face ( library , filename , index , & self -> face ); <nl> } else { <nl> /* need to have allocated storage for font_bytes for the life of the object .*/
files_out : <nl> afc_file_lock ( afc , lockfile , AFC_LOCK_UN ); <nl> afc_file_close ( afc , lockfile ); <nl> lockfile = 0 ; <nl> - do_post_notification ( NP_SYNC_DID_FINISH ); <nl> + if ( cmd == CMD_BACKUP ) <nl> + do_post_notification ( NP_SYNC_DID_FINISH ); <nl> } <nl> } else { <nl> printf (" ERROR : Could not start service % s .\ n ", MOBILEBACKUP2_SERVICE_NAME );
namespace SDDM { <nl> if ( index != - 1 ) <nl> env . insert ( s . left ( index ), s . mid ( index + 1 )); <nl> } <nl> +# else <nl> + // we strdup ' d the string before in this branch <nl> + free ( mapped ); <nl> # endif <nl> env . insert (" HOME ", pw -> pw_dir ); <nl> env . insert (" PWD ", pw -> pw_dir );
array_index ( PyArrayObject * v ) <nl> " one element can be converted to an index "); <nl> return NULL ; <nl> } <nl> + if ( PyArray_NDIM ( v ) != 0 ) { <nl> + if ( DEPRECATE (" converting an array with ndim > 0 to an index " <nl> + " will result in an error in the future ") < 0 ) { <nl> + return NULL ; <nl> + } <nl> + } <nl> return PyArray_DESCR ( v )-> f -> getitem ( PyArray_DATA ( v ), v ); <nl> } <nl> # endif
 <nl> int xerbla_ ( char * srname , integer * info ) <nl> { <nl> - const char * format = " On entry to %.* s " \ <nl> + char * format = " On entry to %.* s " \ <nl> " parameter number % d had an illegal value "; <nl> char buf [ 60 + 6 + 4 ]; /* 6 for name , 4 for param . num . */ <nl> 
public : <nl> set_tid ( rtid ); <nl> } <nl> MOSDSubOp () {} <nl> + private : <nl> + ~ MOSDSubOp () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " osd_sub_op "; } <nl> void print ( ostream & out ) { <nl> out << " osd_sub_op (" << reqid
inline ostream & operator <<( ostream & out , const sockaddr_storage & ss ) <nl>  <nl> inline ostream & operator <<( ostream & out , const sockaddr_in & ss ) <nl> { <nl> - char buf [ NI_MAXHOST ]; <nl> + char buf [ NI_MAXHOST ] = { 0 }; <nl> getnameinfo (( struct sockaddr *)& ss , sizeof ( ss ), buf , sizeof ( buf ), 0 , 0 , NI_NUMERICHOST ); <nl> return out << buf ; <nl> }
int FileJournal :: prepare_single_write ( bufferlist & bl , off64_t & queue_pos , uint64 <nl>  <nl> // add it this entry <nl> entry_header_t h ; <nl> + memset (& h , 0 , sizeof ( h )); <nl> h . seq = seq ; <nl> h . pre_pad = pre_pad ; <nl> h . len = ebl . length ();
void Server :: early_reply ( MDRequest * mdr , CInode * tracei , CDentry * tracedn ) <nl> mds -> logger -> inc ( l_mds_reply ); <nl> double lat = g_clock . now () - mdr -> client_request -> get_recv_stamp (); <nl> mds -> logger -> favg ( l_mds_replyl , lat ); <nl> - dout ( 0 ) << " lat " << lat << dendl ; <nl> + dout ( 20 ) << " lat " << lat << dendl ; <nl> } <nl>  <nl> /*
reprotect_and_return_err : <nl> req_comp ); <nl> } else { <nl> if ( ictx -> cct -> _conf -> rbd_skip_partial_discard ) { <nl> + delete req_comp ; <nl> continue ; <nl> } <nl> req = new AioZero ( ictx , p -> oid . name , p -> objectno , p -> offset , p -> length ,
void ceph_handle_caps ( struct ceph_mds_client * mdsc , <nl> up_write (& mdsc -> snap_rwsem ); <nl> check_caps = 1 ; /* we may have sent a RELEASE to the old auth */ <nl> goto done ; <nl> - <nl> } <nl>  <nl> /* preallocate space for xattrs ? */ <nl> bad : <nl> return ; <nl>  <nl> release : <nl> + up_write (& mdsc -> snap_rwsem ); <nl> send_cap_msg ( mdsc , vino . ino , CEPH_CAP_OP_RELEASE , <nl> 0 , 0 , 0 , <nl> seq , 0 ,
public : <nl> monc_lock (" MonClient :: monc_lock "), <nl> timer ( monc_lock ), <nl> hunting ( false ), <nl> + want_monmap ( false ), <nl> + want_keys ( 0 ), <nl> mounting ( 0 ), mount_err ( 0 ), <nl> auth ( NULL ) { } <nl> ~ MonClient () {
static int do_bench_write ( librbd :: Image & image , uint64_t io_size , <nl>  <nl> printf (" SEC OPS OPS / SEC BYTES / SEC \ n "); <nl> uint64_t off ; <nl> - for ( off = 0 ; off < io_bytes ; off += io_size ) { <nl> + for ( off = 0 ; off < io_bytes ; ) { <nl> b . wait_for ( io_threads - 1 ); <nl> i = 0 ; <nl> while ( i < io_threads && off < io_bytes &&
int RGWRados :: bi_list ( rgw_bucket & bucket , const string & obj_name , const string & <nl> } <nl>  <nl> ret = cls_rgw_bi_list ( bs . index_ctx , bs . bucket_obj , obj_name , marker , max , entries , is_truncated ); <nl> + if ( ret == - ENOENT ) { <nl> + * is_truncated = false ; <nl> + } <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
void FileStore :: _set_replay_guard ( int fd , const SequencerPosition & spos ) <nl> // first make sure the previous operation commits <nl> :: fsync ( fd ); <nl>  <nl> + // sync object_map too . even if this object has a header or keys , <nl> + // it have had them in the past and then removed them , so always <nl> + // sync . <nl> + object_map -> sync (); <nl> + <nl> // then record that we did it <nl> bufferlist v ( 40 ); <nl> :: encode ( spos , v );
public : <nl> pg_temp . reset ( new map < pg_t , vector < int32_t > >(* o . pg_temp )); <nl> osd_uuid . reset ( new vector < uuid_d >(* o . osd_uuid )); <nl>  <nl> + if ( o . osd_primary_affinity ) <nl> + osd_primary_affinity . reset ( new vector < __u32 >(* o . osd_primary_affinity )); <nl> + <nl> // NOTE : this still references shared entity_addr_t ' s . <nl> osd_addrs . reset ( new addrs_s (* o . osd_addrs )); <nl> 
extern " C " int rados_pool_list ( rados_t cluster , char * buf , size_t len ) <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> - if (! buf ) <nl> + if ( len > 0 && ! buf ) <nl> return - EINVAL ; <nl>  <nl> char * b = buf ;
int ReplicatedPG :: do_osd_ops ( OpContext * ctx , vector < OSDOp >& ops ) <nl> case CEPH_OSD_OP_WATCH : <nl> ++ ctx -> num_write ; <nl> { <nl> + if (! obs . exists ) { <nl> + result = - ENOENT ; <nl> + break ; <nl> + } <nl> uint64_t cookie = op . watch . cookie ; <nl> bool do_watch = op . watch . flag & 1 ; <nl> entity_name_t entity = ctx -> reqid . name ;
bool CephXAuthorizer :: verify_reply ( bufferlist :: iterator & indata ) <nl> } <nl> } catch ( buffer :: error * e ) { <nl> dout ( 0 ) << " verify_authorizer_reply exception in decode_decrypt with " << session_key << dendl ; <nl> + delete e ; <nl> return false ; <nl> } <nl> 
IW_IMPL ( int ) iw_read_bmp_file ( struct iw_context * ctx , struct iw_iodescr * iodescr <nl> done : <nl> if (! retval ) { <nl> iw_set_error ( ctx ," BMP read failed "); <nl> + // If we didn ' t call iw_set_input_image , ' img ' still belongs to us , <nl> + // so free its contents . <nl> + iw_free ( ctx , img . pixels ); <nl> } <nl> return retval ; <nl> }
void rfbClientCleanup ( rfbClient * client ) { <nl> client -> clientData = next ; <nl> } <nl>  <nl> + free ( client -> vncRec ); <nl> + <nl> if ( client -> sock != RFB_INVALID_SOCKET ) <nl> rfbCloseSocket ( client -> sock ); <nl> if ( client -> listenSock != RFB_INVALID_SOCKET )
ipf_extract_frags_from_batch ( struct ipf * ipf , struct dp_packet_batch * pb , <nl> ovs_mutex_lock (& ipf -> ipf_lock ); <nl> if (! ipf_handle_frag ( ipf , pkt , dl_type , zone , now , hash_basis )) { <nl> dp_packet_batch_refill ( pb , pkt , pb_idx ); <nl> + } else { <nl> + dp_packet_delete ( pkt ); <nl> } <nl> ovs_mutex_unlock (& ipf -> ipf_lock ); <nl> } else {
hfs_cat_traverse ( HFS_INFO * hfs , <nl> size_t rec_off ; <nl> hfs_btree_key_cat * key ; <nl> uint8_t retval ; <nl> - uint16_t keylen ; <nl> + int keylen ; <nl>  <nl> // get the record offset in the node <nl> rec_off = <nl> hfs_cat_traverse ( HFS_INFO * hfs , <nl> size_t rec_off ; <nl> hfs_btree_key_cat * key ; <nl> uint8_t retval ; <nl> - uint16_t keylen ; <nl> + int keylen ; <nl>  <nl> // get the record offset in the node <nl> rec_off =
innobase_next_autoinc ( <nl> } else { <nl> next_value = current + increment ; <nl> } <nl> - } else { <nl> + } else if ( max_value > current ) { <nl> if ( current > offset ) { <nl> next_value = (( current - offset ) / increment ) + 1 ; <nl> } else { <nl> innobase_next_autoinc ( <nl> next_value += offset ; <nl> } <nl> } <nl> + } else { <nl> + next_value = max_value ; <nl> } <nl>  <nl> ut_a ( next_value <= max_value );
Ndb :: internalize_index_name ( const NdbTableImpl * table , <nl> if (! table ) <nl> { <nl> DBUG_PRINT (" error ", ("! table ")); <nl> - return ret ; <nl> + DBUG_RETURN ( ret ); <nl> } <nl>  <nl> if ( fullyQualifiedNames )
void Dblqh :: writeSinglePage ( Signal * signal , Uint32 pageNo , <nl> signal -> theData [ 7 ] = pageNo ; <nl> sendSignal ( NDBFS_REF , GSN_FSWRITEREQ , signal , 8 , JBA ); <nl>  <nl> + if ( logFilePtr . p -> fileRef == RNIL ) <nl> + { <nl> + signal -> theData [ 0 ] = 2305 ; <nl> + execDUMP_STATE_ORD ( signal ); <nl> + } <nl> ndbrequire ( logFilePtr . p -> fileRef != RNIL ); <nl>  <nl> logPartPtr . p -> m_io_tracker . send_io ( 32768 );
buf_page_init_for_read ( <nl> } <nl> } <nl>  <nl> + ut_a (! block -> page . buf_fix_count ); <nl> + block -> page . buf_fix_count ++;; <nl> rw_lock_x_lock (& block -> lock ); <nl> mutex_exit (& block -> mutex ); <nl> mutex_exit (& buf_pool -> zip_mutex ); <nl> buf_page_init_for_read ( <nl> } <nl>  <nl> buf_zip_decompress ( block , srv_use_checksums ); <nl> + mutex_enter (& block -> mutex ); <nl> + block -> page . buf_fix_count --; <nl> + mutex_exit (& block -> mutex ); <nl> rw_lock_x_unlock (& block -> lock ); <nl>  <nl> return ( NULL );
Prepared_statement :: execute_loop ( String * expanded_query , <nl> int reprepare_attempt = 0 ; <nl>  <nl> /* Check if we got an error when sending long data */ <nl> - if ( state == Query_arena :: ERROR ) <nl> + if ( state == Query_arena :: STMT_ERROR ) <nl> { <nl> my_message ( last_errno , last_error , MYF ( 0 )); <nl> return TRUE ;
void * memchr_inv ( const void * start , int c , size_t bytes ) <nl>  <nl> value64 = value ; <nl> # if defined ( CONFIG_ARCH_HAS_FAST_MULTIPLIER ) && BITS_PER_LONG == 64 <nl> - value64 *= 0x0101010101010101 ; <nl> + value64 *= 0x0101010101010101ULL ; <nl> # elif defined ( CONFIG_ARCH_HAS_FAST_MULTIPLIER ) <nl> value64 *= 0x01010101 ; <nl> value64 |= value64 << 32 ;
void apply_paravirt ( struct paravirt_patch_site * start , <nl> unsigned int used ; <nl>  <nl> BUG_ON ( p -> len > MAX_PATCH_LEN ); <nl> + /* prep the buffer with the original instructions */ <nl> + memcpy ( insnbuf , p -> instr , p -> len ); <nl> used = paravirt_ops . patch ( p -> instrtype , p -> clobbers , insnbuf , <nl> ( unsigned long ) p -> instr , p -> len ); <nl> 
static void <nl> intel_disable_cursor_plane ( struct drm_plane * plane , <nl> struct drm_crtc * crtc ) <nl> { <nl> + struct intel_crtc * intel_crtc = to_intel_crtc ( crtc ); <nl> + <nl> + intel_crtc -> cursor_addr = 0 ; <nl> intel_crtc_update_cursor ( crtc , false ); <nl> } <nl> 
e1000_set_tso ( struct net_device * netdev , u32 data ) <nl> else <nl> netdev -> features &= ~ NETIF_F_TSO ; <nl>  <nl> - if ( data ) <nl> + if ( data && ( adapter -> hw . mac_type > e1000_82547_rev_2 )) <nl> netdev -> features |= NETIF_F_TSO6 ; <nl> else <nl> netdev -> features &= ~ NETIF_F_TSO6 ;
void free_user_ns ( struct kref * kref ) <nl> struct user_namespace * ns ; <nl>  <nl> ns = container_of ( kref , struct user_namespace , kref ); <nl> + free_uid ( ns -> root_user ); <nl> kfree ( ns ); <nl> } <nl> 
static int clk_wzrd_probe ( struct platform_device * pdev ) <nl> reg = ( readl ( clk_wzrd -> base + WZRD_CLK_CFG_REG ( 0 )) & <nl> WZRD_DIVCLK_DIVIDE_MASK ) >> WZRD_DIVCLK_DIVIDE_SHIFT ; <nl> clk_name = kasprintf ( GFP_KERNEL , "% s_mul_div ", dev_name (& pdev -> dev )); <nl> + if (! clk_name ) { <nl> + ret = - ENOMEM ; <nl> + goto err_rm_int_clk ; <nl> + } <nl> + <nl> clk_wzrd -> clks_internal [ wzrd_clk_mul_div ] = clk_register_fixed_factor ( <nl> & pdev -> dev , clk_name , <nl> __clk_get_name ( clk_wzrd -> clks_internal [ wzrd_clk_mul ]),
static int pcmuio_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl>  <nl> /* save the ioport address for each ' port ' of 8 channels in the <nl> subdevice */ <nl> - for ( byte_no = 0 ; byte_no < PORTS_PER_SUBDEV ; ++ byte_no , ++ port ) { <nl> + for ( byte_no = 0 ; byte_no < PORTS_PER_SUBDEV ; <nl> + ++ byte_no , ++ port ) { <nl> if ( port >= PORTS_PER_ASIC ) { <nl> port = 0 ; <nl> ++ asic ;
static int omap_hdmi_dai_hw_params ( struct snd_pcm_substream * substream , <nl> /* <nl> * fill the IEC - 60958 channel status word <nl> */ <nl> + /* initialize the word bytes */ <nl> + memset ( iec -> status , 0 , sizeof ( iec -> status )); <nl>  <nl> /* specify IEC - 60958 - 3 ( commercial use ) */ <nl> iec -> status [ 0 ] &= ~ IEC958_AES0_PROFESSIONAL ;
static long gntalloc_ioctl_alloc ( struct gntalloc_file_private_data * priv , <nl> goto out ; <nl> } <nl>  <nl> - gref_ids = kzalloc ( sizeof ( gref_ids [ 0 ]) * op . count , GFP_TEMPORARY ); <nl> + gref_ids = kcalloc ( op . count , sizeof ( gref_ids [ 0 ]), GFP_TEMPORARY ); <nl> if (! gref_ids ) { <nl> rc = - ENOMEM ; <nl> goto out ;
static void atmel_sha_get_cap ( struct atmel_sha_dev * dd ) <nl>  <nl> /* keep only major version number */ <nl> switch ( dd -> hw_version & 0xff0 ) { <nl> + case 0x420 : <nl> + dd -> caps . has_dma = 1 ; <nl> + dd -> caps . has_dualbuff = 1 ; <nl> + dd -> caps . has_sha224 = 1 ; <nl> + dd -> caps . has_sha_384_512 = 1 ; <nl> + break ; <nl> case 0x410 : <nl> dd -> caps . has_dma = 1 ; <nl> dd -> caps . has_dualbuff = 1 ;
static void igmp_heard_query ( struct in_device * in_dev , struct sk_buff * skb , <nl> * to be intended in a v3 query . <nl> */ <nl> max_delay = IGMPV3_MRC ( ih3 -> code )*( HZ / IGMP_TIMER_SCALE ); <nl> + if (! max_delay ) <nl> + max_delay = 1 ; /* can ' t mod w / 0 */ <nl> } else { /* v3 */ <nl> if (! pskb_may_pull ( skb , sizeof ( struct igmpv3_query ))) <nl> return ;
static void vmw_user_surface_base_release ( struct ttm_base_object ** p_base ) <nl> struct vmw_resource * res = & user_srf -> srf . res ; <nl>  <nl> * p_base = NULL ; <nl> - ttm_base_object_unref (& user_srf -> backup_base ); <nl> + if ( user_srf -> backup_base ) <nl> + ttm_base_object_unref (& user_srf -> backup_base ); <nl> vmw_resource_unreference (& res ); <nl> } <nl> 
void __devinit bttv_init_card2 ( struct bttv * btv ) <nl> } <nl> btv -> pll . pll_current = - 1 ; <nl>  <nl> - bttv_reset_audio ( btv ); <nl> - <nl> /* tuner configuration ( from card list / autodetect / insmod option ) */ <nl> if ( UNSET != bttv_tvcards [ btv -> c . type ]. tuner_type ) <nl> if ( UNSET == btv -> tuner_type )
void hw_cursor_setData ( struct lynx_cursor * cursor , <nl> iowrite16 ( data , pbuffer ); <nl>  <nl> /* assume pitch is 1 , 2 , 4 , 8 ,...*/ <nl> - if (( i + 1 ) % pitch == 0 ) <nl> - { <nl> + if (( i + 1 ) % pitch == 0 ) { <nl> /* need a return */ <nl> pstart += offset ; <nl> pbuffer = pstart ;
void ceph_handle_snap ( struct ceph_mds_client * mdsc , <nl> * queued ( again ) by ceph_update_snap_trace () <nl> * below . Queue it _now_ , under the old context . <nl> */ <nl> + spin_lock (& realm -> inodes_with_caps_lock ); <nl> list_del_init (& ci -> i_snap_realm_item ); <nl> + spin_unlock (& realm -> inodes_with_caps_lock ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> ceph_queue_cap_snap ( ci ,
static int ks8695_poll ( struct napi_struct * napi , int budget ) <nl> if ( work_done < budget ) { <nl> unsigned long flags ; <nl> spin_lock_irqsave (& ksp -> rx_lock , flags ); <nl> + __napi_complete ( napi ); <nl> /* enable rx interrupt */ <nl> writel ( isr | mask_bit , KS8695_IRQ_VA + KS8695_INTEN ); <nl> - __napi_complete ( napi ); <nl> spin_unlock_irqrestore (& ksp -> rx_lock , flags ); <nl> } <nl> return work_done ;
static void vmid_reference ( struct snd_soc_codec * codec ) <nl> WM8994_LINEOUT2_DISCH , <nl> WM8994_LINEOUT_VMID_BUF_ENA ); <nl>  <nl> + wm_hubs_vmid_ena ( codec ); <nl> + <nl> /* Startup bias , VMID ramp & buffer */ <nl> snd_soc_update_bits ( codec , WM8994_ANTIPOP_2 , <nl> WM8994_BIAS_SRC | <nl> static void vmid_reference ( struct snd_soc_codec * codec ) <nl> WM8994_VMID_BUF_ENA | <nl> ( 0x2 << WM8994_VMID_RAMP_SHIFT )); <nl>  <nl> - wm_hubs_vmid_ena ( codec ); <nl> - <nl> /* Main bias enable , VMID = 2x40k */ <nl> snd_soc_update_bits ( codec , WM8994_POWER_MANAGEMENT_1 , <nl> WM8994_BIAS_ENA |
static int __ath10k_pci_hif_power_up ( struct ath10k * ar , bool cold_reset ) <nl> irq_mode = " legacy "; <nl>  <nl> if (! test_bit ( ATH10K_FLAG_FIRST_BOOT_DONE , & ar -> dev_flags )) <nl> - ath10k_info (" pci irq % s \ n ", irq_mode ); <nl> + ath10k_info (" pci irq % s irq_mode % d reset_mode % d \ n ", <nl> + irq_mode , ath10k_pci_irq_mode , <nl> + ath10k_pci_reset_mode ); <nl>  <nl> return 0 ; <nl> 
static struct vmap_area * alloc_vmap_area ( unsigned long size , <nl>  <nl> BUG_ON ( size & ~ PAGE_MASK ); <nl>  <nl> - addr = ALIGN ( vstart , align ); <nl> - <nl> va = kmalloc_node ( sizeof ( struct vmap_area ), <nl> gfp_mask & GFP_RECLAIM_MASK , node ); <nl> if ( unlikely (! va )) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> retry : <nl> + addr = ALIGN ( vstart , align ); <nl> + <nl> spin_lock (& vmap_area_lock ); <nl> /* XXX : could have a last_hole cache */ <nl> n = vmap_area_root . rb_node ;
unlock : <nl>  <nl> for ( skb = segs ; skb ; skb = skb -> next ) { <nl> ipv6h = skb -> nh . ipv6h ; <nl> - ipv6h -> payload_len = htons ( skb -> len - skb -> mac_len ); <nl> + ipv6h -> payload_len = htons ( skb -> len - skb -> mac_len - <nl> + sizeof (* ipv6h )); <nl> } <nl>  <nl> out :
int iwl_power_update_mode ( struct iwl_priv * priv , bool force ) <nl> if ( priv -> cfg -> ops -> lib -> update_chain_flags && <nl> update_chains ) <nl> priv -> cfg -> ops -> lib -> update_chain_flags ( priv ); <nl> - else <nl> + else if ( priv -> cfg -> ops -> lib -> update_chain_flags ) <nl> IWL_DEBUG_POWER ( priv , <nl> " Cannot update the power , chain noise " <nl> " calibration running : % d \ n ",
struct runqueue { <nl> unsigned long ttwu_cnt ; <nl> unsigned long ttwu_local ; <nl> # endif <nl> + struct lock_class_key rq_lock_key ; <nl> }; <nl>  <nl> static DEFINE_PER_CPU ( struct runqueue , runqueues ); <nl> void __init sched_init ( void ) <nl>  <nl> rq = cpu_rq ( i ); <nl> spin_lock_init (& rq -> lock ); <nl> + lockdep_set_class (& rq -> lock , & rq -> rq_lock_key ); <nl> rq -> nr_running = 0 ; <nl> rq -> active = rq -> arrays ; <nl> rq -> expired = rq -> arrays + 1 ;
static int cy_put_char ( struct tty_struct * tty , unsigned char ch ) <nl> return 0 ; <nl>  <nl> if (! info -> xmit_buf ) <nl> - return ; <nl> + return 0 ; <nl>  <nl> local_irq_save ( flags ); <nl> if ( info -> xmit_cnt >= PAGE_SIZE - 1 ) {
static inline long snd_ctl_ioctl_compat ( struct file * file , unsigned int cmd , uns <nl> case SNDRV_CTL_IOCTL_POWER_STATE : <nl> case SNDRV_CTL_IOCTL_ELEM_LOCK : <nl> case SNDRV_CTL_IOCTL_ELEM_UNLOCK : <nl> + case SNDRV_CTL_IOCTL_ELEM_REMOVE : <nl> + case SNDRV_CTL_IOCTL_TLV_READ : <nl> + case SNDRV_CTL_IOCTL_TLV_WRITE : <nl> + case SNDRV_CTL_IOCTL_TLV_COMMAND : <nl> return snd_ctl_ioctl ( file , cmd , ( unsigned long ) argp ); <nl> case SNDRV_CTL_IOCTL_ELEM_LIST32 : <nl> return snd_ctl_elem_list_compat ( ctl -> card , argp );
static int tiadc_buffer_preenable ( struct iio_dev * indio_dev ) <nl> for ( i = 0 ; i < fifo1count ; i ++) <nl> read = tiadc_readl ( adc_dev , REG_FIFO1 ); <nl>  <nl> - return iio_sw_buffer_preenable ( indio_dev ); <nl> + return 0 ; <nl> } <nl>  <nl> static int tiadc_buffer_postenable ( struct iio_dev * indio_dev )
static int __block_prepare_write ( struct inode * inode , struct page * page , <nl> unmap_underlying_metadata ( bh -> b_bdev , <nl> bh -> b_blocknr ); <nl> if ( PageUptodate ( page )) { <nl> + clear_buffer_new ( bh ); <nl> set_buffer_uptodate ( bh ); <nl> + mark_buffer_dirty ( bh ); <nl> continue ; <nl> } <nl> if ( block_end > to || block_start < from ) {
void scsi_eh_prep_cmnd ( struct scsi_cmnd * scmd , struct scsi_eh_save * ses , <nl> ses -> prot_op = scmd -> prot_op ; <nl>  <nl> scmd -> prot_op = SCSI_PROT_NORMAL ; <nl> + scmd -> eh_eflags = 0 ; <nl> scmd -> cmnd = ses -> eh_cmnd ; <nl> memset ( scmd -> cmnd , 0 , BLK_MAX_CDB ); <nl> memset (& scmd -> sdb , 0 , sizeof ( scmd -> sdb ));
int rtw_tkip_encrypt23a ( struct rtw_adapter * padapter , <nl> arcfour_encrypt (& mycontext , payload , payload , length ); <nl> arcfour_encrypt (& mycontext , payload + length , crc , 4 ); <nl>  <nl> - pframe += pxmitpriv -> frag_len ; <nl> - pframe = PTR_ALIGN ( pframe , 4 ); <nl> + pframe += pxmitpriv -> frag_len ; <nl> + pframe = PTR_ALIGN ( pframe , 4 ); <nl> } <nl> } <nl> 
static int GLOB_SBD_init ( void ) <nl> int i ; <nl>  <nl> /* Set debug output level ( 0 ~ 3 ) here . 3 is most verbose */ <nl> - nand_debug_level = 0 ; <nl> - <nl> printk ( KERN_ALERT " Spectra : % s \ n ", GLOB_version ); <nl>  <nl> mutex_init (& spectra_lock );
int mc13xxx_common_init ( struct mc13xxx * mc13xxx , <nl> err_mask : <nl> err_revision : <nl> mc13xxx_unlock ( mc13xxx ); <nl> - kfree ( mc13xxx ); <nl> return ret ; <nl> } <nl> 
static bool tcp_fastopen_create_child ( struct sock * sk , <nl> struct dst_entry * dst , <nl> struct request_sock * req ) <nl> { <nl> - struct tcp_sock * tp = tcp_sk ( sk ); <nl> + struct tcp_sock * tp ; <nl> struct request_sock_queue * queue = & inet_csk ( sk )-> icsk_accept_queue ; <nl> struct sock * child ; <nl> 
static int lowpan_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> if (! netif_running ( dev )) <nl> goto drop_skb ; <nl>  <nl> + if ( skb -> pkt_type == PACKET_OTHERHOST ) <nl> + goto drop_skb ; <nl> + <nl> if ( dev -> type != ARPHRD_IEEE802154 ) <nl> goto drop_skb ; <nl> 
int wm8350_device_init ( struct wm8350 * wm8350 , int irq , <nl> wm8350 -> power . rev_g_coeff = 1 ; <nl> break ; <nl>  <nl> + case 1 : <nl> + dev_info ( wm8350 -> dev , " WM8351 Rev B \ n "); <nl> + wm8350 -> power . rev_g_coeff = 1 ; <nl> + break ; <nl> + <nl> default : <nl> dev_err ( wm8350 -> dev , " Unknown WM8351 CHIP_REV \ n "); <nl> ret = - ENODEV ;
iscsi_get_host_stats ( struct iscsi_transport * transport , struct nlmsghdr * nlh ) <nl> memset ( buf , 0 , host_stats_size ); <nl>  <nl> err = transport -> get_host_stats ( shost , buf , host_stats_size ); <nl> + if ( err ) { <nl> + kfree ( skbhost_stats ); <nl> + goto exit_host_stats ; <nl> + } <nl>  <nl> actual_size = nlmsg_total_size ( sizeof (* ev ) + host_stats_size ); <nl> skb_trim ( skbhost_stats , NLMSG_ALIGN ( actual_size ));
void machine_shutdown ( void ) <nl> */ <nl> void machine_halt ( void ) <nl> { <nl> + local_irq_disable (); <nl> smp_send_stop (); <nl>  <nl> local_irq_disable (); <nl> void machine_halt ( void ) <nl> */ <nl> void machine_power_off ( void ) <nl> { <nl> + local_irq_disable (); <nl> smp_send_stop (); <nl>  <nl> if ( pm_power_off ) <nl> void machine_power_off ( void ) <nl> */ <nl> void machine_restart ( char * cmd ) <nl> { <nl> + local_irq_disable (); <nl> smp_send_stop (); <nl>  <nl> arm_pm_restart ( reboot_mode , cmd );
static int fc_user_scan ( struct Scsi_Host * shost , uint channel , <nl> if ( rport -> scsi_target_id == - 1 ) <nl> continue ; <nl>  <nl> + if ( rport -> port_state != FC_PORTSTATE_ONLINE ) <nl> + continue ; <nl> + <nl> if (( channel == SCAN_WILD_CARD || channel == rport -> channel ) && <nl> ( id == SCAN_WILD_CARD || id == rport -> scsi_target_id )) { <nl> scsi_scan_target (& rport -> dev , rport -> channel ,
struct oz_urb_link { <nl> /* Holds state information about a USB endpoint . <nl> */ <nl> # define OZ_EP_BUFFER_SIZE_ISOC ( 1024 * 24 ) <nl> +# define OZ_EP_BUFFER_SIZE_INT 512 <nl> struct oz_endpoint { <nl> struct list_head urb_list ; /* List of oz_urb_link items . */ <nl> struct list_head link ; /* For isoc ep , links in to isoc <nl> static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> buffer_size = OZ_EP_BUFFER_SIZE_ISOC ; <nl> break ; <nl> case USB_ENDPOINT_XFER_INT : <nl> - buffer_size = 128 ; <nl> + buffer_size = OZ_EP_BUFFER_SIZE_INT ; <nl> break ; <nl> } <nl> }
void * consistent_alloc ( gfp_t gfp , size_t size , dma_addr_t * handle ) <nl> split_page ( page , order ); <nl>  <nl> ret = page_address ( page ); <nl> + memset ( ret , 0 , size ); <nl> * handle = virt_to_phys ( ret ); <nl>  <nl> /*
extern int ext4_init_inode_table ( struct super_block * sb , ext4_group_t group , <nl> group , used_blks , <nl> ext4_itable_unused_count ( sb , gdp )); <nl> ret = 1 ; <nl> - goto out ; <nl> + goto err_out ; <nl> } <nl>  <nl> blk = ext4_inode_table ( sb , gdp ) + used_blks ;
static int ncp_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> case 0x00 : <nl> ncp_dbg ( 1 , " renamed % pd -> % pd \ n ", <nl> old_dentry , new_dentry ); <nl> + ncp_d_prune ( old_dentry ); <nl> + ncp_d_prune ( new_dentry ); <nl> break ; <nl> case 0x9E : <nl> error = - ENAMETOOLONG ;
retry_snap : <nl> goto retry_snap ; <nl> } <nl> } else { <nl> + loff_t old_size = inode -> i_size ; <nl> /* <nl> * No need to acquire the i_truncate_mutex . Because <nl> * the MDS revokes Fwb caps before sending truncate <nl> retry_snap : <nl> written = generic_file_buffered_write ( iocb , iov , nr_segs , <nl> pos , & iocb -> ki_pos , <nl> count , 0 ); <nl> + if ( inode -> i_size > old_size ) <nl> + ceph_fscache_update_objectsize ( inode ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> } <nl> 
ath5k_deinit_softc ( struct ath5k_softc * sc ) <nl> * state and potentially want to use them . <nl> */ <nl> ath5k_hw_deinit ( sc -> ah ); <nl> + kfree ( sc -> ah ); <nl> free_irq ( sc -> irq , sc ); <nl> } <nl> 
try_again : <nl> * based cpu - clock - tick sw counter , which <nl> * is always available even if no PMU support : <nl> */ <nl> - if ( attr -> type == PERF_TYPE_HARDWARE <nl> + if ( err == ENOENT && attr -> type == PERF_TYPE_HARDWARE <nl> && attr -> config == PERF_COUNT_HW_CPU_CYCLES ) { <nl>  <nl> if ( verbose )
static inline void x86_assign_hw_event ( struct perf_event * event , <nl> } else { <nl> hwc -> config_base = x86_pmu_config_addr ( hwc -> idx ); <nl> hwc -> event_base = x86_pmu_event_addr ( hwc -> idx ); <nl> - hwc -> event_base_rdpmc = x86_pmu_addr_offset ( hwc -> idx ); <nl> + hwc -> event_base_rdpmc = hwc -> idx ; <nl> } <nl> } <nl> 
static void deallocate_vmid ( struct device_queue_manager * dqm , <nl> { <nl> int bit = qpd -> vmid - KFD_VMID_START_OFFSET ; <nl>  <nl> + /* Release the vmid mapping */ <nl> + set_pasid_vmid_mapping ( dqm , 0 , qpd -> vmid ); <nl> + <nl> set_bit ( bit , ( unsigned long *)& dqm -> vmid_bitmap ); <nl> qpd -> vmid = 0 ; <nl> q -> properties . vmid = 0 ;
static void __init_memblock memblock_merge_regions ( struct memblock_type * type ) <nl> } <nl>  <nl> this -> size += next -> size ; <nl> - memmove ( next , next + 1 , ( type -> cnt - ( i + 1 )) * sizeof (* next )); <nl> + /* move forward from next + 1 , index of which is i + 2 */ <nl> + memmove ( next , next + 1 , ( type -> cnt - ( i + 2 )) * sizeof (* next )); <nl> type -> cnt --; <nl> } <nl> }
static int cfg80211_netdev_notifier_call ( struct notifier_block * nb , <nl> * Configure power management to the driver here so that its <nl> * correctly set also after interface type changes etc . <nl> */ <nl> - if ( wdev -> iftype == NL80211_IFTYPE_STATION && <nl> + if (( wdev -> iftype == NL80211_IFTYPE_STATION || <nl> + wdev -> iftype == NL80211_IFTYPE_P2P_CLIENT ) && <nl> rdev -> ops -> set_power_mgmt ) <nl> if ( rdev -> ops -> set_power_mgmt ( wdev -> wiphy , dev , <nl> wdev -> ps ,
static int r820t_signal ( struct dvb_frontend * fe , u16 * strength ) <nl>  <nl> /* A higher gain at LNA means a lower signal strength */ <nl> * strength = ( 45 - rc ) << 4 | 0xff ; <nl> + if (* strength == 0xff ) <nl> + * strength = 0 ; <nl> } else { <nl> * strength = 0 ; <nl> }
static struct clk_lookup lookups [] = { <nl>  <nl> /* MSTP32 clocks */ <nl> CLKDEV_DEV_ID (" sh_mmcif ", & mstp_clks [ MSTP331 ]), /* MMC */ <nl> + CLKDEV_DEV_ID (" ffe4e000 . mmcif ", & mstp_clks [ MSTP331 ]), /* MMC */ <nl> CLKDEV_DEV_ID (" sh_mobile_sdhi . 0 ", & mstp_clks [ MSTP323 ]), /* SDHI0 */ <nl> CLKDEV_DEV_ID (" sh_mobile_sdhi . 1 ", & mstp_clks [ MSTP322 ]), /* SDHI1 */ <nl> CLKDEV_DEV_ID (" sh_mobile_sdhi . 2 ", & mstp_clks [ MSTP321 ]), /* SDHI2 */
void btrfsic_unmount ( struct btrfs_root * root , <nl> btrfsic_block_link_free ( l ); <nl> } <nl>  <nl> - if ( b_all -> is_iodone ) <nl> + if ( b_all -> is_iodone || b_all -> never_written ) <nl> btrfsic_block_free ( b_all ); <nl> else <nl> printk ( KERN_INFO " btrfs : attempt to free % c - block "
static int zcache_new_pool ( uint16_t cli_id , uint32_t flags ) <nl> if ( cli == NULL ) <nl> goto out ; <nl> atomic_inc (& cli -> refcount ); <nl> - pool = kmalloc ( sizeof ( struct tmem_pool ), GFP_KERNEL ); <nl> + pool = kmalloc ( sizeof ( struct tmem_pool ), GFP_ATOMIC ); <nl> if ( pool == NULL ) { <nl> pr_info (" zcache : pool creation failed : out of memory \ n "); <nl> goto out ;
static int mt9t112_probe ( struct i2c_client * client , <nl> v4l2_i2c_subdev_init (& priv -> subdev , client , & mt9t112_subdev_ops ); <nl>  <nl> ret = mt9t112_camera_probe ( client ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> kfree ( priv ); <nl> + return ret ; <nl> + } <nl>  <nl> /* Cannot fail : using the default supported pixel code */ <nl> mt9t112_set_params ( priv , & rect , V4L2_MBUS_FMT_UYVY8_2X8 );
static int twlreg_disable ( struct regulator_dev * rdev ) <nl> return grp ; <nl>  <nl> if ( twl_class_is_4030 ()) <nl> - grp &= ~ P1_GRP_4030 ; <nl> + grp &= ~( P1_GRP_4030 | P2_GRP_4030 | P3_GRP_4030 ); <nl> else <nl> - grp &= ~ P1_GRP_6030 ; <nl> + grp &= ~( P1_GRP_6030 | P2_GRP_6030 | P3_GRP_6030 ); <nl>  <nl> return twlreg_write ( info , TWL_MODULE_PM_RECEIVER , VREG_GRP , grp ); <nl> }
static int parse_audio_selector_unit ( struct mixer_build * state , int unitid , void <nl> if (! len && check_input_term ( state , desc -> baSourceID [ i ], & iterm ) >= 0 ) <nl> len = get_term_name ( state , & iterm , namelist [ i ], MAX_ITEM_NAME_LEN , 0 ); <nl> if (! len ) <nl> - sprintf ( namelist [ i ], " Input % d ", i ); <nl> + sprintf ( namelist [ i ], " Input % u ", i ); <nl> } <nl>  <nl> kctl = snd_ctl_new1 (& mixer_selectunit_ctl , cval );
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> tnl_hlen = skb_tnl_header_len ( skb ); <nl> - if ( skb_headroom ( skb ) < ( tnl_hlen + frag_hdr_sz )) { <nl> + if ( skb -> mac_header < ( tnl_hlen + frag_hdr_sz )) { <nl> if ( gso_pskb_expand_head ( skb , tnl_hlen + frag_hdr_sz )) <nl> goto out ; <nl> }
static int pm860x_led_dt_init ( struct platform_device * pdev , <nl> of_property_read_u32 ( np , " marvell , 88pm860x - iset ", <nl> & iset ); <nl> data -> iset = PM8606_LED_CURRENT ( iset ); <nl> + of_node_put ( np ); <nl> break ; <nl> } <nl> }
static int virtnet_set_channels ( struct net_device * dev , <nl> if ( channels -> rx_count || channels -> tx_count || channels -> other_count ) <nl> return - EINVAL ; <nl>  <nl> - if ( queue_pairs > vi -> max_queue_pairs ) <nl> + if ( queue_pairs > vi -> max_queue_pairs || queue_pairs == 0 ) <nl> return - EINVAL ; <nl>  <nl> get_online_cpus ();
static int t4_sge_init_soft ( struct adapter * adap ) <nl> # undef READ_FL_BUF <nl>  <nl> if ( fl_small_pg != PAGE_SIZE || <nl> - ( fl_large_pg != 0 && ( fl_large_pg <= fl_small_pg || <nl> + ( fl_large_pg != 0 && ( fl_large_pg < fl_small_pg || <nl> ( fl_large_pg & ( fl_large_pg - 1 )) != 0 ))) { <nl> dev_err ( adap -> pdev_dev , " bad SGE FL page buffer sizes [% d , % d ]\ n ", <nl> fl_small_pg , fl_large_pg );
static void do_reads ( struct mirror_set * ms , struct bio_list * reads ) <nl> /* <nl> * We can only read balance if the region is in sync . <nl> */ <nl> - if ( rh_in_sync (& ms -> rh , region , 0 )) <nl> + if ( rh_in_sync (& ms -> rh , region , 1 )) <nl> m = choose_mirror ( ms , bio -> bi_sector ); <nl> else <nl> m = ms -> default_mirror ;
static int sky2_rx_start ( struct sky2_port * sky2 ) <nl> rx_set_checksum ( sky2 ); <nl>  <nl> /* Space needed for frame data + headers rounded up */ <nl> - size = ALIGN ( sky2 -> netdev -> mtu + ETH_HLEN + VLAN_HLEN , 8 ) <nl> - + 8 ; <nl> + size = roundup ( sky2 -> netdev -> mtu + ETH_HLEN + VLAN_HLEN , 8 ); <nl>  <nl> /* Stopping point for hardware truncation */ <nl> thresh = ( size - 8 ) / sizeof ( u32 );
static bool igbvf_clean_rx_irq ( struct igbvf_adapter * adapter , <nl> dma_unmap_single (& pdev -> dev , buffer_info -> dma , <nl> adapter -> rx_ps_hdr_size , <nl> DMA_FROM_DEVICE ); <nl> + buffer_info -> dma = 0 ; <nl> skb_put ( skb , hlen ); <nl> } <nl> 
static int process_counter ( struct perf_evsel * counter ) <nl> int i , ret ; <nl>  <nl> aggr -> val = aggr -> ena = aggr -> run = 0 ; <nl> - memset ( ps -> res_stats , 0 , sizeof ( ps -> res_stats )); <nl> + init_stats ( ps -> res_stats ); <nl>  <nl> if ( counter -> per_pkg ) <nl> zero_per_pkg ( counter );
static pci_ers_result_t ixgbe_io_error_detected ( struct pci_dev * pdev , <nl>  <nl> netif_device_detach ( netdev ); <nl>  <nl> + if ( state == pci_channel_io_perm_failure ) <nl> + return PCI_ERS_RESULT_DISCONNECT ; <nl> + <nl> if ( netif_running ( netdev )) <nl> ixgbe_down ( adapter ); <nl> pci_disable_device ( pdev );
static struct mtd_partition * newpart ( char * s , <nl> s ++; <nl> } else { <nl> size = memparse ( s , & s ); <nl> - if ( size < PAGE_SIZE ) { <nl> - printk ( KERN_ERR ERRP " partition size too small (% llx )\ n ", <nl> - size ); <nl> + if (! size ) { <nl> + printk ( KERN_ERR ERRP " partition has size 0 \ n "); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> }
static int img_spfi_start_dma ( struct spi_master * master , <nl> dma_async_issue_pending ( spfi -> rx_ch ); <nl> } <nl>  <nl> + spfi_start ( spfi ); <nl> + <nl> if ( xfer -> tx_buf ) { <nl> spfi -> tx_dma_busy = true ; <nl> dmaengine_submit ( txdesc ); <nl> dma_async_issue_pending ( spfi -> tx_ch ); <nl> } <nl>  <nl> - spfi_start ( spfi ); <nl> - <nl> return 1 ; <nl>  <nl> stop_dma :
static unsigned int ip_vs_post_routing ( unsigned int hooknum , <nl> { <nl> if (!((* pskb )-> ipvs_property )) <nl> return NF_ACCEPT ; <nl> - <nl> /* The packet was sent from IPVS , exit this chain */ <nl> - (* okfn )(* pskb ); <nl> - <nl> - return NF_STOLEN ; <nl> + return NF_STOP ; <nl> } <nl>  <nl> u16 ip_vs_checksum_complete ( struct sk_buff * skb , int offset )
static struct perf_pmu * pmu_lookup ( const char * name ) <nl> LIST_HEAD ( aliases ); <nl> __u32 type ; <nl>  <nl> + /* No support for intel_bts or intel_pt so disallow them */ <nl> + if (! strcmp ( name , " intel_bts ") || ! strcmp ( name , " intel_pt ")) <nl> + return NULL ; <nl> + <nl> /* <nl> * The pmu data we store & need consists of the pmu <nl> * type value and format definitions . Load both right
struct e820entry ; <nl> char * __init machine_specific_memory_setup ( void ); <nl> char * memory_setup ( void ); <nl>  <nl> - int __init copy_e820_map ( struct e820entry * biosmap , int nr_map ); <nl> - int __init sanitize_e820_map ( struct e820entry * biosmap , char * pnr_map ); <nl> + int __init copy_e820_map ( struct e820entry * biosmap , int nr_map ); <nl> + int __init sanitize_e820_map ( struct e820entry * biosmap , char * pnr_map ); <nl> void __init add_memory_region ( unsigned long long start , <nl> unsigned long long size , int type ); <nl> 
static int wacom_set_device_mode ( struct hid_device * hdev , int report_id , <nl> if ( error >= 0 ) <nl> error = wacom_get_report ( hdev , HID_FEATURE_REPORT , <nl> rep_data , length , 1 ); <nl> - } while (( error < 0 || rep_data [ 1 ] != mode ) && limit ++ < WAC_MSG_RETRIES ); <nl> + } while ( error >= 0 && rep_data [ 1 ] != mode && limit ++ < WAC_MSG_RETRIES ); <nl>  <nl> kfree ( rep_data ); <nl> 
static int dlfb_realloc_framebuffer ( struct dlfb_data * dev , struct fb_info * info ) <nl> int new_len ; <nl> unsigned char * old_fb = info -> screen_base ; <nl> unsigned char * new_fb ; <nl> - unsigned char * new_back = 0 ; <nl> + unsigned char * new_back = NULL ; <nl>  <nl> pr_warn (" Reallocating framebuffer . Addresses will change !\ n "); <nl> 
static long hcall_vphn ( unsigned long cpu , __be32 * associativity ) <nl> long retbuf [ PLPAR_HCALL9_BUFSIZE ] = { 0 }; <nl> u64 flags = 1 ; <nl> int hwcpu = get_hard_smp_processor_id ( cpu ); <nl> + int i ; <nl>  <nl> rc = plpar_hcall9 ( H_HOME_NODE_ASSOCIATIVITY , retbuf , flags , hwcpu ); <nl> + for ( i = 0 ; i < 6 ; i ++) <nl> + retbuf [ i ] = cpu_to_be64 ( retbuf [ i ]); <nl> vphn_unpack_associativity ( retbuf , associativity ); <nl>  <nl> return rc ;
static void macb_handle_link_change ( struct net_device * dev ) <nl>  <nl> if ( phydev -> duplex ) <nl> reg |= MACB_BIT ( FD ); <nl> - if ( phydev -> speed ) <nl> + if ( phydev -> speed == SPEED_100 ) <nl> reg |= MACB_BIT ( SPD ); <nl>  <nl> macb_writel ( bp , NCFGR , reg );
static inline int get_ni_value ( int mclk , int rate ) <nl> if ( ni_div [ i ]. mclk >= mclk ) <nl> break ; <nl> } <nl> + if ( i == ARRAY_SIZE ( ni_div )) <nl> + return - EINVAL ; <nl>  <nl> switch ( rate ) { <nl> case 8000 :
static int pxa2xx_pcm_hw_free ( struct snd_pcm_substream * substream ) <nl> return 0 ; <nl> } <nl>  <nl> - struct snd_pcm_ops pxa2xx_pcm_ops = { <nl> + static struct snd_pcm_ops pxa2xx_pcm_ops = { <nl> . open = __pxa2xx_pcm_open , <nl> . close = __pxa2xx_pcm_close , <nl> . ioctl = snd_pcm_lib_ioctl ,
DECLARE_PCI_FIXUP_HEADER ( PCI_VENDOR_ID_ATI , PCI_DEVICE_ID_ATI_SBX00_SMBUS , <nl>  <nl> # if defined ( CONFIG_PCI ) && defined ( CONFIG_NUMA ) <nl> /* Set correct numa_node information for AMD NB functions */ <nl> - static void __init quirk_amd_nb_node ( struct pci_dev * dev ) <nl> + static void __devinit quirk_amd_nb_node ( struct pci_dev * dev ) <nl> { <nl> struct pci_dev * nb_ht ; <nl> unsigned int devfn ;
static inline u32 skb_mstamp_us_delta ( const struct skb_mstamp * t1 , <nl> return delta_us ; <nl> } <nl>  <nl> + static inline bool skb_mstamp_after ( const struct skb_mstamp * t1 , <nl> + const struct skb_mstamp * t0 ) <nl> +{ <nl> + s32 diff = t1 -> stamp_jiffies - t0 -> stamp_jiffies ; <nl> + <nl> + if (! diff ) <nl> + diff = t1 -> stamp_us - t0 -> stamp_us ; <nl> + return diff > 0 ; <nl> +} <nl>  <nl> /** <nl> * struct sk_buff - socket buffer
ohci_enable_phys_dma ( struct fw_card * card , int node_id , int generation ) <nl> 1 << ( node_id - 32 )); <nl> } <nl> flush_writes ( ohci ); <nl> - <nl> - spin_unlock_irqrestore (& ohci -> lock , flags ); <nl> - <nl> out : <nl> + spin_unlock_irqrestore (& ohci -> lock , flags ); <nl> return retval ; <nl> } <nl> 
match1 : <nl> ndoms_cur = 0 ; <nl> doms_new = & fallback_doms ; <nl> cpus_andnot ( doms_new [ 0 ], cpu_online_map , cpu_isolated_map ); <nl> - dattr_new = NULL ; <nl> + WARN_ON_ONCE ( dattr_new ); <nl> } <nl>  <nl> /* Build new domains */
static int sysfs_add_link ( struct dentry * parent , const char * name , struct kobj <nl> if (! error ) <nl> return 0 ; <nl>  <nl> + kobject_put ( target ); <nl> kfree ( sl -> link_name ); <nl> exit2 : <nl> kfree ( sl );
done : <nl>  <nl> out : <nl> if ( unlikely ( frozen_buffer )) /* It ' s usually NULL */ <nl> - kfree ( frozen_buffer ); <nl> + jbd_slab_free ( frozen_buffer , bh -> b_size ); <nl>  <nl> JBUFFER_TRACE ( jh , " exit "); <nl> return error ;
void local_touch_nmi ( void ) <nl> { <nl> __this_cpu_write ( last_nmi_rip , 0 ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( local_touch_nmi );
static int bond_xmit_roundrobin ( struct sk_buff * skb , struct net_device * bond_dev <nl> * send the join / membership reports . The curr_active_slave found <nl> * will send all of this type of traffic . <nl> */ <nl> - if (( iph -> protocol == htons ( IPPROTO_IGMP )) && <nl> + if (( iph -> protocol == IPPROTO_IGMP ) && <nl> ( skb -> protocol == htons ( ETH_P_IP ))) { <nl>  <nl> read_lock (& bond -> curr_slave_lock );
static u8 * __init alloc_event_buffer ( struct amd_iommu * iommu ) <nl> if ( iommu -> evt_buf == NULL ) <nl> return NULL ; <nl>  <nl> + iommu -> evt_buf_size = EVT_BUFFER_SIZE ; <nl> + <nl> return iommu -> evt_buf ; <nl> } <nl> 
int mccic_irq ( struct mcam_camera * cam , unsigned int irqs ) <nl> if ( irqs & ( IRQ_EOF0 << frame )) { <nl> mcam_frame_complete ( cam , frame ); <nl> handled = 1 ; <nl> + if ( cam -> buffer_mode == B_DMA_sg ) <nl> + break ; <nl> } <nl> /* <nl> * If a frame starts , note that we have DMA active . This
static int new_term ( struct parse_events_term ** _term , int type_val , <nl> term -> val . str = str ; <nl> break ; <nl> default : <nl> + free ( term ); <nl> return - EINVAL ; <nl> } <nl> 
static const struct pvr2_ctl_info control_defs [] = { <nl> . internal_id = PVR2_CID_CROPW , <nl> . default_value = 720 , <nl> DEFREF ( cropw ), <nl> + DEFINT ( 0 , 864 ), <nl> . get_max_value = ctrl_cropw_max_get , <nl> . get_def_value = ctrl_get_cropcapdw , <nl> }, { <nl> static const struct pvr2_ctl_info control_defs [] = { <nl> . internal_id = PVR2_CID_CROPH , <nl> . default_value = 480 , <nl> DEFREF ( croph ), <nl> + DEFINT ( 0 , 576 ), <nl> . get_max_value = ctrl_croph_max_get , <nl> . get_def_value = ctrl_get_cropcapdh , <nl> }, {
static int mga_vram_init ( struct mga_device * mdev ) <nl> { <nl> void __iomem * mem ; <nl> struct apertures_struct * aper = alloc_apertures ( 1 ); <nl> + if (! aper ) <nl> + return - ENOMEM ; <nl>  <nl> /* BAR 0 is VRAM */ <nl> mdev -> mc . vram_base = pci_resource_start ( mdev -> dev -> pdev , 0 );
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
static int i915_pipe_crc_open ( struct inode * inode , struct file * filep ) <nl> struct drm_i915_private * dev_priv = info -> dev -> dev_private ; <nl> struct intel_pipe_crc * pipe_crc = & dev_priv -> pipe_crc [ info -> pipe ]; <nl>  <nl> + if ( info -> pipe >= INTEL_INFO ( info -> dev )-> num_pipes ) <nl> + return - ENODEV ; <nl> + <nl> spin_lock_irq (& pipe_crc -> lock ); <nl>  <nl> if ( pipe_crc -> opened ) {
EXPORT_SYMBOL_GPL ( rcu_sched_lock_map ); <nl> # endif <nl>  <nl> int rcu_scheduler_active __read_mostly ; <nl> + EXPORT_SYMBOL_GPL ( rcu_scheduler_active ); <nl>  <nl> /* <nl> * This function is invoked towards the end of the scheduler ' s initialization
static int __devinit twl4030_madc_probe ( struct platform_device * pdev ) <nl> if (! madc ) <nl> return - ENOMEM ; <nl>  <nl> + madc -> dev = & pdev -> dev ; <nl> + <nl> /* <nl> * Phoenix provides 2 interrupt lines . The first one is connected to <nl> * the OMAP . The other one can be connected to the other processor such
static int virtscsi_queuecommand ( struct Scsi_Host * sh , struct scsi_cmnd * sc ) <nl> sizeof cmd -> req . cmd , sizeof cmd -> resp . cmd , <nl> GFP_ATOMIC ) >= 0 ) <nl> ret = 0 ; <nl> + else <nl> + mempool_free ( cmd , virtscsi_cmd_pool ); <nl>  <nl> out : <nl> return ret ;
static int alps_enter_command_mode ( struct psmouse * psmouse , <nl> return - 1 ; <nl> } <nl>  <nl> - if ( param [ 0 ] != 0x88 && param [ 1 ] != 0x07 ) { <nl> + if ( param [ 0 ] != 0x88 || ( param [ 1 ] != 0x07 && param [ 1 ] != 0x08 )) { <nl> psmouse_dbg ( psmouse , <nl> " unknown response while entering command mode \ n "); <nl> return - 1 ;
static void do_ubd_request ( struct request_queue * q ) <nl> " errno = % d \ n ", - n ); <nl> else if ( list_empty (& dev -> restart )) <nl> list_add (& dev -> restart , & restart ); <nl> + kfree ( io_req ); <nl> return ; <nl> } <nl> 
void ieee80211_beacon_connection_loss_work ( struct work_struct * work ) <nl> struct sta_info * sta ; <nl>  <nl> if ( ifmgd -> associated ) { <nl> + rcu_read_lock (); <nl> sta = sta_info_get ( sdata , ifmgd -> bssid ); <nl> if ( sta ) <nl> sta -> beacon_loss_count ++; <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> if ( sdata -> local -> hw . flags & IEEE80211_HW_CONNECTION_MONITOR )
int radeon_info_ioctl ( struct drm_device * dev , void * data , struct drm_file * filp ) <nl> */ <nl> int radeon_driver_firstopen_kms ( struct drm_device * dev ) <nl> { <nl> + struct radeon_device * rdev = dev -> dev_private ; <nl> + <nl> + if ( rdev -> powered_down ) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
static struct mount * clone_mnt ( struct mount * old , struct dentry * root , <nl> } <nl>  <nl> /* Don ' t allow unprivileged users to reveal what is under a mount */ <nl> - if (( flag & CL_UNPRIVILEGED ) && list_empty (& old -> mnt_expire )) <nl> + if (( flag & CL_UNPRIVILEGED ) && <nl> + (!( flag & CL_EXPIRE ) || list_empty (& old -> mnt_expire ))) <nl> mnt -> mnt . mnt_flags |= MNT_LOCKED ; <nl>  <nl> atomic_inc (& sb -> s_active );
static int chaoskey_rng_read ( struct hwrng * rng , void * data , <nl> if ( this_time > max ) <nl> this_time = max ; <nl>  <nl> - memcpy ( data , dev -> buf , this_time ); <nl> + memcpy ( data , dev -> buf + dev -> used , this_time ); <nl>  <nl> dev -> used += this_time ; <nl> 
static int add_munmap ( unsigned long addr , unsigned long len , <nl> struct host_vm_op * last ; <nl> int ret = 0 ; <nl>  <nl> + if (( addr >= STUB_START ) && ( addr < STUB_END )) <nl> + return - EINVAL ; <nl> + <nl> if ( hvc -> index != 0 ) { <nl> last = & hvc -> ops [ hvc -> index - 1 ]; <nl> if (( last -> type == MUNMAP ) &&
ldebugfs_fid_write_common ( const char __user * buffer , size_t count , <nl> rc = sscanf ( kernbuf , "[% llx - % llx ]\ n ", <nl> ( unsigned long long *)& tmp . lsr_start , <nl> ( unsigned long long *)& tmp . lsr_end ); <nl> + if ( rc != 2 ) <nl> + return - EINVAL ; <nl> if (! range_is_sane (& tmp ) || range_is_zero (& tmp ) || <nl> tmp . lsr_start < range -> lsr_start || tmp . lsr_end > range -> lsr_end ) <nl> return - EINVAL ;
static void moxa_start ( struct tty_struct * tty ) <nl> if ( ch == NULL ) <nl> return ; <nl>  <nl> - if (!( ch -> statusflags & TXSTOPPED )) <nl> + if (! test_bit ( TXSTOPPED , & ch -> statusflags )) <nl> return ; <nl>  <nl> MoxaPortTxEnable ( ch );
void ata_bmdma_error_handler ( struct ata_port * ap ) <nl> */ <nl> void ata_bmdma_post_internal_cmd ( struct ata_queued_cmd * qc ) <nl> { <nl> - ata_bmdma_stop ( qc ); <nl> + if ( qc -> ap -> ioaddr . bmdma_addr ) <nl> + ata_bmdma_stop ( qc ); <nl> } <nl>  <nl> # ifdef CONFIG_PCI
static void blade_image_blit ( struct tridentfb_par * par , const char * data , <nl> writemmr ( par , DST1 , point ( x , y )); <nl> writemmr ( par , DST2 , point ( x + w - 1 , y + h - 1 )); <nl>  <nl> - memcpy ( par -> io_virt + 0x10000 , data , 4 * size ); <nl> + iowrite32_rep ( par -> io_virt + 0x10000 , data , size ); <nl> } <nl>  <nl> static void blade_copy_rect ( struct tridentfb_par * par ,
static int tcp_tso_should_defer ( struct sock * sk , struct tcp_sock * tp , struct sk_ <nl> if ( TCP_SKB_CB ( skb )-> flags & TCPCB_FLAG_FIN ) <nl> return 0 ; <nl>  <nl> + if ( tp -> ca_state != TCP_CA_Open ) <nl> + return 0 ; <nl> + <nl> in_flight = tcp_packets_in_flight ( tp ); <nl>  <nl> BUG_ON ( tcp_skb_pcount ( skb ) <= 1 ||
i915_gem_set_tiling ( struct drm_device * dev , void * data , <nl> } <nl>  <nl> mutex_lock (& dev -> struct_mutex ); <nl> - if ( i915_gem_obj_is_pinned ( obj ) || obj -> framebuffer_references ) { <nl> + if ( obj -> pin_display || obj -> framebuffer_references ) { <nl> ret = - EBUSY ; <nl> goto err ; <nl> }
ia64_global_tlb_purge ( struct mm_struct * mm , unsigned long start , <nl> { <nl> static DEFINE_SPINLOCK ( ptcg_lock ); <nl>  <nl> - if ( mm != current -> active_mm ) { <nl> + if ( mm != current -> active_mm || ! current -> mm ) { <nl> flush_tlb_all (); <nl> return ; <nl> }
static struct ib_mr * mthca_reg_phys_mr ( struct ib_pd * pd , <nl> convert_access ( acc ), mr ); <nl>  <nl> if ( err ) { <nl> + kfree ( page_list ); <nl> kfree ( mr ); <nl> return ERR_PTR ( err ); <nl> }
again : <nl> * refill the WL pool synchronous . */ <nl> if ( pool -> used == pool -> size || wl_pool -> used == wl_pool -> size ) { <nl> spin_unlock (& ubi -> wl_lock ); <nl> - ubi_update_fastmap ( ubi ); <nl> + ret = ubi_update_fastmap ( ubi ); <nl> + if ( ret ) { <nl> + ubi_msg ( ubi , " Unable to write a new fastmap : % i ", ret ); <nl> + return - ENOSPC ; <nl> + } <nl> spin_lock (& ubi -> wl_lock ); <nl> } <nl> 
void blkcg_drain_queue ( struct request_queue * q ) <nl> { <nl> lockdep_assert_held ( q -> queue_lock ); <nl>  <nl> + /* <nl> + * @ q could be exiting and already have destroyed all blkgs as <nl> + * indicated by NULL root_blkg . If so , don ' t confuse policies . <nl> + */ <nl> + if (! q -> root_blkg ) <nl> + return ; <nl> + <nl> blk_throtl_drain ( q ); <nl> } <nl> 
static void __init h2_init_smc91x ( void ) <nl>  <nl> static struct i2c_board_info __initdata h2_i2c_board_info [] = { <nl> { <nl> + I2C_BOARD_INFO (" tps65010 ", 0x48 ), <nl> + . type = " tps65010 ", <nl> + . irq = OMAP_GPIO_IRQ ( 58 ), <nl> + }, { <nl> I2C_BOARD_INFO (" isp1301_omap ", 0x2d ), <nl> . type = " isp1301_omap ", <nl> . irq = OMAP_GPIO_IRQ ( 2 ),
static int __devinit fealnx_init_one ( struct pci_dev * pdev , <nl> if ( np -> flags == HAS_MII_XCVR ) { <nl> int phy , phy_idx = 0 ; <nl>  <nl> - for ( phy = 1 ; phy < 32 && phy_idx < 4 ; phy ++) { <nl> + for ( phy = 1 ; phy < 32 && phy_idx < ARRAY_SIZE ( np -> phys ); <nl> + phy ++) { <nl> int mii_status = mdio_read ( dev , phy , 1 ); <nl>  <nl> if ( mii_status != 0xffff && mii_status != 0x0000 ) {
static int send_reply ( struct svcxprt_rdma * rdma , <nl> " svcrdma : could not post a receive buffer , err =% d ." <nl> " Closing transport % p .\ n ", ret , rdma ); <nl> set_bit ( XPT_CLOSE , & rdma -> sc_xprt . xpt_flags ); <nl> - return 0 ; <nl> + svc_rdma_put_context ( ctxt , 0 ); <nl> + return - ENOTCONN ; <nl> } <nl>  <nl> /* Prepare the context */
int clockevents_unbind_device ( struct clock_event_device * ced , int cpu ) <nl> mutex_unlock (& clockevents_mutex ); <nl> return ret ; <nl> } <nl> - EXPORT_SYMBOL_GPL ( clockevents_unbind ); <nl> + EXPORT_SYMBOL_GPL ( clockevents_unbind_device ); <nl>  <nl> /** <nl> * clockevents_register_device - register a clock event device
int ixgbe_ndo_set_vf_spoofchk ( struct net_device * netdev , int vf , bool setting ) <nl> struct ixgbe_hw * hw = & adapter -> hw ; <nl> u32 regval ; <nl>  <nl> + if ( vf >= adapter -> num_vfs ) <nl> + return - EINVAL ; <nl> + <nl> adapter -> vfinfo [ vf ]. spoofchk_enabled = setting ; <nl>  <nl> regval = IXGBE_READ_REG ( hw , IXGBE_PFVFSPOOF ( vf_target_reg ));
static void ftdi_process_read ( struct work_struct * work ) <nl> spin_unlock_irqrestore (& priv -> rx_lock , flags ); <nl> dbg ("% s - deferring remainder until unthrottled ", <nl> __func__ ); <nl> - return ; <nl> + goto out ; <nl> } <nl> spin_unlock_irqrestore (& priv -> rx_lock , flags ); <nl> /* if the port is closed stop trying to read */
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> if (! dev -> suspend_in_progress ) <nl> b43_rng_init ( wl ); <nl>  <nl> + ieee80211_wake_queues ( dev -> wl -> hw ); <nl> + <nl> b43_set_status ( dev , B43_STAT_INITIALIZED ); <nl>  <nl> if (! dev -> suspend_in_progress )
int rt2x00mac_set_key ( struct ieee80211_hw * hw , enum set_key_cmd cmd , <nl> crypto . cipher = rt2x00crypto_key_to_cipher ( key ); <nl> if ( crypto . cipher == CIPHER_NONE ) <nl> return - EOPNOTSUPP ; <nl> + if ( crypto . cipher == CIPHER_TKIP && rt2x00_is_usb ( rt2x00dev )) <nl> + return - EOPNOTSUPP ; <nl>  <nl> crypto . cmd = cmd ; <nl> 
void rtw_alloc_hwxmits ( struct adapter * padapter ) <nl>  <nl> pxmitpriv -> hwxmit_entry = HWXMIT_ENTRY ; <nl>  <nl> - pxmitpriv -> hwxmits = kzalloc ( sizeof ( struct hw_xmit ) * pxmitpriv -> hwxmit_entry , GFP_KERNEL ); <nl> + pxmitpriv -> hwxmits = kcalloc ( pxmitpriv -> hwxmit_entry , <nl> + sizeof ( struct hw_xmit ), GFP_KERNEL ); <nl>  <nl> hwxmits = pxmitpriv -> hwxmits ; <nl> 
out : <nl> if ( rc != MIGRATEPAGE_SUCCESS && put_new_page ) <nl> put_new_page ( new_hpage , private ); <nl> else <nl> - put_page ( new_hpage ); <nl> + putback_active_hugepage ( new_hpage ); <nl>  <nl> if ( result ) { <nl> if ( rc )
int __init main ( int argc , char ** argv , char ** envp ) <nl> # endif <nl>  <nl> do_uml_initcalls (); <nl> + change_sig ( SIGPIPE , 0 ); <nl> ret = linux_main ( argc , argv ); <nl>  <nl> /*
static int omap3_onenand_read_bufferram ( struct mtd_info * mtd , int area , <nl> if ( bram_offset & 3 || ( size_t ) buf & 3 || count < 384 ) <nl> goto out_copy ; <nl>  <nl> + /* panic_write () may be in an interrupt context */ <nl> + if ( in_interrupt ()) <nl> + goto out_copy ; <nl> + <nl> if ( buf >= high_memory ) { <nl> struct page * p1 ; <nl> 
static int vfio_set_trigger ( struct vfio_platform_device * vdev , int index , <nl> int ret ; <nl>  <nl> if ( irq -> trigger ) { <nl> + irq_clear_status_flags ( irq -> hwirq , IRQ_NOAUTOEN ); <nl> free_irq ( irq -> hwirq , irq ); <nl> kfree ( irq -> name ); <nl> eventfd_ctx_put ( irq -> trigger );
static int exynos_drm_fbdev_update ( struct drm_fb_helper * helper , <nl>  <nl> fbi -> screen_base = buffer -> kvaddr + offset ; <nl> fbi -> screen_size = size ; <nl> + fbi -> fix . smem_len = size ; <nl>  <nl> return 0 ; <nl> }
__perf_counter_exit_task ( struct task_struct * child , <nl> } <nl> } <nl>  <nl> - kfree ( child_counter ); <nl> + if (! child_counter -> filp || ! atomic_long_read (& child_counter -> filp -> f_count )) <nl> + kfree ( child_counter ); <nl> } <nl>  <nl> /*
static unsigned long cfq_slice_offset ( struct cfq_data * cfqd , <nl> /* <nl> * just an approximation , should be ok . <nl> */ <nl> - return (( cfqd -> busy_queues - 1 ) * cfq_prio_slice ( cfqd , 1 , 0 )); <nl> + return ( cfqd -> busy_queues - 1 ) * ( cfq_prio_slice ( cfqd , 1 , 0 ) - <nl> + cfq_prio_slice ( cfqd , cfq_cfqq_sync ( cfqq ), cfqq -> ioprio )); <nl> } <nl>  <nl> /*
static void pxa3xx_nand_cmdfunc ( struct mtd_info * mtd , unsigned command , <nl> /* disable HW ECC to get all the OOB data */ <nl> info -> buf_count = mtd -> writesize + mtd -> oobsize ; <nl> info -> buf_start = mtd -> writesize + column ; <nl> + memset ( info -> data_buff , 0xFF , info -> buf_count ); <nl>  <nl> if ( prepare_read_prog_cmd ( info , cmdset -> read1 , column , page_addr )) <nl> break ;
struct ion_device * ion_device_create ( long (* custom_ioctl ) <nl> ret = misc_register (& idev -> dev ); <nl> if ( ret ) { <nl> pr_err (" ion : failed to register misc device .\ n "); <nl> + kfree ( idev ); <nl> return ERR_PTR ( ret ); <nl> } <nl> 
static enum odd_mech_type zpodd_get_mech_type ( struct ata_device * dev ) <nl> static bool odd_can_poweroff ( struct ata_device * ata_dev ) <nl> { <nl> acpi_handle handle ; <nl> - acpi_status status ; <nl> struct acpi_device * acpi_dev ; <nl>  <nl> handle = ata_dev_acpi_handle ( ata_dev ); <nl> if (! handle ) <nl> return false ; <nl>  <nl> - status = acpi_bus_get_device ( handle , & acpi_dev ); <nl> - if ( ACPI_FAILURE ( status )) <nl> + if ( acpi_bus_get_device ( handle , & acpi_dev )) <nl> return false ; <nl>  <nl> return acpi_device_can_poweroff ( acpi_dev );
static __devinit int vpbe_probe ( struct platform_device * pdev ) <nl>  <nl> if ( cfg -> outputs -> num_modes > 0 ) <nl> vpbe_dev -> current_timings = vpbe_dev -> cfg -> outputs [ 0 ]. modes [ 0 ]; <nl> - else <nl> + else { <nl> + kfree ( vpbe_dev ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> /* set the driver data in platform device */ <nl> platform_set_drvdata ( pdev , vpbe_dev );
struct kvm_vcpu_arch { <nl> struct kvm_mmu_memory_cache mmu_page_cache ; <nl>  <nl> /* Target CPU and feature flags */ <nl> - u32 target ; <nl> + int target ; <nl> DECLARE_BITMAP ( features , KVM_VCPU_MAX_FEATURES ); <nl>  <nl> /* Detect first run of a vcpu */
int __nvme_submit_sync_cmd ( struct request_queue * q , struct nvme_command * cmd , <nl> return PTR_ERR ( req ); <nl>  <nl> req -> cmd_type = REQ_TYPE_DRV_PRIV ; <nl> + req -> cmd_flags = REQ_FAILFAST_DRIVER ; <nl> req -> __data_len = 0 ; <nl> req -> __sector = ( sector_t ) - 1 ; <nl> req -> bio = req -> biotail = NULL ;
static int uevent_net_init ( struct net * net ) <nl> if (! ue_sk -> sk ) { <nl> printk ( KERN_ERR <nl> " kobject_uevent : unable to create netlink socket !\ n "); <nl> + kfree ( ue_sk ); <nl> return - ENODEV ; <nl> } <nl> mutex_lock (& uevent_sock_mutex );
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl> ! iwlwifi_mod_params . sw_crypto ) <nl> hw -> flags |= IEEE80211_HW_MFP_CAPABLE ; <nl>  <nl> - if ( mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_UAPSD_SUPPORT ) { <nl> + if ( 0 && mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_UAPSD_SUPPORT ) { <nl> hw -> flags |= IEEE80211_HW_SUPPORTS_UAPSD ; <nl> hw -> uapsd_queues = IWL_UAPSD_AC_INFO ; <nl> hw -> uapsd_max_sp_len = IWL_UAPSD_MAX_SP ;
static int generic_set_freq ( struct dvb_frontend * fe , <nl> goto err ; <nl>  <nl> rc = r820t_sysfreq_sel ( priv , freq , type , std , delsys ); <nl> + if ( rc < 0 ) <nl> + goto err ; <nl> + <nl> + tuner_dbg ("% s : PLL locked on frequency % d Hz , gain =% d \ n ", <nl> + __func__ , freq , r820t_read_gain ( priv )); <nl> + <nl> err : <nl>  <nl> if ( rc < 0 )
int f2fs_getxattr ( struct inode * inode , int name_index , const char * name , <nl> if ( name == NULL ) <nl> return - EINVAL ; <nl> name_len = strlen ( name ); <nl> + if ( name_len > F2FS_NAME_LEN ) <nl> + return - ERANGE ; <nl>  <nl> base_addr = read_all_xattrs ( inode , NULL ); <nl> if (! base_addr )
int iwl_enqueue_hcmd ( struct iwl_priv * priv , struct iwl_host_cmd * cmd ) <nl> return - EIO ; <nl> } <nl>  <nl> + if (( priv -> ucode_owner == IWL_OWNERSHIP_TM ) && <nl> + !( cmd -> flags & CMD_ON_DEMAND )) { <nl> + IWL_DEBUG_HC ( priv , " tm own the uCode , no regular hcmd send \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> copy_size = sizeof ( out_cmd -> hdr ); <nl> cmd_size = sizeof ( out_cmd -> hdr ); <nl> 
static void ext4_orphan_cleanup ( struct super_block * sb , <nl> jbd_debug ( 2 , " truncating inode % lu to % lld bytes \ n ", <nl> inode -> i_ino , inode -> i_size ); <nl> mutex_lock (& inode -> i_mutex ); <nl> + truncate_inode_pages ( inode -> i_mapping , inode -> i_size ); <nl> ext4_truncate ( inode ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> nr_truncates ++;
void assert_pipe ( struct drm_i915_private * dev_priv , <nl> u32 val ; <nl> bool cur_state ; <nl>  <nl> + /* if we need the pipe A quirk it must be always on */ <nl> + if ( pipe == PIPE_A && dev_priv -> quirks & QUIRK_PIPEA_FORCE ) <nl> + state = true ; <nl> + <nl> reg = PIPECONF ( pipe ); <nl> val = I915_READ ( reg ); <nl> cur_state = !!( val & PIPECONF_ENABLE );
static int ide_diag_taskfile ( ide_drive_t * drive , ide_task_t * args , unsigned long <nl> struct request rq ; <nl>  <nl> memset (& rq , 0 , sizeof ( rq )); <nl> + rq . ref_count = 1 ; <nl> rq . cmd_type = REQ_TYPE_ATA_TASKFILE ; <nl> rq . buffer = buf ; <nl> 
static int mga_vram_init ( struct mga_device * mdev ) <nl> aper -> count = 1 ; <nl>  <nl> remove_conflicting_framebuffers ( aper , " mgafb ", true ); <nl> + kfree ( aper ); <nl>  <nl> if (! request_mem_region ( mdev -> mc . vram_base , mdev -> mc . vram_window , <nl> " mgadrmfb_vram ")) {
static int visornic_probe ( struct visor_device * dev ) <nl> goto cleanup_netdev ; <nl> } <nl>  <nl> - devdata -> rcvbuf = kzalloc ( sizeof ( struct sk_buff *) * <nl> - devdata -> num_rcv_bufs , GFP_KERNEL ); <nl> + devdata -> rcvbuf = kcalloc ( devdata -> num_rcv_bufs , <nl> + sizeof ( struct sk_buff *), GFP_KERNEL ); <nl> if (! devdata -> rcvbuf ) { <nl> err = - ENOMEM ; <nl> goto cleanup_rcvbuf ;
bool f2fs_may_inline ( struct inode * inode ) <nl> if ( f2fs_is_atomic_file ( inode )) <nl> return false ; <nl>  <nl> - if (! S_ISREG ( inode -> i_mode )) <nl> + if (! S_ISREG ( inode -> i_mode ) && ! S_ISLNK ( inode -> i_mode )) <nl> return false ; <nl>  <nl> if ( i_size_read ( inode ) > MAX_INLINE_DATA )
static int snd_card_asihpi_trigger ( struct snd_pcm_substream * substream , <nl> VPRINTK1 ( KERN_INFO " start \ n "); <nl> /* start the master stream */ <nl> snd_card_asihpi_pcm_timer_start ( substream ); <nl> - if ( substream -> stream == SNDRV_PCM_STREAM_CAPTURE ) <nl> + if (( substream -> stream == SNDRV_PCM_STREAM_CAPTURE ) || <nl> + ! card -> support_mmap ) <nl> hpi_handle_error ( hpi_stream_start ( dpcm -> h_stream )); <nl> break ; <nl> 
static int patch_stac922x ( struct hda_codec * codec ) <nl> */ <nl> printk ( KERN_INFO " hda_codec : STAC922x , Apple subsys_id =% x \ n ", codec -> subsystem_id ); <nl> switch ( codec -> subsystem_id ) { <nl> + case 0x106b0a00 : /* MacBook First generatoin */ <nl> + spec -> board_config = STAC_MACBOOK ; <nl> + break ; <nl> case 0x106b0200 : /* MacBook Pro first generation */ <nl> spec -> board_config = STAC_MACBOOK_PRO_V1 ; <nl> break ;
static int of_mpc8xxx_spi_get_chipselects ( struct device * dev ) <nl> gpio = of_get_gpio_flags ( np , i , & flags ); <nl> if (! gpio_is_valid ( gpio )) { <nl> dev_err ( dev , " invalid gpio #% d : % d \ n ", i , gpio ); <nl> + ret = gpio ; <nl> goto err_loop ; <nl> } <nl> 
static void __exit powernv_cpufreq_exit ( void ) <nl> unregister_reboot_notifier (& powernv_cpufreq_reboot_nb ); <nl> opal_message_notifier_unregister ( OPAL_MSG_OCC , <nl> & powernv_cpufreq_opal_nb ); <nl> + kfree ( chips ); <nl> cpufreq_unregister_driver (& powernv_cpufreq_driver ); <nl> } <nl> module_exit ( powernv_cpufreq_exit );
static DEFINE_MUTEX ( nb_smu_ind_mutex ); <nl> * Control ] <nl> */ <nl> # define F15H_M60H_REPORTED_TEMP_CTRL_OFFSET 0xd8200ca4 <nl> -# define PCI_DEVICE_ID_AMD_15H_M60H_NB_F3 0x1573 <nl>  <nl> static void amd_nb_smu_index_read ( struct pci_dev * pdev , unsigned int devfn , <nl> int offset , u32 * val )
static int dn_shutdown ( struct socket * sock , int how ) <nl> if ( scp -> state == DN_O ) <nl> goto out ; <nl>  <nl> - if ( how != SHUTDOWN_MASK ) <nl> + if ( how != SHUT_RDWR ) <nl> goto out ; <nl>  <nl> - sk -> sk_shutdown = how ; <nl> + sk -> sk_shutdown = SHUTDOWN_MASK ; <nl> dn_destroy_sock ( sk ); <nl> err = 0 ; <nl> 
static ssize_t hid_debug_events_read ( struct file * file , char __user * buffer , <nl>  <nl> if (! list -> hdev || ! list -> hdev -> debug ) { <nl> ret = - EIO ; <nl> - break ; <nl> + set_current_state ( TASK_RUNNING ); <nl> + goto out ; <nl> } <nl>  <nl> /* allow O_NONBLOCK from other threads */
int pstore_register ( struct pstore_info * psi ) <nl> add_timer (& pstore_timer ); <nl> } <nl>  <nl> + /* <nl> + * Update the module parameter backend , so it is visible <nl> + * through / sys / module / pstore / parameters / backend <nl> + */ <nl> + backend = psi -> name ; <nl> + <nl> pr_info (" Registered % s as persistent store backend \ n ", psi -> name ); <nl>  <nl> return 0 ;
EXPORT_SYMBOL_GPL ( irq_of_parse_and_map ); <nl>  <nl> void irq_dispose_mapping ( unsigned int virq ) <nl> { <nl> - struct irq_host * host = irq_map [ virq ]. host ; <nl> + struct irq_host * host ; <nl> irq_hw_number_t hwirq ; <nl> unsigned long flags ; <nl>  <nl> + if ( virq == NO_IRQ ) <nl> + return ; <nl> + <nl> + host = irq_map [ virq ]. host ; <nl> WARN_ON ( host == NULL ); <nl> if ( host == NULL ) <nl> return ;
static struct hw_breakpoint { <nl> unsigned long addr ; <nl> int len ; <nl> int type ; <nl> - struct perf_event ** pev ; <nl> + struct perf_event * __percpu * pev ; <nl> } breakinfo [ HBP_NUM ]; <nl>  <nl> static unsigned long early_dr7 ;
cache_type_store ( struct device * dev , struct device_attribute * attr , <nl> buffer_data [ 2 ] &= ~ 0x05 ; <nl> buffer_data [ 2 ] |= wce << 2 | rcd ; <nl> sp = buffer_data [ 0 ] & 0x80 ? 1 : 0 ; <nl> + buffer_data [ 0 ] &= ~ 0x80 ; <nl>  <nl> if ( scsi_mode_select ( sdp , 1 , sp , 8 , buffer_data , len , SD_TIMEOUT , <nl> SD_MAX_RETRIES , & data , & sshdr )) {
static int ad1836_register ( struct ad1836_priv * ad1836 ) <nl>  <nl> if ( ad1836_codec ) { <nl> dev_err ( codec -> dev , " Another ad1836 is registered \ n "); <nl> + kfree ( ad1836 ); <nl> return - EINVAL ; <nl> } <nl> 
int ide_device_add ( u8 idx [ 4 ]) <nl>  <nl> hwif = & ide_hwifs [ idx [ i ]]; <nl>  <nl> - if ( hwif -> present ) <nl> + if ( hwif -> present ) { <nl> + if ( hwif -> chipset == ide_unknown || <nl> + hwif -> chipset == ide_forced ) <nl> + hwif -> chipset = ide_generic ; <nl> hwif_register_devices ( hwif ); <nl> + } <nl> } <nl>  <nl> for ( i = 0 ; i < 4 ; i ++) {
static int generic_hdmi_build_controls ( struct hda_codec * codec ) <nl> struct snd_pcm_chmap * chmap ; <nl> struct snd_kcontrol * kctl ; <nl> int i ; <nl> + <nl> + if (! codec -> pcm_info [ pin_idx ]. pcm ) <nl> + break ; <nl> err = snd_pcm_add_chmap_ctls ( codec -> pcm_info [ pin_idx ]. pcm , <nl> SNDRV_PCM_STREAM_PLAYBACK , <nl> NULL , 0 , pin_idx , & chmap );
static int rtas_excl_open ( struct inode * inode , struct file * file ) <nl>  <nl> /* Enforce exclusive open with use count of PDE */ <nl> spin_lock (& flash_file_open_lock ); <nl> - if ( atomic_read (& dp -> count ) > 1 ) { <nl> + if ( atomic_read (& dp -> count ) > 2 ) { <nl> spin_unlock (& flash_file_open_lock ); <nl> return - EBUSY ; <nl> }
static inline void of_pci_check_probe_only ( void ) { } <nl> int of_pci_get_host_bridge_resources ( struct device_node * dev , <nl> unsigned char busno , unsigned char bus_max , <nl> struct list_head * resources , resource_size_t * io_base ); <nl> +# else <nl> + static inline int of_pci_get_host_bridge_resources ( struct device_node * dev , <nl> + unsigned char busno , unsigned char bus_max , <nl> + struct list_head * resources , resource_size_t * io_base ) <nl> +{ <nl> + return - EINVAL ; <nl> +} <nl> # endif <nl>  <nl> # if defined ( CONFIG_OF ) && defined ( CONFIG_PCI_MSI )
static void ath6kl_recovery_work ( struct work_struct * work ) <nl>  <nl> ar -> fw_recovery . err_reason = 0 ; <nl>  <nl> - if ( ar -> fw_recovery . enable ) <nl> + if ( ar -> fw_recovery . hb_poll ) <nl> mod_timer (& ar -> fw_recovery . hb_timer , jiffies + <nl> msecs_to_jiffies ( ar -> fw_recovery . hb_poll )); <nl> }
match ( const struct sk_buff * skb , <nl> return 0 ; <nl> } <nl>  <nl> + if ( mh -> ip6mh_proto != IPPROTO_NONE ) { <nl> + duprintf (" Dropping invalid MH Payload Proto : % u \ n ", <nl> + mh -> ip6mh_proto ); <nl> + * hotdrop = 1 ; <nl> + return 0 ; <nl> + } <nl> + <nl> return type_match ( mhinfo -> types [ 0 ], mhinfo -> types [ 1 ], mh -> ip6mh_type , <nl> !!( mhinfo -> invflags & IP6T_MH_INV_TYPE )); <nl> }
static int balloon ( void * _vballoon ) <nl> try_to_freeze (); <nl> wait_event_interruptible ( vb -> config_change , <nl> ( diff = towards_target ( vb )) != 0 <nl> - || kthread_should_stop ()); <nl> + || kthread_should_stop () <nl> + || freezing ( current )); <nl> if ( diff > 0 ) <nl> fill_balloon ( vb , diff ); <nl> else if ( diff < 0 )
struct vm_struct * alloc_vm_area ( size_t size ) <nl> return NULL ; <nl> } <nl>  <nl> - /* Make sure the pagetables are constructed in process kernel <nl> - mappings */ <nl> - vmalloc_sync_all (); <nl> - <nl> return area ; <nl> } <nl> EXPORT_SYMBOL_GPL ( alloc_vm_area );
int xfrm_init_replay ( struct xfrm_state * x ) <nl> replay_esn -> bmp_len * sizeof ( __u32 ) * 8 ) <nl> return - EINVAL ; <nl>  <nl> + if (( x -> props . flags & XFRM_STATE_ESN ) && replay_esn -> replay_window == 0 ) <nl> + return - EINVAL ; <nl> + <nl> if (( x -> props . flags & XFRM_STATE_ESN ) && x -> replay_esn ) <nl> x -> repl = & xfrm_replay_esn ; <nl> else
static int imx1_pinctrl_parse_functions ( struct device_node * np , <nl> /* Initialise function */ <nl> func -> name = np -> name ; <nl> func -> num_groups = of_get_child_count ( np ); <nl> - if ( func -> num_groups <= 0 ) <nl> + if ( func -> num_groups == 0 ) <nl> return - EINVAL ; <nl>  <nl> func -> groups = devm_kzalloc ( info -> dev ,
static int do_ip_getsockopt ( struct sock * sk , int level , int optname , <nl> case IP_HDRINCL : <nl> val = inet -> hdrincl ; <nl> break ; <nl> + case IP_NODEFRAG : <nl> + val = inet -> nodefrag ; <nl> + break ; <nl> case IP_MTU_DISCOVER : <nl> val = inet -> pmtudisc ; <nl> break ;
int symbol__init ( void ) <nl> if ( symbol_conf . initialized ) <nl> return 0 ; <nl>  <nl> + symbol_conf . priv_size = ALIGN ( symbol_conf . priv_size , sizeof ( u64 )); <nl> + <nl> elf_version ( EV_CURRENT ); <nl> if ( symbol_conf . sort_by_name ) <nl> symbol_conf . priv_size += ( sizeof ( struct symbol_name_rb_node ) -
static int rt2800_init_registers ( struct rt2x00_dev * rt2x00dev ) <nl>  <nl> rt2800_register_read ( rt2x00dev , MM40_PROT_CFG , & reg ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_RATE , 0x4084 ); <nl> - rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_CTRL , <nl> - ! rt2x00_is_usb ( rt2x00dev )); <nl> + rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_CTRL , 0 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_NAV , 1 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_TX_OP_ALLOW_CCK , 1 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_TX_OP_ALLOW_OFDM , 1 );
static enum dma_status omap_dma_tx_status ( struct dma_chan * chan , <nl>  <nl> if ( d -> dir == DMA_MEM_TO_DEV ) <nl> pos = omap_dma_get_src_pos ( c ); <nl> - else if ( d -> dir == DMA_DEV_TO_MEM ) <nl> + else if ( d -> dir == DMA_DEV_TO_MEM || d -> dir == DMA_MEM_TO_MEM ) <nl> pos = omap_dma_get_dst_pos ( c ); <nl> else <nl> pos = 0 ;
static enum BC_STATUS bc_cproc_download_fw ( struct crystalhd_cmd * ctx , <nl> sts = crystalhd_download_fw ( ctx -> adp , ( uint8_t *) idata -> add_cdata , <nl> idata -> add_cdata_sz ); <nl>  <nl> - if ( sts != BC_STS_SUCCESS ) { <nl> + if ( sts != BC_STS_SUCCESS ) <nl> BCMLOG_ERR (" Firmware Download Failure !! - % d \ n ", sts ); <nl> - } else <nl> + else <nl> ctx -> state |= BC_LINK_INIT ; <nl>  <nl> return sts ;
static struct ieee80211_ops agnx_ops = { <nl> static void __devexit agnx_pci_remove ( struct pci_dev * pdev ) <nl> { <nl> struct ieee80211_hw * dev = pci_get_drvdata ( pdev ); <nl> - struct agnx_priv * priv = dev -> priv ; <nl> + struct agnx_priv * priv ; <nl> AGNX_TRACE ; <nl>  <nl> if (! dev ) <nl> return ; <nl> + priv = dev -> priv ; <nl> ieee80211_unregister_hw ( dev ); <nl> pci_iounmap ( pdev , priv -> ctl ); <nl> pci_iounmap ( pdev , priv -> data );
static struct dmar_domain * dmar_insert_one_dev_info ( struct intel_iommu * iommu , <nl>  <nl> if ( ret ) { <nl> spin_unlock_irqrestore (& device_domain_lock , flags ); <nl> + free_devinfo_mem ( info ); <nl> return NULL ; <nl> } <nl> 
int setup_arg_pages ( struct linux_binprm * bprm , <nl> # else <nl> stack_top = arch_align_stack ( stack_top ); <nl> stack_top = PAGE_ALIGN ( stack_top ); <nl> + <nl> + if ( unlikely ( stack_top < mmap_min_addr ) || <nl> + unlikely ( vma -> vm_end - vma -> vm_start >= stack_top - mmap_min_addr )) <nl> + return - ENOMEM ; <nl> + <nl> stack_shift = vma -> vm_end - stack_top ; <nl>  <nl> bprm -> p -= stack_shift ;
INTEL_VGA_DEVICE ( 0x191D , info ) /* WKS GT2 */ <nl>  <nl> # define INTEL_SKL_GT3_IDS ( info ) \ <nl> + INTEL_VGA_DEVICE ( 0x1923 , info ), /* ULT GT3 */ \ <nl> INTEL_VGA_DEVICE ( 0x1926 , info ), /* ULT GT3 */ \ <nl> + INTEL_VGA_DEVICE ( 0x1927 , info ), /* ULT GT3 */ \ <nl> INTEL_VGA_DEVICE ( 0x192B , info ), /* Halo GT3 */ \ <nl> INTEL_VGA_DEVICE ( 0x192A , info ) /* SRV GT3 */ <nl> 
void omap_gem_init ( struct drm_device * dev ) <nl> } <nl>  <nl> usergart = kzalloc ( 3 * sizeof (* usergart ), GFP_KERNEL ); <nl> + if (! usergart ) { <nl> + dev_warn ( dev -> dev , " could not allocate usergart \ n "); <nl> + return ; <nl> + } <nl>  <nl> /* reserve 4k aligned / wide regions for userspace mappings : */ <nl> for ( i = 0 ; i < ARRAY_SIZE ( fmts ); i ++) {
xfs_acl_from_disk ( struct xfs_acl * aclp ) <nl> int count , i ; <nl>  <nl> count = be32_to_cpu ( aclp -> acl_cnt ); <nl> + if ( count > XFS_ACL_MAX_ENTRIES ) <nl> + return ERR_PTR (- EFSCORRUPTED ); <nl>  <nl> acl = posix_acl_alloc ( count , GFP_KERNEL ); <nl> if (! acl )
static struct sk_buff * ieee80211_build_hdr ( struct ieee80211_sub_if_data * sdata , <nl> authorized = test_sta_flag ( sta , WLAN_STA_AUTHORIZED ); <nl> wme_sta = sta -> sta . wme ; <nl> have_station = true ; <nl> + } else if ( sdata -> wdev . use_4addr ) { <nl> + ret = - ENOLINK ; <nl> + goto free ; <nl> } <nl> ap_sdata = container_of ( sdata -> bss , struct ieee80211_sub_if_data , <nl> u . ap );
static int dtv_property_legacy_params_sync ( struct dvb_frontend * fe , <nl>  <nl> static bool has_get_frontend ( struct dvb_frontend * fe ) <nl> { <nl> - return fe -> ops . get_frontend ; <nl> + return fe -> ops . get_frontend != NULL ; <nl> } <nl>  <nl> /*
static int xhci_setup_device ( struct usb_hcd * hcd , struct usb_device * udev , <nl>  <nl> mutex_lock (& xhci -> mutex ); <nl>  <nl> + if ( xhci -> xhc_state ) /* dying or halted */ <nl> + goto out ; <nl> + <nl> if (! udev -> slot_id ) { <nl> xhci_dbg_trace ( xhci , trace_xhci_dbg_address , <nl> " Bad Slot ID % d ", udev -> slot_id );
static int show_numa_map ( struct seq_file * m , void * v , int is_pid ) <nl> walk . mm = mm ; <nl>  <nl> pol = get_vma_policy ( task , vma , vma -> vm_start ); <nl> - mpol_to_str ( buffer , sizeof ( buffer ), pol ); <nl> + n = mpol_to_str ( buffer , sizeof ( buffer ), pol ); <nl> mpol_cond_put ( pol ); <nl> + if ( n < 0 ) <nl> + return n ; <nl>  <nl> seq_printf ( m , "% 08lx % s ", vma -> vm_start , buffer ); <nl> 
static int mei_irq_thread_write_handler ( struct mei_io_list * cmpl_list , <nl> return 0 ; <nl> } <nl> * slots = mei_count_empty_write_slots ( dev ); <nl> + if (* slots <= 0 ) <nl> + return - EMSGSIZE ; <nl> + <nl> /* complete all waiting for write CB */ <nl> dev_dbg (& dev -> pdev -> dev , " complete all waiting for write cb .\ n "); <nl> 
int bench_sched_messaging ( int argc , const char ** argv , <nl> break ; <nl> } <nl>  <nl> + free ( pth_tab ); <nl> + <nl> return 0 ; <nl> }
int ip6_xmit ( struct sock * sk , struct sk_buff * skb , struct flowi * fl , <nl> skb_reset_network_header ( skb ); <nl> hdr = ipv6_hdr ( skb ); <nl>  <nl> + /* Allow local fragmentation . */ <nl> + if ( ipfragok ) <nl> + skb -> local_df = 1 ; <nl> + <nl> /* <nl> * Fill in the IPv6 header <nl> */
static void handle_callback ( struct gfs2_glock * gl , unsigned int state , int remot <nl> } <nl> return ; <nl> } <nl> - } else if ( gl -> gl_demote_state != LM_ST_UNLOCKED ) { <nl> - gl -> gl_demote_state = state ; <nl> + } else if ( gl -> gl_demote_state != LM_ST_UNLOCKED && <nl> + gl -> gl_demote_state != state ) { <nl> + gl -> gl_demote_state = LM_ST_UNLOCKED ; <nl> } <nl> spin_unlock (& gl -> gl_spin ); <nl> }
int do_huge_pmd_numa_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl>  <nl> check_same : <nl> spin_lock (& mm -> page_table_lock ); <nl> - if ( unlikely (! pmd_same ( pmd , * pmdp ))) <nl> + if ( unlikely (! pmd_same ( pmd , * pmdp ))) { <nl> + /* Someone else took our fault */ <nl> + current_nid = - 1 ; <nl> goto out_unlock ; <nl> + } <nl> clear_pmdnuma : <nl> pmd = pmd_mknonnuma ( pmd ); <nl> set_pmd_at ( mm , haddr , pmdp , pmd );
__visible void prepare_exit_to_usermode ( struct pt_regs * regs ) <nl> READ_ONCE ( pt_regs_to_thread_info ( regs )-> flags ); <nl>  <nl> if (!( cached_flags & ( _TIF_SIGPENDING | _TIF_NOTIFY_RESUME | <nl> - _TIF_UPROBE | _TIF_NEED_RESCHED ))) <nl> + _TIF_UPROBE | _TIF_NEED_RESCHED | <nl> + _TIF_USER_RETURN_NOTIFY ))) <nl> break ; <nl>  <nl> /* We have work to do . */
static int __init usba_udc_probe ( struct platform_device * pdev ) <nl> usba_writel ( udc , CTRL , USBA_DISABLE_MASK ); <nl> clk_disable ( pclk ); <nl>  <nl> - usba_ep = kmalloc ( sizeof ( struct usba_ep ) * pdata -> num_ep , <nl> + usba_ep = kzalloc ( sizeof ( struct usba_ep ) * pdata -> num_ep , <nl> GFP_KERNEL ); <nl> if (! usba_ep ) <nl> goto err_alloc_ep ;
scsi_reset_provider ( struct scsi_device * dev , int flag ) <nl> rtn = FAILED ; <nl> } <nl>  <nl> - scsi_delete_timer ( scmd ); <nl> scsi_next_command ( scmd ); <nl> return rtn ; <nl> }
pvpanic_panic_notify ( struct notifier_block * nb , unsigned long code , <nl>  <nl> static struct notifier_block pvpanic_panic_nb = { <nl> . notifier_call = pvpanic_panic_notify , <nl> + . priority = 1 , /* let this called before broken drm_fb_helper */ <nl> }; <nl>  <nl> 
int lustre_start_mgc ( struct super_block * sb ) <nl>  <nl> /* Random uuid for MGC allows easier reconnects */ <nl> OBD_ALLOC_PTR ( uuid ); <nl> + if (! uuid ) { <nl> + rc = - ENOMEM ; <nl> + goto out_free ; <nl> + } <nl> + <nl> ll_generate_random_uuid ( uuidc ); <nl> class_uuid_unparse ( uuidc , uuid ); <nl> 
void odm_DIGInit ( struct odm_dm_struct * pDM_Odm ) <nl> struct adapter * adapter = pDM_Odm -> Adapter ; <nl> struct rtw_dig * pDM_DigTable = & pDM_Odm -> DM_DigTable ; <nl>  <nl> - pDM_DigTable -> CurIGValue = ( u8 ) PHY_QueryBBReg ( adapter , ODM_REG ( IGI_A , pDM_Odm ), ODM_BIT ( IGI , pDM_Odm )); <nl> + pDM_DigTable -> CurIGValue = ( u8 ) PHY_QueryBBReg ( adapter , ODM_REG_IGI_A_11N , ODM_BIT_IGI_11N ); <nl> pDM_DigTable -> RssiLowThresh = DM_DIG_THRESH_LOW ; <nl> pDM_DigTable -> RssiHighThresh = DM_DIG_THRESH_HIGH ; <nl> pDM_DigTable -> FALowThresh = DM_false_ALARM_THRESH_LOW ;
xlog_recover_add_to_trans ( <nl> " bad number of regions (% d ) in inode log format ", <nl> in_f -> ilf_size ); <nl> ASSERT ( 0 ); <nl> + free ( ptr ); <nl> return XFS_ERROR ( EIO ); <nl> } <nl> 
static void * vexpress_sysreg_config_func_get ( struct device * dev , <nl> struct device_node * node ) <nl> { <nl> struct vexpress_sysreg_config_func * config_func ; <nl> - u32 site ; <nl> + u32 site = 0 ; <nl> u32 position = 0 ; <nl> u32 dcc = 0 ; <nl> u32 func_device [ 2 ];
static int r820t_set_tv_standard ( struct r820t_priv * priv , <nl> return rc ; <nl> msleep ( 1 ); <nl> } <nl> - priv -> int_freq = if_khz ; <nl> + priv -> int_freq = if_khz * 1000 ; <nl>  <nl> /* Check if standard changed . If so , filter calibration is needed */ <nl> if ( type != priv -> type )
int __inet_inherit_port ( const struct sock * sk , struct sock * child ) <nl>  <nl> spin_lock (& head -> lock ); <nl> tb = inet_csk ( sk )-> icsk_bind_hash ; <nl> + if ( unlikely (! tb )) { <nl> + spin_unlock (& head -> lock ); <nl> + return - ENOENT ; <nl> + } <nl> if ( tb -> port != port ) { <nl> /* NOTE : using tproxy and redirecting skbs to a proxy <nl> * on a different listener port breaks the assumption
int pci_vpd_truncate ( struct pci_dev * dev , size_t size ) <nl> return - EINVAL ; <nl>  <nl> dev -> vpd -> len = size ; <nl> - dev -> vpd -> attr -> size = size ; <nl> + if ( dev -> vpd -> attr ) <nl> + dev -> vpd -> attr -> size = size ; <nl>  <nl> return 0 ; <nl> }
int drm_fb_helper_init ( struct drm_device * dev , <nl> struct drm_crtc * crtc ; <nl> int i ; <nl>  <nl> + if (! max_conn_count ) <nl> + return - EINVAL ; <nl> + <nl> fb_helper -> dev = dev ; <nl>  <nl> INIT_LIST_HEAD (& fb_helper -> kernel_fb_list );
int mac_partition ( struct parsed_partitions * state , struct block_device * bdev ) <nl> be32_to_cpu ( part -> start_block ) * ( secsize / 512 ), <nl> be32_to_cpu ( part -> block_count ) * ( secsize / 512 )); <nl>  <nl> + if (! strnicmp ( part -> type , " Linux_RAID ", 10 )) <nl> + state -> parts [ slot ]. flags = 1 ; <nl> # ifdef CONFIG_PPC_PMAC <nl> /* <nl> * If this is the first bootable partition , tell the
static void _gb_power_supplies_release ( struct gb_power_supplies * supplies ) <nl> { <nl> int i ; <nl>  <nl> + if (! supplies -> supply ) <nl> + return ; <nl> + <nl> mutex_lock (& supplies -> supplies_lock ); <nl> for ( i = 0 ; i < supplies -> supplies_count ; i ++) <nl> _gb_power_supply_release (& supplies -> supply [ i ]);
static int bnx2x_cnic_handle_cfc_del ( struct bnx2x * bp , u32 cid , <nl> union event_ring_elem * elem ) <nl> { <nl> if (! bp -> cnic_eth_dev . starting_cid || <nl> - cid < bp -> cnic_eth_dev . starting_cid ) <nl> + ( cid < bp -> cnic_eth_dev . starting_cid && <nl> + cid != bp -> cnic_eth_dev . iscsi_l2_cid )) <nl> return 1 ; <nl>  <nl> DP ( BNX2X_MSG_SP , " got delete ramrod for CNIC CID % d \ n ", cid );
static int __devinit plat_nand_probe ( struct platform_device * pdev ) <nl> struct resource * res ; <nl> int err = 0 ; <nl>  <nl> + if ( pdata -> chip . nr_chips < 1 ) { <nl> + dev_err (& pdev -> dev , " invalid number of chips specified \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> if (! res ) <nl> return - ENXIO ;
static struct task_struct * dup_task_struct ( struct task_struct * orig ) <nl>  <nl> /* One for us , one for whoever does the " release_task ()" ( usually parent ) */ <nl> atomic_set (& tsk -> usage , 2 ); <nl> + atomic_set (& tsk -> fs_excl , 0 ); <nl> return tsk ; <nl> } <nl> 
static int do_proc_dointvec_jiffies_conv ( bool * negp , unsigned long * lvalp , <nl> int write , void * data ) <nl> { <nl> if ( write ) { <nl> - if (* lvalp > LONG_MAX / HZ ) <nl> + if (* lvalp > INT_MAX / HZ ) <nl> return 1 ; <nl> * valp = * negp ? -(* lvalp * HZ ) : (* lvalp * HZ ); <nl> } else {
void rtl92e_set_key ( struct net_device * dev , u8 EntryNo , u8 KeyIndex , <nl> } <nl> } <nl> priv -> rtllib -> is_set_key = true ; <nl> - if ( EntryNo >= TOTAL_CAM_ENTRY ) <nl> + if ( EntryNo >= TOTAL_CAM_ENTRY ) { <nl> netdev_info ( dev , "% s (): Invalid CAM entry \ n ", __func__ ); <nl> + return ; <nl> + } <nl>  <nl> RT_TRACE ( COMP_SEC , <nl> "====> to rtl92e_set_key (), dev :% p , EntryNo :% d , KeyIndex :% d , KeyType :% d , MacAddr % pM \ n ",
static void bit_putcs ( struct vc_data * vc , struct fb_info * info , <nl> image . depth = 1 ; <nl>  <nl> if ( attribute ) { <nl> - buf = kmalloc ( cellsize , GFP_KERNEL ); <nl> + buf = kmalloc ( cellsize , GFP_ATOMIC ); <nl> if (! buf ) <nl> return ; <nl> }
static const struct x86_cpu_id rapl_ids [] = { <nl> RAPL_CPU ( 0x45 , rapl_defaults_core ),/* Haswell ULT */ <nl> RAPL_CPU ( 0x4C , rapl_defaults_atom ),/* Braswell */ <nl> RAPL_CPU ( 0x4A , rapl_defaults_atom ),/* Tangier */ <nl> + RAPL_CPU ( 0x56 , rapl_defaults_core ),/* Future Xeon */ <nl> RAPL_CPU ( 0x5A , rapl_defaults_atom ),/* Annidale */ <nl> {} <nl> };
omap_i2c_probe ( struct platform_device * pdev ) <nl> struct omap_i2c_dev * dev ; <nl> struct i2c_adapter * adap ; <nl> struct resource * mem , * irq , * ioarea ; <nl> - struct omap_i2c_bus_platform_data * pdata = pdev -> dev . platform_data ; <nl> + const struct omap_i2c_bus_platform_data * pdata = <nl> + pdev -> dev . platform_data ; <nl> struct device_node * node = pdev -> dev . of_node ; <nl> const struct of_device_id * match ; <nl> irq_handler_t isr ;
static int __init musb_probe ( struct platform_device * pdev ) <nl> void __iomem * base ; <nl>  <nl> iomem = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> - if (! iomem || irq == 0 ) <nl> + if (! iomem || irq <= 0 ) <nl> return - ENODEV ; <nl>  <nl> base = ioremap ( iomem -> start , resource_size ( iomem ));
static s16 * read_rds_samples ( struct cx88_core * core , u32 * N ) <nl> current_address , <nl> current_address - srch -> fifo_start , sample_count , <nl> cx_read ( MO_AUD_INTSTAT )); <nl> - <nl> - samples = kmalloc ( sizeof ( s16 )* sample_count , GFP_KERNEL ); <nl> + samples = kmalloc_array ( sample_count , sizeof (* samples ), GFP_KERNEL ); <nl> if (! samples ) <nl> return NULL ; <nl> 
static int ff_layout_async_handle_error_v3 ( struct rpc_task * task , <nl> return - NFS4ERR_RESET_TO_PNFS ; <nl> out_retry : <nl> task -> tk_status = 0 ; <nl> - rpc_restart_call ( task ); <nl> + rpc_restart_call_prepare ( task ); <nl> rpc_delay ( task , NFS_JUKEBOX_RETRY_TIME ); <nl> return - EAGAIN ; <nl> }
int hfsplus_find_cat ( struct super_block * sb , u32 cnid , <nl> return - EIO ; <nl> } <nl>  <nl> + if ( be16_to_cpu ( tmp . thread . nodeName . length ) > 255 ) { <nl> + printk ( KERN_ERR " hfs : catalog name length corrupted \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> hfsplus_cat_build_key_uni ( fd -> search_key , be32_to_cpu ( tmp . thread . parentID ), <nl> & tmp . thread . nodeName ); <nl> return hfs_brec_find ( fd );
static struct task_struct * first_tid ( struct task_struct * leader , <nl> pos = NULL ; <nl> if ( nr && nr >= get_nr_threads ( leader )) <nl> goto out ; <nl> + /* It could be unhashed before we take rcu lock */ <nl> + if (! pid_alive ( leader )) <nl> + goto out ; <nl>  <nl> /* If we haven ' t found our starting place yet start <nl> * with the leader and walk nr threads forward .
struct fimc_fmt * find_format ( struct v4l2_format * f , unsigned int mask ) <nl>  <nl> for ( i = 0 ; i < ARRAY_SIZE ( fimc_formats ); ++ i ) { <nl> fmt = & fimc_formats [ i ]; <nl> - if ( fmt -> fourcc == f -> fmt . pix . pixelformat && <nl> + if ( fmt -> fourcc == f -> fmt . pix_mp . pixelformat && <nl> ( fmt -> flags & mask )) <nl> break ; <nl> }
static bool new_idmap_permitted ( const struct file * file , <nl> u32 id = new_map -> extent [ 0 ]. lower_first ; <nl> if ( cap_setid == CAP_SETUID ) { <nl> kuid_t uid = make_kuid ( ns -> parent , id ); <nl> - if ( uid_eq ( uid , file -> f_cred -> fsuid )) <nl> + if ( uid_eq ( uid , file -> f_cred -> euid )) <nl> return true ; <nl> } <nl> }
static int kvm_vcpu_ioctl_x86_set_vcpu_events ( struct kvm_vcpu * vcpu , <nl> | KVM_VCPUEVENT_VALID_SMM )) <nl> return - EINVAL ; <nl>  <nl> + if ( events -> exception . injected && <nl> + ( events -> exception . nr > 31 || events -> exception . nr == NMI_VECTOR )) <nl> + return - EINVAL ; <nl> + <nl> process_nmi ( vcpu ); <nl> vcpu -> arch . exception . pending = events -> exception . injected ; <nl> vcpu -> arch . exception . nr = events -> exception . nr ;
static ssize_t tcmu_set_configfs_dev_params ( struct se_device * dev , <nl> default : <nl> break ; <nl> } <nl> + <nl> + if ( ret ) <nl> + break ; <nl> } <nl>  <nl> kfree ( orig );
dev_config ( struct file * fd , const char __user * buf , size_t len , loff_t * ptr ) <nl>  <nl> spin_lock_irq (& dev -> lock ); <nl> value = - EINVAL ; <nl> - if ( dev -> buf ) <nl> + if ( dev -> buf ) { <nl> + kfree ( kbuf ); <nl> goto fail ; <nl> + } <nl> dev -> buf = kbuf ; <nl>  <nl> /* full or low speed config */
static int __devinit corgipm_init ( void ) <nl> { <nl> int ret ; <nl>  <nl> + if (! machine_is_corgi () && ! machine_is_shepherd () <nl> + && ! machine_is_husky ()) <nl> + return - ENODEV ; <nl> + <nl> corgipm_device = platform_device_alloc (" sharpsl - pm ", - 1 ); <nl> if (! corgipm_device ) <nl> return - ENOMEM ;
static int handshake_on_error_set_halt ( struct ehci_hcd * ehci , void __iomem * ptr , <nl> if ( error ) { <nl> ehci_halt ( ehci ); <nl> ehci_to_hcd ( ehci )-> state = HC_STATE_HALT ; <nl> - ehci_err ( ehci , " force halt ; handhake % p % 08x % 08x -> % d \ n ", <nl> + ehci_err ( ehci , " force halt ; handshake % p % 08x % 08x -> % d \ n ", <nl> ptr , mask , done , error ); <nl> } <nl> 
static int cp2112_gpio_direction_input ( struct gpio_chip * chip , unsigned offset ) <nl>  <nl> exit : <nl> mutex_unlock (& dev -> lock ); <nl> - return ret <= 0 ? ret : - EIO ; <nl> + return ret < 0 ? ret : - EIO ; <nl> } <nl>  <nl> static void cp2112_gpio_set ( struct gpio_chip * chip , unsigned offset , int value )
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> spin_unlock_bh (& g_cdev . lock ); <nl> if ( pd == NULL ) <nl> return - ENXIO ; <nl> + if (!( pd -> state & OZ_PD_S_CONNECTED )) <nl> + return - EAGAIN ; <nl> eb = & pd -> elt_buff ; <nl> ei = oz_elt_info_alloc ( eb ); <nl> if ( ei == NULL ) {
static int atc_control ( struct dma_chan * chan , enum dma_ctrl_cmd cmd , <nl> list_splice_init (& atchan -> queue , & list ); <nl> list_splice_init (& atchan -> active_list , & list ); <nl>  <nl> - spin_unlock_bh (& atchan -> lock ); <nl> - <nl> /* Flush all pending and queued descriptors */ <nl> list_for_each_entry_safe ( desc , _desc , & list , desc_node ) <nl> atc_chain_complete ( atchan , desc ); <nl>  <nl> + spin_unlock_bh (& atchan -> lock ); <nl> + <nl> return 0 ; <nl> } <nl> 
void __init s3c6400_init_irq ( void ) <nl> s3c64xx_init_irq (~ 0 & ~( 0xf << 5 ), ~ 0 ); <nl> } <nl>  <nl> - struct sysdev_class s3c6400_sysclass = { <nl> + static struct sysdev_class s3c6400_sysclass = { <nl> . name = " s3c6400 - core ", <nl> }; <nl> 
int i915_gem_stolen_setup_compression ( struct drm_device * dev , int size , int fb_c <nl> if (! drm_mm_initialized (& dev_priv -> mm . stolen )) <nl> return - ENODEV ; <nl>  <nl> - if ( size < dev_priv -> fbc . uncompressed_size ) <nl> + if ( size <= dev_priv -> fbc . uncompressed_size ) <nl> return 0 ; <nl>  <nl> /* Release any current block */
MPI mpi_read_raw_data ( const void * xbuffer , size_t nbytes ) <nl> mpi_limb_t a ; <nl> MPI val = NULL ; <nl>  <nl> - while ( nbytes >= 0 && buffer [ 0 ] == 0 ) { <nl> + while ( nbytes > 0 && buffer [ 0 ] == 0 ) { <nl> buffer ++; <nl> nbytes --; <nl> }
int snd_timer_new ( struct snd_card * card , char * id , struct snd_timer_id * tid , <nl> timer -> tmr_subdevice = tid -> subdevice ; <nl> if ( id ) <nl> strlcpy ( timer -> id , id , sizeof ( timer -> id )); <nl> + timer -> sticks = 1 ; <nl> INIT_LIST_HEAD (& timer -> device_list ); <nl> INIT_LIST_HEAD (& timer -> open_list_head ); <nl> INIT_LIST_HEAD (& timer -> active_list_head );
static int fn_trie_insert ( struct fib_table * tb , struct fib_config * cfg ) <nl> struct fib_info * fi_drop ; <nl> u8 state ; <nl>  <nl> + if ( fi -> fib_treeref > 1 ) <nl> + goto out ; <nl> + <nl> err = - ENOBUFS ; <nl> new_fa = kmem_cache_alloc ( fn_alias_kmem , GFP_KERNEL ); <nl> if ( new_fa == NULL )
u32 mp_query_psd ( struct adapter * pAdapter , u8 * data ) <nl> sscanf ( data , " pts =% d , start =% d , stop =% d ", & psd_pts , & psd_start , & psd_stop ); <nl> } <nl>  <nl> - _rtw_memset ( data , '\ 0 ', sizeof ( data )); <nl> + _rtw_memset ( data , '\ 0 ', sizeof (* data )); <nl>  <nl> i = psd_start ; <nl> while ( i < psd_stop ) {
static int serio_raw_connect ( struct serio * serio , struct serio_driver * drv ) <nl>  <nl> serio_raw -> dev . minor = PSMOUSE_MINOR ; <nl> serio_raw -> dev . name = serio_raw -> name ; <nl> + serio_raw -> dev . dev = & serio -> dev ; <nl> serio_raw -> dev . fops = & serio_raw_fops ; <nl>  <nl> err = misc_register (& serio_raw -> dev );
x86_emulate_insn ( struct x86_emulate_ctxt * ctxt ) <nl> goto done ; <nl> } <nl>  <nl> + if (( c -> d & SrcMask ) == SrcMemFAddr && c -> src . type != OP_MEM ) { <nl> + emulate_ud ( ctxt ); <nl> + goto done ; <nl> + } <nl> + <nl> /* Privileged instruction can be executed only in CPL = 0 */ <nl> if (( c -> d & Priv ) && ops -> cpl ( ctxt -> vcpu )) { <nl> emulate_gp ( ctxt , 0 );
static int newseg ( struct ipc_namespace * ns , struct ipc_params * params ) <nl> if ( size < SHMMIN || size > ns -> shm_ctlmax ) <nl> return - EINVAL ; <nl>  <nl> + if ( numpages << PAGE_SHIFT < size ) <nl> + return - ENOSPC ; <nl> + <nl> if ( ns -> shm_tot + numpages < ns -> shm_tot || <nl> ns -> shm_tot + numpages > ns -> shm_ctlall ) <nl> return - ENOSPC ;
int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl> + IEEE80211_ENCRYPT_HEADROOM ; <nl> ndev -> needed_tailroom = IEEE80211_ENCRYPT_TAILROOM ; <nl>  <nl> + ret = dev_alloc_name ( ndev , ndev -> name ); <nl> + if ( ret < 0 ) <nl> + goto fail ; <nl> + <nl> ieee80211_assign_perm_addr ( local , ndev , type ); <nl> memcpy ( ndev -> dev_addr , ndev -> perm_addr , ETH_ALEN ); <nl> SET_NETDEV_DEV ( ndev , wiphy_dev ( local -> hw . wiphy ));
static int __devinit snd_hdspm_create_hwdep ( struct snd_card * card , <nl>  <nl> hw -> ops . open = snd_hdspm_hwdep_dummy_op ; <nl> hw -> ops . ioctl = snd_hdspm_hwdep_ioctl ; <nl> + hw -> ops . ioctl_compat = snd_hdspm_hwdep_ioctl ; <nl> hw -> ops . release = snd_hdspm_hwdep_dummy_op ; <nl>  <nl> return 0 ;
static inline struct page * pte_alloc_one ( struct mm_struct * mm , <nl> struct page * pte ; <nl>  <nl> pte = alloc_pages ( GFP_KERNEL | __GFP_REPEAT , PTE_ORDER ); <nl> - if ( pte ) { <nl> - clear_highpage ( pte ); <nl> - pgtable_page_ctor ( pte ); <nl> + if (! pte ) <nl> + return NULL ; <nl> + clear_highpage ( pte ); <nl> + if (! pgtable_page_ctor ( pte )) { <nl> + __free_page ( pte ); <nl> + return NULL ; <nl> } <nl> return pte ; <nl> }
int usb_disable_lpm ( struct usb_device * udev ) <nl> return 0 ; <nl>  <nl> udev -> lpm_disable_count ++; <nl> - if (( udev -> u1_params . timeout == 0 && udev -> u1_params . timeout == 0 )) <nl> + if (( udev -> u1_params . timeout == 0 && udev -> u2_params . timeout == 0 )) <nl> return 0 ; <nl>  <nl> /* If LPM is enabled , attempt to disable it . */
struct radeon_device { <nl> struct radeon_object * fbdev_robj ; <nl> struct radeon_framebuffer * fbdev_rfb ; <nl> /* Register mmio */ <nl> - unsigned long rmmio_base ; <nl> - unsigned long rmmio_size ; <nl> + resource_size_t rmmio_base ; <nl> + resource_size_t rmmio_size ; <nl> void * rmmio ; <nl> radeon_rreg_t mm_rreg ; <nl> radeon_wreg_t mm_wreg ;
static struct priority_group * parse_priority_group ( struct arg_set * as , <nl>  <nl> if ( as -> argc < nr_params ) { <nl> ti -> error = " not enough path parameters "; <nl> + r = - EINVAL ; <nl> goto bad ; <nl> } <nl> 
int xhci_urb_enqueue ( struct usb_hcd * hcd , struct urb * urb , gfp_t mem_flags ) <nl> if (! xhci -> devs || ! xhci -> devs [ slot_id ]) { <nl> if (! in_interrupt ()) <nl> dev_warn (& urb -> dev -> dev , " WARN : urb submitted for dev with no Slot ID \ n "); <nl> - return - EINVAL ; <nl> + ret = - EINVAL ; <nl> + goto exit ; <nl> } <nl> if (! test_bit ( HCD_FLAG_HW_ACCESSIBLE , & hcd -> flags )) { <nl> if (! in_interrupt ())
static int clcdfb_of_get_mode ( struct device * dev , struct device_node * endpoint , <nl>  <nl> len = clcdfb_snprintf_mode ( NULL , 0 , mode ); <nl> name = devm_kzalloc ( dev , len + 1 , GFP_KERNEL ); <nl> + if (! name ) <nl> + return - ENOMEM ; <nl> + <nl> clcdfb_snprintf_mode ( name , len + 1 , mode ); <nl> mode -> name = name ; <nl> 
static irqreturn_t skge_intr ( int irq , void * dev_id ) <nl>  <nl> if ( status & IS_HW_ERR ) <nl> skge_error_irq ( hw ); <nl> - <nl> + out : <nl> skge_write32 ( hw , B0_IMSK , hw -> intr_mask ); <nl> skge_read32 ( hw , B0_IMSK ); <nl> - out : <nl> spin_unlock (& hw -> hw_lock ); <nl>  <nl> return IRQ_RETVAL ( handled );
static int __init twl4030_pwrbutton_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> free_irq : <nl> - free_irq ( irq , NULL ); <nl> + free_irq ( irq , pwr ); <nl> free_input_dev : <nl> input_free_device ( pwr ); <nl> return err ;
int cmd_top ( int argc , const char ** argv , const char * prefix ) <nl> event_id [ 0 ] = 0 ; <nl> } <nl>  <nl> + if ( delay_secs < 1 ) <nl> + delay_secs = 1 ; <nl> + <nl> for ( counter = 0 ; counter < nr_counters ; counter ++) { <nl> if ( event_count [ counter ]) <nl> continue ;
i915_gem_execbuffer ( struct drm_device * dev , void * data , <nl> ( int ) args -> buffers_ptr , args -> buffer_count , args -> batch_len ); <nl> # endif <nl>  <nl> + if ( args -> buffer_count < 1 ) { <nl> + DRM_ERROR (" execbuf with % d buffers \ n ", args -> buffer_count ); <nl> + return - EINVAL ; <nl> + } <nl> /* Copy in the exec list from userland */ <nl> exec_list = drm_calloc ( sizeof (* exec_list ), args -> buffer_count , <nl> DRM_MEM_DRIVER );
void ieee80211_check_fast_xmit ( struct sta_info * sta ) <nl> goto out ; <nl>  <nl> /* fast - xmit doesn ' t handle fragmentation at all */ <nl> - if ( local -> hw . wiphy -> frag_threshold != ( u32 )- 1 ) <nl> + if ( local -> hw . wiphy -> frag_threshold != ( u32 )- 1 && <nl> + ! local -> ops -> set_frag_threshold ) <nl> goto out ; <nl>  <nl> rcu_read_lock ();
static int gb_interface_resume ( struct device * dev ) <nl> return ret ; <nl> } <nl>  <nl> + ret = gb_timesync_schedule_synchronous ( intf ); <nl> + if ( ret ) { <nl> + dev_err ( dev , " failed to synchronize FrameTime : % d \ n ", ret ); <nl> + return ret ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
befs_find_brun_dblindirect ( struct super_block * sb , <nl> struct buffer_head * indir_block ; <nl> befs_block_run indir_run ; <nl> befs_disk_inode_addr * iaddr_array ; <nl> - struct befs_sb_info * befs_sb = BEFS_SB ( sb ); <nl>  <nl> befs_blocknr_t indir_start_blk = <nl> - data -> max_indirect_range >> befs_sb -> block_shift ; <nl> + data -> max_indirect_range >> BEFS_SB ( sb )-> block_shift ; <nl>  <nl> off_t dbl_indir_off = blockno - indir_start_blk ; <nl> 
static int s3c_fb_check_var ( struct fb_var_screeninfo * var , <nl>  <nl> default : <nl> dev_err ( sfb -> dev , " invalid bpp \ n "); <nl> + return - EINVAL ; <nl> } <nl>  <nl> dev_dbg ( sfb -> dev , "% s : verified parameters \ n ", __func__ );
static struct of_device * __init scan_one_device ( struct device_node * dp , <nl> if (! parent ) <nl> strcpy ( op -> dev . bus_id , " root "); <nl> else <nl> - strcpy ( op -> dev . bus_id , dp -> path_component_name ); <nl> + sprintf ( op -> dev . bus_id , "% s @% 08x ", dp -> name , dp -> node ); <nl>  <nl> if ( of_device_register ( op )) { <nl> printk ("% s : Could not register of device .\ n ",
int netvsc_send ( struct hv_device * device , <nl> if (! net_device ) <nl> return - ENODEV ; <nl>  <nl> + /* We may race with netvsc_connect_vsp ()/ netvsc_init_buf () and get <nl> + * here before the negotiation with the host is finished and <nl> + * send_section_map may not be allocated yet . <nl> + */ <nl> + if (! net_device -> send_section_map ) <nl> + return - EAGAIN ; <nl> + <nl> out_channel = net_device -> chn_table [ q_idx ]; <nl>  <nl> packet -> send_buf_index = NETVSC_INVALID_INDEX ;
unsigned fuse_file_poll ( struct file * file , poll_table * wait ) <nl>  <nl> req = fuse_get_req ( fc ); <nl> if ( IS_ERR ( req )) <nl> - return PTR_ERR ( req ); <nl> + return POLLERR ; <nl>  <nl> req -> in . h . opcode = FUSE_POLL ; <nl> req -> in . h . nodeid = ff -> nodeid ;
static void <nl> activate_substream ( struct snd_usb_caiaqdev * dev , <nl> struct snd_pcm_substream * sub ) <nl> { <nl> + spin_lock (& dev -> spinlock ); <nl> + <nl> if ( sub -> stream == SNDRV_PCM_STREAM_PLAYBACK ) <nl> dev -> sub_playback [ sub -> number ] = sub ; <nl> else <nl> dev -> sub_capture [ sub -> number ] = sub ; <nl> + <nl> + spin_unlock (& dev -> spinlock ); <nl> } <nl>  <nl> static void
extern struct mlog_bits mlog_and_bits , mlog_not_bits ; <nl> # define mlog_errno ( st ) do { \ <nl> int _st = ( st ); \ <nl> if ( _st != - ERESTARTSYS && _st != - EINTR && \ <nl> - _st != AOP_TRUNCATED_PAGE ) \ <nl> + _st != AOP_TRUNCATED_PAGE && _st != - ENOSPC ) \ <nl> mlog ( ML_ERROR , " status = % lld \ n ", ( long long ) _st ); \ <nl> } while ( 0 ) <nl> 
static int ds2760_battery_remove ( struct platform_device * pdev ) <nl> & di -> set_charged_work ); <nl> destroy_workqueue ( di -> monitor_wqueue ); <nl> power_supply_unregister (& di -> bat ); <nl> + kfree ( di ); <nl>  <nl> return 0 ; <nl> }
static int perf_top_config ( const char * var , const char * value , void * cb ) <nl>  <nl> if (! strcmp ( var , " top . call - graph ")) <nl> return record_parse_callchain ( value , & top -> record_opts ); <nl> + if (! strcmp ( var , " top . children ")) { <nl> + symbol_conf . cumulate_callchain = perf_config_bool ( var , value ); <nl> + return 0 ; <nl> + } <nl>  <nl> return perf_default_config ( var , value , cb ); <nl> }
static void ieee80211_do_stop ( struct ieee80211_sub_if_data * sdata , <nl> rcu_barrier (); <nl> sta_info_flush_cleanup ( sdata ); <nl>  <nl> - skb_queue_purge (& sdata -> skb_queue ); <nl> - <nl> /* <nl> * Free all remaining keys , there shouldn ' t be any , <nl> * except maybe in WDS mode ? <nl> */ <nl> ieee80211_free_keys ( sdata ); <nl>  <nl> + /* fall through */ <nl> + case NL80211_IFTYPE_AP : <nl> + skb_queue_purge (& sdata -> skb_queue ); <nl> + <nl> drv_remove_interface_debugfs ( local , sdata ); <nl>  <nl> if ( going_down )
s32 fm10k_disable_queues_generic ( struct fm10k_hw * hw , u16 q_cnt ) <nl> /* clear tx_ready to prevent any false hits for reset */ <nl> hw -> mac . tx_ready = false ; <nl>  <nl> + if ( FM10K_REMOVED ( hw -> hw_addr )) <nl> + return 0 ; <nl> + <nl> /* clear the enable bit for all rings */ <nl> for ( i = 0 ; i < q_cnt ; i ++) { <nl> reg = fm10k_read_reg ( hw , FM10K_TXDCTL ( i ));
struct mmc_host * mmc_alloc_host ( int extra , struct device * dev ) <nl>  <nl> if ( mmc_gpio_alloc ( host )) { <nl> put_device (& host -> class_dev ); <nl> + ida_simple_remove (& mmc_host_ida , host -> index ); <nl> + kfree ( host ); <nl> return NULL ; <nl> } <nl> 
static void acm_softint ( struct work_struct * work ) <nl> static void acm_waker ( struct work_struct * waker ) <nl> { <nl> struct acm * acm = container_of ( waker , struct acm , waker ); <nl> - long flags ; <nl> + unsigned long flags ; <nl> int rv ; <nl>  <nl> rv = usb_autopm_get_interface ( acm -> control );
static noinline size_t if_nlmsg_size ( const struct net_device * dev , <nl> + rtnl_link_get_af_size ( dev , ext_filter_mask ) /* IFLA_AF_SPEC */ <nl> + nla_total_size ( MAX_PHYS_ITEM_ID_LEN ) /* IFLA_PHYS_PORT_ID */ <nl> + nla_total_size ( MAX_PHYS_ITEM_ID_LEN ) /* IFLA_PHYS_SWITCH_ID */ <nl> + + nla_total_size ( IFNAMSIZ ) /* IFLA_PHYS_PORT_NAME */ <nl> + nla_total_size ( 1 ); /* IFLA_PROTO_DOWN */ <nl>  <nl> }
static const struct usb_device_id usb_quirk_list [] = { <nl> /* Creative SB Audigy 2 NX */ <nl> { USB_DEVICE ( 0x041e , 0x3020 ), . driver_info = USB_QUIRK_RESET_RESUME }, <nl>  <nl> + /* Microsoft LifeCam - VX700 v2 . 0 */ <nl> + { USB_DEVICE ( 0x045e , 0x0770 ), . driver_info = USB_QUIRK_RESET_RESUME }, <nl> + <nl> /* Logitech Quickcam Fusion */ <nl> { USB_DEVICE ( 0x046d , 0x08c1 ), . driver_info = USB_QUIRK_RESET_RESUME }, <nl> 
static unsigned long si5351_clkout_recalc_rate ( struct clk_hw * hw , <nl> unsigned char reg ; <nl> unsigned char rdiv ; <nl>  <nl> - if ( hwdata -> num > 5 ) <nl> + if ( hwdata -> num <= 5 ) <nl> reg = si5351_msynth_params_address ( hwdata -> num ) + 2 ; <nl> else <nl> reg = SI5351_CLK6_7_OUTPUT_DIVIDER ;
static bool have_cpu_die ( void ) <nl> # ifdef CONFIG_HOTPLUG_CPU <nl> int any_cpu = raw_smp_processor_id (); <nl>  <nl> - if ( cpu_ops [ any_cpu ]-> cpu_die ) <nl> + if ( cpu_ops [ any_cpu ] && cpu_ops [ any_cpu ]-> cpu_die ) <nl> return true ; <nl> # endif <nl> return false ;
static int eseqiv_givencrypt ( struct skcipher_givcrypt_request * req ) <nl> if ( err ) <nl> goto out ; <nl>  <nl> - eseqiv_complete2 ( req ); <nl> + if ( giv != req -> giv ) <nl> + eseqiv_complete2 ( req ); <nl>  <nl> out : <nl> return err ;
static void * ixgbe_fwd_add ( struct net_device * pdev , struct net_device * vdev ) <nl> ( adapter -> num_rx_pools > IXGBE_MAX_MACVLANS )) <nl> return ERR_PTR (- EBUSY ); <nl>  <nl> - fwd_adapter = kcalloc ( 1 , sizeof ( struct ixgbe_fwd_adapter ), GFP_KERNEL ); <nl> + fwd_adapter = kzalloc ( sizeof (* fwd_adapter ), GFP_KERNEL ); <nl> if (! fwd_adapter ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
int __devinit snd_emu10k1_mixer ( struct snd_emu10k1 * emu , <nl> if ( emu -> ac97 -> id == AC97_ID_STAC9758 ) { <nl> emu -> rear_ac97 = 1 ; <nl> snd_emu10k1_ptr_write ( emu , AC97SLOT , 0 , AC97SLOT_CNTR | AC97SLOT_LFE | AC97SLOT_REAR_LEFT | AC97SLOT_REAR_RIGHT ); <nl> + snd_ac97_write_cache ( emu -> ac97 , AC97_HEADPHONE , 0x0202 ); <nl> } <nl> /* remove unused AC97 controls */ <nl> snd_ac97_write_cache ( emu -> ac97 , AC97_SURROUND_MASTER , 0x0202 );
static int noinline dirty_and_release_pages ( struct btrfs_trans_handle * trans , <nl> */ <nl> inline_size = end_pos ; <nl> if ( isize >= BTRFS_MAX_INLINE_DATA_SIZE ( root ) || <nl> - inline_size > 32768 || <nl> + inline_size > 8192 || <nl> inline_size >= BTRFS_MAX_INLINE_DATA_SIZE ( root )) { <nl> u64 last_end ; <nl> u64 existing_delalloc = 0 ;
static int rr_init_one ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> return 0 ; <nl>  <nl> out : <nl> + if ( rrpriv -> evt_ring ) <nl> + pci_free_consistent ( pdev , EVT_RING_SIZE , rrpriv -> evt_ring , <nl> + rrpriv -> evt_ring_dma ); <nl> if ( rrpriv -> rx_ring ) <nl> pci_free_consistent ( pdev , RX_TOTAL_SIZE , rrpriv -> rx_ring , <nl> rrpriv -> rx_ring_dma );
restart : <nl> * with multiple processes reclaiming pages , the total <nl> * freeing target can get unreasonably large . <nl> */ <nl> - if ( nr_reclaimed >= nr_to_reclaim && priority < DEF_PRIORITY ) <nl> + if ( nr_reclaimed >= nr_to_reclaim ) <nl> + nr_to_reclaim = 0 ; <nl> + else <nl> + nr_to_reclaim -= nr_reclaimed ; <nl> + <nl> + if (! nr_to_reclaim && priority < DEF_PRIORITY ) <nl> break ; <nl> } <nl> blk_finish_plug (& plug );
static int aspeed_gpio_set_config ( struct gpio_chip * chip , unsigned int offset , <nl> param == PIN_CONFIG_BIAS_PULL_DOWN || <nl> param == PIN_CONFIG_DRIVE_STRENGTH ) <nl> return pinctrl_gpio_set_config ( offset , config ); <nl> + else if ( param == PIN_CONFIG_DRIVE_OPEN_DRAIN || <nl> + param == PIN_CONFIG_DRIVE_OPEN_SOURCE ) <nl> + /* Return - ENOTSUPP to trigger emulation , as per datasheet */ <nl> + return - ENOTSUPP ; <nl>  <nl> return - ENOTSUPP ; <nl> }
isdn_net_setcfg ( isdn_net_ioctl_cfg * cfg ) <nl> char * c , <nl> * e ; <nl>  <nl> + if ( strnlen ( cfg -> drvid , sizeof ( cfg -> drvid )) == <nl> + sizeof ( cfg -> drvid )) <nl> + return - EINVAL ; <nl> drvidx = - 1 ; <nl> chidx = - 1 ; <nl> strcpy ( drvid , cfg -> drvid );
static struct pernet_operations __net_initdata proc_net_ns_ops = { <nl>  <nl> int __init proc_net_init ( void ) <nl> { <nl> - proc_symlink (" net ", NULL , " self / net "); <nl> + proc_symlink (" net ", NULL , " thread - self / net "); <nl>  <nl> return register_pernet_subsys (& proc_net_ns_ops ); <nl> }
lpfc_send_els_event ( struct lpfc_vport * vport , <nl> sizeof ( struct lpfc_name )); <nl> break ; <nl> default : <nl> + kfree ( els_data ); <nl> return ; <nl> } <nl> memcpy ( els_data -> wwpn , & ndlp -> nlp_portname , sizeof ( struct lpfc_name ));
static void k8_map_sysaddr_to_csrow ( struct mem_ctl_info * mci , <nl> * different from the node that detected the error . <nl> */ <nl> src_mci = find_mc_by_sys_addr ( mci , SystemAddress ); <nl> - if ( src_mci ) { <nl> + if (! src_mci ) { <nl> amd64_mc_printk ( mci , KERN_ERR , <nl> " failed to map error address 0x % lx to a node \ n ", <nl> ( unsigned long ) SystemAddress );
void __devinit pci_read_bridge_bases ( struct pci_bus * child ) <nl> struct resource * res ; <nl> int i ; <nl>  <nl> - if (! child -> parent ) /* It ' s a host bus , nothing to read */ <nl> + if ( pci_is_root_bus ( child )) /* It ' s a host bus , nothing to read */ <nl> return ; <nl>  <nl> if ( dev -> transparent ) {
static int __init bm2835_mmal_init ( void ) <nl> num_cameras = MAX_BCM2835_CAMERAS ; <nl>  <nl> for ( camera = 0 ; camera < num_cameras ; camera ++) { <nl> - dev = kzalloc ( sizeof ( struct bm2835_mmal_dev ), GFP_KERNEL ); <nl> + dev = kzalloc ( sizeof (* dev ), GFP_KERNEL ); <nl> if (! dev ) { <nl> ret = - ENOMEM ; <nl> goto cleanup_gdev ;
static u32 slic_card_locate ( struct adapter * adapter ) <nl> if (! physcard ) { <nl> /* no structure allocated for this physical card yet */ <nl> physcard = kzalloc ( sizeof ( struct physcard ), GFP_ATOMIC ); <nl> - if (! physcard ) <nl> + if (! physcard ) { <nl> + if ( card_hostid == SLIC_HOSTID_DEFAULT ) <nl> + kfree ( card ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> physcard -> next = slic_global . phys_card ; <nl> slic_global . phys_card = physcard ;
int cfg80211_wext_siwscan ( struct net_device * dev , <nl> wext_freq_not_found : ; <nl> } <nl> } <nl> + /* No channels found ? */ <nl> + if (! i ) { <nl> + err = - EINVAL ; <nl> + goto out ; <nl> + } <nl>  <nl> /* Set real number of channels specified in creq -> channels [] */ <nl> creq -> n_channels = i ;
struct ttm_tt * ttm_tt_create ( struct ttm_bo_device * bdev , unsigned long size , <nl> ttm -> dummy_read_page = dummy_read_page ; <nl>  <nl> ttm_tt_alloc_page_directory ( ttm ); <nl> - if (! ttm -> pages ) { <nl> + if (! ttm -> pages || ! ttm -> dma_address ) { <nl> ttm_tt_destroy ( ttm ); <nl> printk ( KERN_ERR TTM_PFX " Failed allocating page table \ n "); <nl> return NULL ;
int perf_evlist__parse_mmap_pages ( const struct option * opt , const char * str , <nl> unsigned long max = UINT_MAX ; <nl> long pages ; <nl>  <nl> - if ( max < SIZE_MAX / page_size ) <nl> + if ( max > SIZE_MAX / page_size ) <nl> max = SIZE_MAX / page_size ; <nl>  <nl> pages = parse_pages_arg ( str , 1 , max );
void disable_lapic_nmi_watchdog ( void ) <nl> return ; <nl>  <nl> on_each_cpu ( stop_apic_nmi_watchdog , NULL , 0 , 1 ); <nl> - wd_ops -> unreserve (); <nl> + <nl> + if ( wd_ops ) <nl> + wd_ops -> unreserve (); <nl>  <nl> BUG_ON ( atomic_read (& nmi_active ) != 0 ); <nl> }
static int proc_thermal_add ( struct device * dev , <nl> int ret ; <nl>  <nl> adev = ACPI_COMPANION ( dev ); <nl> + if (! adev ) <nl> + return - ENODEV ; <nl>  <nl> status = acpi_evaluate_object ( adev -> handle , " PPCC ", NULL , & buf ); <nl> if ( ACPI_FAILURE ( status ))
void ath9k_ps_wakeup ( struct ath_softc * sc ) <nl> spin_lock (& common -> cc_lock ); <nl> ath_hw_cycle_counters_update ( common ); <nl> memset (& common -> cc_survey , 0 , sizeof ( common -> cc_survey )); <nl> + memset (& common -> cc_ani , 0 , sizeof ( common -> cc_ani )); <nl> spin_unlock (& common -> cc_lock ); <nl> } <nl> 
static int ext4_mb_init_per_dev_proc ( struct super_block * sb ) <nl> struct proc_dir_entry * proc ; <nl> char devname [ 64 ]; <nl>  <nl> + if ( proc_root_ext4 == NULL ) { <nl> + sbi -> s_mb_proc = NULL ; <nl> + return - EINVAL ; <nl> + } <nl> bdevname ( sb -> s_bdev , devname ); <nl> sbi -> s_mb_proc = proc_mkdir ( devname , proc_root_ext4 ); <nl> 
__xfs_printk ( <nl> const struct xfs_mount * mp , <nl> struct va_format * vaf ) <nl> { <nl> - if ( mp && mp -> m_fsname ) <nl> + if ( mp && mp -> m_fsname ) { <nl> printk ("% sXFS (% s ): % pV \ n ", level , mp -> m_fsname , vaf ); <nl> + return ; <nl> + } <nl> printk ("% sXFS : % pV \ n ", level , vaf ); <nl> } <nl> 
set_v4l_control ( struct inode * inode , <nl>  <nl> /* ----------------------------------------------------------------- */ <nl>  <nl> - const static unsigned int palette2pixelformat [] = { <nl> + static const unsigned int palette2pixelformat [] = { <nl> [ VIDEO_PALETTE_GREY ] = V4L2_PIX_FMT_GREY , <nl> [ VIDEO_PALETTE_RGB555 ] = V4L2_PIX_FMT_RGB555 , <nl> [ VIDEO_PALETTE_RGB565 ] = V4L2_PIX_FMT_RGB565 ,
static void ixgbe_watchdog_link_is_up ( struct ixgbe_adapter * adapter ) <nl> ( flow_tx ? " TX " : " None ")))); <nl>  <nl> netif_carrier_on ( netdev ); <nl> -# ifdef HAVE_IPLINK_VF_CONFIG <nl> ixgbe_check_vf_rate_limit ( adapter ); <nl> -# endif /* HAVE_IPLINK_VF_CONFIG */ <nl> } <nl>  <nl> /**
static int rcar_vin_videobuf_setup ( struct vb2_queue * vq , <nl> unsigned int bytes_per_line ; <nl> int ret ; <nl>  <nl> + if ( fmt -> fmt . pix . sizeimage < icd -> sizeimage ) <nl> + return - EINVAL ; <nl> + <nl> xlate = soc_camera_xlate_by_fourcc ( icd , <nl> fmt -> fmt . pix . pixelformat ); <nl> if (! xlate )
static int be_get_config ( struct be_adapter * adapter ) <nl> if ( status ) <nl> return status ; <nl>  <nl> - /* primary mac needs 1 pmac entry */ <nl> - adapter -> pmac_id = kcalloc ( be_max_uc ( adapter ) + 1 , sizeof ( u32 ), <nl> - GFP_KERNEL ); <nl> + adapter -> pmac_id = kcalloc ( be_max_uc ( adapter ), <nl> + sizeof (* adapter -> pmac_id ), GFP_KERNEL ); <nl> if (! adapter -> pmac_id ) <nl> return - ENOMEM ; <nl> 
long drm_ioctl ( struct file * filp , <nl> usize = asize = _IOC_SIZE ( cmd ); <nl> if ( drv_size > asize ) <nl> asize = drv_size ; <nl> + cmd = ioctl -> cmd_drv ; <nl> } <nl> else if (( nr >= DRM_COMMAND_END ) || ( nr < DRM_COMMAND_BASE )) { <nl> ioctl = & drm_ioctls [ nr ];
static int e1000_change_mtu ( struct net_device * netdev , int new_mtu ) <nl> struct e1000_adapter * adapter = netdev_priv ( netdev ); <nl> int max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN ; <nl>  <nl> - if (( max_frame < ETH_ZLEN + ETH_FCS_LEN ) || <nl> + if (( new_mtu < ETH_ZLEN + ETH_FCS_LEN + VLAN_HLEN ) || <nl> ( max_frame > MAX_JUMBO_FRAME_SIZE )) { <nl> e_err (" Invalid MTU setting \ n "); <nl> return - EINVAL ;
ieee80211_tx_h_rate_ctrl ( struct ieee80211_tx_data * tx ) <nl> IEEE80211_TX_RC_USE_RTS_CTS ; <nl>  <nl> /* RC is busted */ <nl> - if ( WARN_ON ( info -> control . rates [ i ]. idx >= <nl> - sband -> n_bitrates )) { <nl> + if ( WARN_ON_ONCE ( info -> control . rates [ i ]. idx >= <nl> + sband -> n_bitrates )) { <nl> info -> control . rates [ i ]. idx = - 1 ; <nl> continue ; <nl> }
static void intel_sanitize_crtc ( struct intel_crtc * crtc ) <nl> * ... */ <nl> plane = crtc -> plane ; <nl> crtc -> plane = ! plane ; <nl> + crtc -> primary_enabled = true ; <nl> dev_priv -> display . crtc_disable (& crtc -> base ); <nl> crtc -> plane = plane ; <nl> 
static inline void calculate_imbalance ( struct lb_env * env , struct sd_lb_stats * s <nl> * max load less than avg load ( as we skip the groups at or below <nl> * its cpu_power , while calculating max_load ..) <nl> */ <nl> - if ( busiest -> avg_load < sds -> avg_load ) { <nl> + if ( busiest -> avg_load <= sds -> avg_load || <nl> + local -> avg_load >= sds -> avg_load ) { <nl> env -> imbalance = 0 ; <nl> return fix_small_imbalance ( env , sds ); <nl> }
static int stripe_ctr ( struct dm_target * ti , unsigned int argc , char ** argv ) <nl> sc -> stripes_shift = __ffs ( stripes ); <nl>  <nl> r = dm_set_target_max_io_len ( ti , chunk_size ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( sc ); <nl> return r ; <nl> + } <nl>  <nl> ti -> num_flush_bios = stripes ; <nl> ti -> num_discard_bios = stripes ;
static void discard_cap_releases ( struct ceph_mds_client * mdsc , <nl> num = le32_to_cpu ( head -> num ); <nl> dout (" discard_cap_releases mds % d % p % u \ n ", session -> s_mds , msg , num ); <nl> head -> num = cpu_to_le32 ( 0 ); <nl> + msg -> front . iov_len = sizeof (* head ); <nl> session -> s_num_cap_releases += num ; <nl>  <nl> /* requeue completed messages */
static int ascot2e_write_regs ( struct ascot2e_priv * priv , <nl> } <nl> }; <nl>  <nl> - if ( len + 1 >= sizeof ( buf )) { <nl> + if ( len + 1 > sizeof ( buf )) { <nl> dev_warn (& priv -> i2c -> dev ," wr reg =% 04x : len =% d is too big !\ n ", <nl> reg , len + 1 ); <nl> return - E2BIG ;
struct nv40_mpeg_priv { <nl> }; <nl>  <nl> struct nv40_mpeg_chan { <nl> - struct nouveau_mpeg base ; <nl> + struct nouveau_mpeg_chan base ; <nl> }; <nl>  <nl> /*******************************************************************************
static int ocfs2_rename ( struct inode * old_dir , <nl> * <nl> * And that ' s why , just like the VFS , we need a file system <nl> * rename lock . */ <nl> - if ( old_dentry != new_dentry ) { <nl> + if ( old_dir != new_dir && S_ISDIR ( old_inode -> i_mode )) { <nl> status = ocfs2_rename_lock ( osb ); <nl> if ( status < 0 ) { <nl> mlog_errno ( status );
bool gw_out_of_range ( struct bat_priv * bat_priv , <nl> } <nl>  <nl> neigh_old = find_router ( bat_priv , orig_dst_node , NULL ); <nl> - if (!! neigh_old ) <nl> + if (! neigh_old ) <nl> goto out ; <nl>  <nl> if ( curr_tq_avg - neigh_old -> tq_avg > GW_THRESHOLD )
static int sil24_softreset ( struct ata_port * ap , unsigned int * class ) <nl> goto err ; <nl> } <nl>  <nl> - /* <nl> - * XXX : Not sure whether the following sleep is needed or not . <nl> - * The original driver had it . So .... <nl> - */ <nl> - msleep ( 10 ); <nl> - <nl> + /* do SRST */ <nl> prb -> ctrl = PRB_CTRL_SRST ; <nl> prb -> fis [ 1 ] = 0 ; /* no PM yet */ <nl> 
struct kvm_vcpu * kvm_arch_vcpu_create ( struct kvm * kvm , unsigned int id ) <nl> int err ; <nl> struct kvm_vcpu * vcpu ; <nl>  <nl> + if ( irqchip_in_kernel ( kvm ) && vgic_initialized ( kvm )) { <nl> + err = - EBUSY ; <nl> + goto out ; <nl> + } <nl> + <nl> vcpu = kmem_cache_zalloc ( kvm_vcpu_cache , GFP_KERNEL ); <nl> if (! vcpu ) { <nl> err = - ENOMEM ;
static ssize_t dfs_file_write ( struct file * file , const char __user * user_buf , <nl> } else if ( dent == d -> dfs_emulate_power_cut ) { <nl> if ( kstrtoint ( buf , 0 , & val ) != 0 ) <nl> count = - EINVAL ; <nl> - d -> emulate_power_cut = val ; <nl> + else <nl> + d -> emulate_power_cut = val ; <nl> goto out ; <nl> } <nl> 
cy_ioctl ( struct tty_struct * tty , <nl> break ; <nl> # ifndef CONFIG_CYZ_INTR <nl> case CYZSETPOLLCYCLE : <nl> + if ( arg > LONG_MAX / HZ ) <nl> + return - ENODEV ; <nl> cyz_polling_cycle = ( arg * HZ ) / 1000 ; <nl> break ; <nl> case CYZGETPOLLCYCLE :
static bool gfar_add_rx_frag ( struct gfar_rx_buff * rxb , u32 lstatus , <nl> } <nl>  <nl> /* try reuse page */ <nl> - if ( unlikely ( page_count ( page ) != 1 )) <nl> + if ( unlikely ( page_count ( page ) != 1 || page_is_pfmemalloc ( page ))) <nl> return false ; <nl>  <nl> /* change offset to the other half */
static void xmit_common ( struct sk_buff * skb , struct ehea_swqe * swqe ) <nl> { <nl> swqe -> tx_control |= EHEA_SWQE_IMM_DATA_PRESENT | EHEA_SWQE_CRC ; <nl>  <nl> - if ( skb -> protocol != htons ( ETH_P_IP )) <nl> + if ( vlan_get_protocol ( skb ) != htons ( ETH_P_IP )) <nl> return ; <nl>  <nl> if ( skb -> ip_summed == CHECKSUM_PARTIAL )
int p54_parse_eeprom ( struct ieee80211_hw * dev , void * eeprom , int len ) <nl> case PDR_END : <nl> i = len ; <nl> break ; <nl> + default : <nl> + printk ( KERN_INFO " p54 : unknown eeprom code : 0x % x \ n ", <nl> + le16_to_cpu ( entry -> code )); <nl> + break ; <nl> } <nl>  <nl> entry = ( void *) entry + ( entry_len + 1 )* 2 ;
static struct platform_driver snvs_rtc_driver = { <nl> . name = " snvs_rtc ", <nl> . owner = THIS_MODULE , <nl> . pm = & snvs_rtc_pm_ops , <nl> - . of_match_table = of_match_ptr ( snvs_dt_ids ), <nl> + . of_match_table = snvs_dt_ids , <nl> }, <nl> . probe = snvs_rtc_probe , <nl> };
int snd_hda_codec_reset ( struct hda_codec * codec ) <nl> codec -> num_pcms = 0 ; <nl> codec -> pcm_info = NULL ; <nl> codec -> preset = NULL ; <nl> + memset (& codec -> patch_ops , 0 , sizeof ( codec -> patch_ops )); <nl> + codec -> slave_dig_outs = NULL ; <nl> + codec -> spdif_status_reset = 0 ; <nl> module_put ( codec -> owner ); <nl> codec -> owner = NULL ; <nl> 
struct iwl_cfg iwl6000i_2bg_cfg = { <nl> . fw_name_pre = IWL6050_FW_PRE , \ <nl> . ucode_api_max = IWL6050_UCODE_API_MAX , \ <nl> . ucode_api_min = IWL6050_UCODE_API_MIN , \ <nl> + . valid_tx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> + . valid_rx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> . ops = & iwl6050_ops , \ <nl> . eeprom_ver = EEPROM_6050_EEPROM_VERSION , \ <nl> . eeprom_calib_ver = EEPROM_6050_TX_POWER_VERSION , \
static struct hv_pci_dev * new_pcichild_device ( struct hv_pcibus_device * hbus , <nl> struct hv_pci_dev * hpdev ; <nl> struct pci_child_message * res_req ; <nl> struct q_res_req_compl comp_pkt ; <nl> - union { <nl> - struct pci_packet init_packet ; <nl> - u8 buffer [ 0x100 ]; <nl> + struct { <nl> + struct pci_packet init_packet ; <nl> + u8 buffer [ sizeof ( struct pci_child_message )]; <nl> } pkt ; <nl> unsigned long flags ; <nl> int ret ;
static ssize_t iio_ev_value_store ( struct device * dev , <nl> unsigned long val ; <nl> int ret ; <nl>  <nl> + if (! indio_dev -> info -> write_event_value ) <nl> + return - EINVAL ; <nl> + <nl> ret = strict_strtoul ( buf , 10 , & val ); <nl> if ( ret ) <nl> return ret ;
static void EChannel_proc_rcv ( struct hisax_d_if * d_if ) <nl> # ifdef CONFIG_PCI <nl> # include < linux / pci . h > <nl>  <nl> - static struct pci_device_id hisax_pci_tbl [] __devinitdata = { <nl> + static struct pci_device_id hisax_pci_tbl [] __devinitdata __used = { <nl> # ifdef CONFIG_HISAX_FRITZPCI <nl> { PCI_VDEVICE ( AVM , PCI_DEVICE_ID_AVM_A1 ) }, <nl> # endif
int mwifiex_bss_start ( struct mwifiex_private * priv , struct cfg80211_bss * bss , <nl> mwifiex_dbg ( adapter , ERROR , <nl> " Attempt to reconnect on csa closed chan (% d )\ n ", <nl> bss_desc -> channel ); <nl> + ret = - 1 ; <nl> goto done ; <nl> } <nl> 
int vty_init ( const struct file_operations * console_fops ); <nl>  <nl> static inline bool vt_force_oops_output ( struct vc_data * vc ) <nl> { <nl> - if ( oops_in_progress && vc -> vc_panic_force_write ) <nl> + if ( oops_in_progress && vc -> vc_panic_force_write && panic_timeout >= 0 ) <nl> return true ; <nl> return false ; <nl> }
typedef struct { <nl> uint8_t max_lun ; <nl>  <nl> uint32_t unique_id ; <nl> - uint8_t irq ; <nl> + int irq ; <nl> uint8_t ito ; <nl> caddr_t ibuf ; <nl> dma_addr_t ibuf_dma_h ;
void hid_sensor_remove_trigger ( struct iio_dev * indio_dev ) <nl> { <nl> iio_trigger_unregister ( indio_dev -> trig ); <nl> iio_trigger_free ( indio_dev -> trig ); <nl> + indio_dev -> trig = NULL ; <nl> } <nl> EXPORT_SYMBOL ( hid_sensor_remove_trigger ); <nl> 
struct device_type usb_device_type = { <nl>  <nl> static int ksuspend_usb_init ( void ) <nl> { <nl> - ksuspend_usb_wq = create_singlethread_workqueue (" ksuspend_usbd "); <nl> + /* This workqueue is supposed to be both freezable and <nl> + * singlethreaded . Its job doesn ' t justify running on more <nl> + * than one CPU . <nl> + */ <nl> + ksuspend_usb_wq = create_freezeable_workqueue (" ksuspend_usbd "); <nl> if (! ksuspend_usb_wq ) <nl> return - ENOMEM ; <nl> return 0 ;
void r8712_joinbss_event_callback ( struct _adapter * adapter , u8 * pbuf ) <nl>  <nl> if ( sizeof ( struct list_head ) == 4 * sizeof ( u32 )) { <nl> pnetwork = kmalloc ( sizeof ( struct wlan_network ), GFP_ATOMIC ); <nl> + if (! pnetwork ) <nl> + return ; <nl> memcpy (( u8 *) pnetwork + 16 , ( u8 *) pbuf + 8 , <nl> sizeof ( struct wlan_network ) - 16 ); <nl> } else
int sirfsoc_uart_probe ( struct platform_device * pdev ) <nl>  <nl> if ( sirfport -> hw_flow_ctrl ) { <nl> sirfport -> p = pinctrl_get_select_default (& pdev -> dev ); <nl> - ret = IS_ERR ( sirfport -> p ); <nl> - if ( ret ) <nl> + if ( IS_ERR ( sirfport -> p )) { <nl> + ret = PTR_ERR ( sirfport -> p ); <nl> goto err ; <nl> + } <nl> } <nl>  <nl> sirfport -> clk = clk_get (& pdev -> dev , NULL );
static int st33zp24_spi_evaluate_latency ( void * phy_id ) <nl> & data , 1 ); <nl> latency ++; <nl> } <nl> + if ( status < 0 ) <nl> + return status ; <nl> + if ( latency == MAX_SPI_LATENCY ) <nl> + return - ENODEV ; <nl> + <nl> return latency - 1 ; <nl> } /* evaluate_latency () */ <nl> 
static ide_startstop_t cdrom_do_block_pc ( ide_drive_t * drive , struct request * rq ) <nl> /* <nl> * check if dma is safe <nl> */ <nl> - if (( rq -> data_len & mask ) || ( addr & mask )) <nl> + if (( rq -> data_len & 3 ) || ( addr & mask )) <nl> info -> dma = 0 ; <nl> } <nl> 
static void flush_tlb_others_ipi ( const struct cpumask * cpumask , <nl> * We have to send the IPI only to <nl> * CPUs affected . <nl> */ <nl> - send_IPI_mask ( cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl> + send_IPI_mask ( f -> flush_cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl>  <nl> while (! cpumask_empty ( to_cpumask ( f -> flush_cpumask ))) <nl> cpu_relax ();
void pcmcia_disable_device ( struct pcmcia_device * p_dev ) { <nl> pcmcia_release_configuration ( p_dev ); <nl> pcmcia_release_io ( p_dev , & p_dev -> io ); <nl> pcmcia_release_irq ( p_dev , & p_dev -> irq ); <nl> - if (& p_dev -> win ) <nl> + if ( p_dev -> win ) <nl> pcmcia_release_window ( p_dev -> win ); <nl> } <nl> EXPORT_SYMBOL ( pcmcia_disable_device );
static int __init camellia_aesni_init ( void ) <nl> { <nl> const char * feature_name ; <nl>  <nl> + if (! cpu_has_avx || ! cpu_has_aes || ! cpu_has_osxsave ) { <nl> + pr_info (" AVX or AES - NI instructions are not detected .\ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> if (! cpu_has_xfeatures ( XSTATE_SSE | XSTATE_YMM , & feature_name )) { <nl> pr_info (" CPU feature '% s ' is not supported .\ n ", feature_name ); <nl> return - ENODEV ;
static void enqueue_cmd_and_start_io ( ctlr_info_t * h , <nl> spin_lock_irqsave (& h -> lock , flags ); <nl> addQ (& h -> reqQ , c ); <nl> h -> Qdepth ++; <nl> + if ( h -> Qdepth > h -> maxQsinceinit ) <nl> + h -> maxQsinceinit = h -> Qdepth ; <nl> start_io ( h ); <nl> spin_unlock_irqrestore (& h -> lock , flags ); <nl> }
int irq_domain_simple_dt_translate ( struct irq_domain * d , <nl> return - EINVAL ; <nl> if ( intsize < 1 ) <nl> return - EINVAL ; <nl> + if ( d -> nr_irq && (( intspec [ 0 ] < d -> hwirq_base ) || <nl> + ( intspec [ 0 ] >= d -> hwirq_base + d -> nr_irq ))) <nl> + return - EINVAL ; <nl>  <nl> * out_hwirq = intspec [ 0 ]; <nl> * out_type = IRQ_TYPE_NONE ;
out_attach : <nl> static void ocfs2_drop_dentry_lock ( struct ocfs2_super * osb , <nl> struct ocfs2_dentry_lock * dl ) <nl> { <nl> + iput ( dl -> dl_inode ); <nl> ocfs2_simple_drop_lockres ( osb , & dl -> dl_lockres ); <nl> ocfs2_lock_res_free (& dl -> dl_lockres ); <nl> - iput ( dl -> dl_inode ); <nl> kfree ( dl ); <nl> } <nl> 
struct dvb_frontend * tda829x_attach ( struct dvb_frontend * fe , <nl> } <nl>  <nl> if ((!( cfg ) || ( TDA829X_PROBE_TUNER == cfg -> probe_tuner )) && <nl> - ( tda829x_find_tuner ( fe ) < 0 )) <nl> + ( tda829x_find_tuner ( fe ) < 0 )) { <nl> + memset (& fe -> ops . analog_ops , 0 , sizeof ( struct analog_demod_ops )); <nl> + <nl> goto fail ; <nl> + } <nl>  <nl> switch ( priv -> ver ) { <nl> case TDA8290 :
i915_gem_set_tiling ( struct drm_device * dev , void * data , <nl> } <nl>  <nl> mutex_lock (& dev -> struct_mutex ); <nl> - if ( i915_gem_obj_is_pinned ( obj ) || obj -> framebuffer_references ) { <nl> + if ( obj -> pin_display || obj -> framebuffer_references ) { <nl> ret = - EBUSY ; <nl> goto err ; <nl> }
static ssize_t bonding_show_mii_status ( struct device * d , <nl> char * buf ) <nl> { <nl> struct bonding * bond = to_bond ( d ); <nl> + bool active = !! rcu_access_pointer ( bond -> curr_active_slave ); <nl>  <nl> - return sprintf ( buf , "% s \ n ", bond -> curr_active_slave ? " up " : " down "); <nl> + return sprintf ( buf , "% s \ n ", active ? " up " : " down "); <nl> } <nl> static DEVICE_ATTR ( mii_status , S_IRUGO , bonding_show_mii_status , NULL ); <nl> 
static int exynos_drm_crtc_mode_set_base ( struct drm_crtc * crtc , int x , int y , <nl>  <nl> DRM_DEBUG_KMS ("% s \ n ", __FILE__ ); <nl>  <nl> + /* when framebuffer changing is requested , crtc ' s dpms should be on */ <nl> + if ( exynos_crtc -> dpms > DRM_MODE_DPMS_ON ) { <nl> + DRM_ERROR (" failed framebuffer changing request .\ n "); <nl> + return - EPERM ; <nl> + } <nl> + <nl> crtc_w = crtc -> fb -> width - x ; <nl> crtc_h = crtc -> fb -> height - y ; <nl> 
int savagefb_probe_i2c_connector ( struct fb_info * info , u8 ** out_edid ) <nl> } <nl> } <nl>  <nl> - if ( out_edid ) <nl> - * out_edid = edid ; <nl> + * out_edid = edid ; <nl>  <nl> return ( edid ) ? 0 : 1 ; <nl> }
intel_dp_check_link_status ( struct intel_dp * intel_dp ) <nl> if (! to_intel_crtc ( intel_encoder -> base . crtc )-> active ) <nl> return ; <nl>  <nl> + /* FIXME : we need to synchronize this sort of stuff with hardware <nl> + * readout */ <nl> + if ( WARN_ON_ONCE (! intel_dp -> lane_count )) <nl> + return ; <nl> + <nl> /* if link training is requested we should perform it always */ <nl> if (( intel_dp -> compliance_test_type == DP_TEST_LINK_TRAINING ) || <nl> (! drm_dp_channel_eq_ok ( link_status , intel_dp -> lane_count ))) {
static void clk_pllv2_unprepare ( struct clk_hw * hw ) <nl> __raw_writel ( reg , pllbase + MXC_PLL_DP_CTL ); <nl> } <nl>  <nl> - struct clk_ops clk_pllv2_ops = { <nl> + static struct clk_ops clk_pllv2_ops = { <nl> . prepare = clk_pllv2_prepare , <nl> . unprepare = clk_pllv2_unprepare , <nl> . recalc_rate = clk_pllv2_recalc_rate ,
void <nl> trace_printk_seq ( struct trace_seq * s ) <nl> { <nl> /* Probably should print a warning here . */ <nl> - if ( s -> len >= 1000 ) <nl> - s -> len = 1000 ; <nl> + if ( s -> len >= TRACE_MAX_PRINT ) <nl> + s -> len = TRACE_MAX_PRINT ; <nl>  <nl> /* should be zero ended , but we are paranoid . */ <nl> s -> buffer [ s -> len ] = 0 ;
static int send_reply ( struct svcxprt_rdma * rdma , <nl> " svcrdma : could not post a receive buffer , err =% d ." <nl> " Closing transport % p .\ n ", ret , rdma ); <nl> set_bit ( XPT_CLOSE , & rdma -> sc_xprt . xpt_flags ); <nl> - return 0 ; <nl> + svc_rdma_put_context ( ctxt , 0 ); <nl> + return - ENOTCONN ; <nl> } <nl>  <nl> /* Prepare the context */
pte_t * huge_pte_offset ( struct mm_struct * mm , unsigned long addr ) <nl> pmd_t * pmd = NULL ; <nl>  <nl> pgd = pgd_offset ( mm , addr ); <nl> - pud = pud_offset ( pgd , addr ); <nl> - pmd = pmd_offset ( pud , addr ); <nl> + if ( pgd_present (* pgd )) { <nl> + pud = pud_offset ( pgd , addr ); <nl> + if ( pud_present (* pud )) <nl> + pmd = pmd_offset ( pud , addr ); <nl> + } <nl> return ( pte_t *) pmd ; <nl> } <nl> 
static int proc_sys_readdir ( struct file * file , struct dir_context * ctx ) <nl> ctl_dir = container_of ( head , struct ctl_dir , header ); <nl>  <nl> if (! dir_emit_dots ( file , ctx )) <nl> - return 0 ; <nl> + goto out ; <nl>  <nl> pos = 2 ; <nl>  <nl> static int proc_sys_readdir ( struct file * file , struct dir_context * ctx ) <nl> break ; <nl> } <nl> } <nl> + out : <nl> sysctl_head_finish ( head ); <nl> return 0 ; <nl> }
nv134_chipset = { <nl> . fb = gp104_fb_new , <nl> . fuse = gm107_fuse_new , <nl> . gpio = gk104_gpio_new , <nl> + . i2c = gm200_i2c_new , <nl> . imem = nv50_instmem_new , <nl> . mc = gp100_mc_new , <nl> . mmu = gf100_mmu_new ,
static int record_connection ( char * host , char * port , char * busid , int rhport ) <nl> char buff [ MAX_BUFF + 1 ]; <nl> int ret ; <nl>  <nl> - mkdir ( VHCI_STATE_PATH , 0700 ); <nl> + ret = mkdir ( VHCI_STATE_PATH , 0700 ); <nl> + if ( ret < 0 ) <nl> + return - 1 ; <nl>  <nl> snprintf ( path , PATH_MAX , VHCI_STATE_PATH "/ port % d ", rhport ); <nl> 
static int bcm_parse_target_params ( PMINI_ADAPTER Adapter ) <nl> if (! buff ) <nl> return - ENOMEM ; <nl>  <nl> - if (( Adapter -> pstargetparams = kmalloc ( sizeof ( STARGETPARAMS ), GFP_KERNEL )) == NULL ) { <nl> + Adapter -> pstargetparams = kmalloc ( sizeof ( STARGETPARAMS ), GFP_KERNEL ); <nl> + if ( Adapter -> pstargetparams == NULL ) { <nl> kfree ( buff ); <nl> return - ENOMEM ; <nl> }
int perf_session__cpu_bitmap ( struct perf_session * session , <nl> } <nl>  <nl> map = cpu_map__new ( cpu_list ); <nl> + if ( map == NULL ) { <nl> + pr_err (" Invalid cpu_list \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> for ( i = 0 ; i < map -> nr ; i ++) { <nl> int cpu = map -> map [ i ];
static unsigned long ramfs_nommu_get_unmapped_area ( struct file * file , <nl>  <nl> /* gang - find the pages */ <nl> ret = - ENOMEM ; <nl> - pages = kzalloc ( lpages * sizeof ( struct page *), GFP_KERNEL ); <nl> + pages = kcalloc ( lpages , sizeof ( struct page *), GFP_KERNEL ); <nl> if (! pages ) <nl> goto out_free ; <nl> 
static int tg3_bmcr_reset ( struct tg3 * tp ) <nl> } <nl> udelay ( 10 ); <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ; <nl> static int tg3_wait_macro_done ( struct tg3 * tp ) <nl> break ; <nl> } <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ;
int del_mtd_blktrans_dev ( struct mtd_blktrans_dev * old ) <nl> BUG (); <nl> } <nl>  <nl> - /* Stop new requests to arrive */ <nl> - del_gendisk ( old -> disk ); <nl> - <nl> if ( old -> disk_attributes ) <nl> sysfs_remove_group (& disk_to_dev ( old -> disk )-> kobj , <nl> old -> disk_attributes ); <nl>  <nl> + /* Stop new requests to arrive */ <nl> + del_gendisk ( old -> disk ); <nl> + <nl> + <nl> /* Stop the thread */ <nl> kthread_stop ( old -> thread ); <nl> 
void dlm_lowcomms_stop ( void ) <nl> con = __nodeid2con ( i , 0 ); <nl> if ( con ) { <nl> close_connection ( con , true ); <nl> + if ( con -> othercon ) <nl> + kmem_cache_free ( con_cache , con -> othercon ); <nl> kmem_cache_free ( con_cache , con ); <nl> } <nl> }
tracing_buffers_splice_read ( struct file * file , loff_t * ppos , <nl> if (! ref ) <nl> break ; <nl>  <nl> + ref -> ref = 1 ; <nl> ref -> buffer = info -> tr -> buffer ; <nl> ref -> page = ring_buffer_alloc_read_page ( ref -> buffer ); <nl> if (! ref -> page ) {
static inline struct kmem_cache * cache_from_obj ( struct kmem_cache * s , void * x ) <nl> return cachep ; <nl>  <nl> pr_err ("% s : Wrong slab cache . % s but object is from % s \ n ", <nl> - __FUNCTION__ , cachep -> name , s -> name ); <nl> + __func__ , cachep -> name , s -> name ); <nl> WARN_ON_ONCE ( 1 ); <nl> return s ; <nl> }
static ssize_t rbd_add ( struct bus_type * bus , <nl> if (! try_module_get ( THIS_MODULE )) <nl> return - ENODEV ; <nl>  <nl> - mon_dev_name = kmalloc ( RBD_MAX_OPT_LEN , GFP_KERNEL ); <nl> + mon_dev_name = kmalloc ( count , GFP_KERNEL ); <nl> if (! mon_dev_name ) <nl> goto err_out_mod ; <nl>  <nl> - options = kmalloc ( RBD_MAX_OPT_LEN , GFP_KERNEL ); <nl> + options = kmalloc ( count , GFP_KERNEL ); <nl> if (! options ) <nl> goto err_mon_dev ; <nl> 
static int read_exceptions ( struct pstore * ps , <nl> r = insert_exceptions ( ps , area , callback , callback_context , <nl> & full ); <nl>  <nl> + if (! full ) <nl> + memcpy ( ps -> area , area , ps -> store -> chunk_size << SECTOR_SHIFT ); <nl> + <nl> dm_bufio_release ( bp ); <nl>  <nl> dm_bufio_forget ( client , chunk );
radeon_atom_encoder_dpms_dig ( struct drm_encoder * encoder , int mode ) <nl> * does the same thing and more . <nl> */ <nl> if (( rdev -> family != CHIP_RV710 ) && ( rdev -> family != CHIP_RV730 ) && <nl> - ( rdev -> family != CHIP_RS880 )) <nl> + ( rdev -> family != CHIP_RS780 ) && ( rdev -> family != CHIP_RS880 )) <nl> atombios_dig_transmitter_setup ( encoder , ATOM_TRANSMITTER_ACTION_ENABLE_OUTPUT , 0 , 0 ); <nl> } <nl> if ( ENCODER_MODE_IS_DP ( atombios_get_encoder_mode ( encoder )) && connector ) {
static int __init list_sort_test ( void ) <nl> } <nl> count ++; <nl> } <nl> + if ( head . prev != cur ) { <nl> + printk ( KERN_ERR " list_sort_test : error : list is corrupted \ n "); <nl> + goto exit ; <nl> + } <nl> + <nl>  <nl> if ( count != TEST_LIST_LEN ) { <nl> printk ( KERN_ERR " list_sort_test : error : bad list length % d ",
set_ether_config ( struct eth_dev * dev , gfp_t gfp_flags ) <nl> result = usb_ep_enable ( dev -> out_ep , dev -> out ); <nl> if ( result != 0 ) { <nl> DEBUG ( dev , " enable % s --> % d \ n ", <nl> - dev -> in_ep -> name , result ); <nl> + dev -> out_ep -> name , result ); <nl> goto done ; <nl> } <nl> }
static irqreturn_t ad7298_trigger_handler ( int irq , void * p ) <nl> struct iio_dev * indio_dev = pf -> indio_dev ; <nl> struct ad7298_state * st = iio_priv ( indio_dev ); <nl> struct iio_buffer * ring = indio_dev -> buffer ; <nl> - s64 time_ns ; <nl> + s64 time_ns = 0 ; <nl> __u16 buf [ 16 ]; <nl> int b_sent , i ; <nl> 
void psched_ratecfg_precompute ( struct psched_ratecfg * r , u32 rate ) <nl> u64 mult ; <nl> int shift ; <nl>  <nl> - r -> rate_bps = rate << 3 ; <nl> + r -> rate_bps = ( u64 ) rate << 3 ; <nl> r -> shift = 0 ; <nl> r -> mult = 1 ; <nl> /*
* ( you will need to reboot afterwards ) */ <nl> /* # define BNX2X_STOP_ON_ERROR */ <nl>  <nl> -# define DRV_MODULE_VERSION " 1 . 72 . 10 - 0 " <nl> -# define DRV_MODULE_RELDATE " 2012 / 02 / 20 " <nl> +# define DRV_MODULE_VERSION " 1 . 72 . 17 - 0 " <nl> +# define DRV_MODULE_RELDATE " 2012 / 04 / 02 " <nl> # define BNX2X_BC_VER 0x040200 <nl>  <nl> # if defined ( CONFIG_DCB )
static int bdc_udc_set_selfpowered ( struct usb_gadget * gadget , <nl> unsigned long flags ; <nl>  <nl> dev_dbg ( bdc -> dev , "% s ()\ n ", __func__ ); <nl> + gadget -> is_selfpowered = ( is_self != 0 ); <nl> spin_lock_irqsave (& bdc -> lock , flags ); <nl> if (! is_self ) <nl> bdc -> devstatus |= 1 << USB_DEVICE_SELF_POWERED ;
static bool _is_valid_div ( struct clk_divider * divider , unsigned int div ) <nl> static int _round_up_table ( const struct clk_div_table * table , int div ) <nl> { <nl> const struct clk_div_table * clkt ; <nl> - int up = _get_table_maxdiv ( table ); <nl> + int up = INT_MAX ; <nl>  <nl> for ( clkt = table ; clkt -> div ; clkt ++) { <nl> if ( clkt -> div == div )
static int __ocfs2_change_file_space ( struct file * file , struct inode * inode , <nl> if ( ret < 0 ) <nl> mlog_errno ( ret ); <nl>  <nl> + if ( file -> f_flags & O_SYNC ) <nl> + handle -> h_sync = 1 ; <nl> + <nl> ocfs2_commit_trans ( osb , handle ); <nl>  <nl> out_inode_unlock :
static int smc91c92_config ( struct pcmcia_device * link ) <nl> struct net_device * dev = link -> priv ; <nl> struct smc_private * smc = netdev_priv ( dev ); <nl> char * name ; <nl> - int i , j , rev ; <nl> + int i , rev , j = 0 ; <nl> unsigned int ioaddr ; <nl> u_long mir ; <nl> 
void register_lapic_address ( unsigned long address ); <nl> extern void setup_boot_APIC_clock ( void ); <nl> extern void setup_secondary_APIC_clock ( void ); <nl> extern int APIC_init_uniprocessor ( void ); <nl> + <nl> +# ifdef CONFIG_X86_64 <nl> + static inline int apic_force_enable ( unsigned long addr ) <nl> +{ <nl> + return - 1 ; <nl> +} <nl> +# else <nl> extern int apic_force_enable ( unsigned long addr ); <nl> +# endif <nl>  <nl> extern int apic_bsp_setup ( bool upmode ); <nl> extern void apic_ap_setup ( void );
int blkcg_init_queue ( struct request_queue * q ) <nl> if ( preloaded ) <nl> radix_tree_preload_end (); <nl>  <nl> - if ( IS_ERR ( blkg )) { <nl> - blkg_free ( new_blkg ); <nl> + if ( IS_ERR ( blkg )) <nl> return PTR_ERR ( blkg ); <nl> - } <nl>  <nl> q -> root_blkg = blkg ; <nl> q -> root_rl . blkg = blkg ;
static int s3c24xx_eint_init ( struct samsung_pinctrl_drv_data * d ) <nl> irq = bank -> eint_offset ; <nl> mask = bank -> eint_mask ; <nl> for ( pin = 0 ; mask ; ++ pin , mask >>= 1 ) { <nl> - if ( irq > NUM_EINT ) <nl> + if ( irq >= NUM_EINT ) <nl> break ; <nl> if (!( mask & 1 )) <nl> continue ;
int snd_pcm_status ( struct snd_pcm_substream * substream , <nl> runtime -> status -> audio_tstamp ; <nl> goto _tstamp_end ; <nl> } <nl> + } else { <nl> + /* get tstamp only in fallback mode and only if enabled */ <nl> + if ( runtime -> tstamp_mode == SNDRV_PCM_TSTAMP_ENABLE ) <nl> + snd_pcm_gettime ( runtime , & status -> tstamp ); <nl> } <nl> - snd_pcm_gettime ( runtime , & status -> tstamp ); <nl> _tstamp_end : <nl> status -> appl_ptr = runtime -> control -> appl_ptr ; <nl> status -> hw_ptr = runtime -> status -> hw_ptr ;
static int raid10_add_disk ( mddev_t * mddev , mdk_rdev_t * rdev ) <nl> if (! enough ( conf )) <nl> return - EINVAL ; <nl>  <nl> - if ( rdev -> raid_disk ) <nl> + if ( rdev -> raid_disk >= 0 ) <nl> first = last = rdev -> raid_disk ; <nl>  <nl> if ( rdev -> saved_raid_disk >= 0 &&
static int __init cy_detect_isa ( void ) <nl> continue ; <nl> } <nl> # ifdef MODULE <nl> - if ( isparam && irq [ i ]) <nl> + if ( isparam && i < NR_CARDS && irq [ i ]) <nl> cy_isa_irq = irq [ i ]; <nl> else <nl> # endif
skbfree ( struct sk_buff * skb ) <nl> return ; <nl> while ( atomic_read (& skb_shinfo ( skb )-> dataref ) != 1 && i -- > 0 ) <nl> msleep ( Sms ); <nl> - if ( i <= 0 ) { <nl> + if ( i < 0 ) { <nl> printk ( KERN_ERR <nl> " aoe : % s holds ref : % s \ n ", <nl> skb -> dev ? skb -> dev -> name : " netif ",
static int newseg ( struct ipc_namespace * ns , struct ipc_params * params ) <nl> if ( size < SHMMIN || size > ns -> shm_ctlmax ) <nl> return - EINVAL ; <nl>  <nl> - if ( ns -> shm_tot + numpages > ns -> shm_ctlall ) <nl> + if ( ns -> shm_tot + numpages < ns -> shm_tot || <nl> + ns -> shm_tot + numpages > ns -> shm_ctlall ) <nl> return - ENOSPC ; <nl>  <nl> shp = ipc_rcu_alloc ( sizeof (* shp ));
static void mwifiex_unregister_dev ( struct mwifiex_adapter * adapter ) <nl> { <nl> struct pcie_service_card * card = adapter -> card ; <nl> const struct mwifiex_pcie_card_reg * reg ; <nl> - struct pci_dev * pdev = card -> dev ; <nl> + struct pci_dev * pdev ; <nl> int i ; <nl>  <nl> if ( card ) { <nl> + pdev = card -> dev ; <nl> if ( card -> msix_enable ) { <nl> for ( i = 0 ; i < MWIFIEX_NUM_MSIX_VECTORS ; i ++) <nl> synchronize_irq ( card -> msix_entries [ i ]. vector );
int vmw_kms_update_layout_ioctl ( struct drm_device * dev , void * data , <nl> ret = copy_from_user ( rects , user_rects , rects_size ); <nl> if ( unlikely ( ret != 0 )) { <nl> DRM_ERROR (" Failed to get rects .\ n "); <nl> + ret = - EFAULT ; <nl> goto out_free ; <nl> } <nl> 
static ssize_t store_scaling_governor ( struct cpufreq_policy * policy , <nl> return - EINVAL ; <nl>  <nl> ret = cpufreq_set_policy ( policy , & new_policy ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> policy -> user_policy . policy = policy -> policy ; <nl> policy -> user_policy . governor = policy -> governor ; <nl> - <nl> - if ( ret ) <nl> - return ret ; <nl> - else <nl> - return count ; <nl> + return count ; <nl> } <nl>  <nl> /**
static void taal_esd_work ( struct work_struct * work ) <nl> } <nl> /* Self - diagnostics result is also shown on TE GPIO line . We need <nl> * to re - enable TE after self diagnostics */ <nl> - if ( td -> use_ext_te && td -> te_enabled ) <nl> - taal_enable_te ( dssdev , true ); <nl> + if ( td -> use_ext_te && td -> te_enabled ) { <nl> + r = taal_dcs_write_1 ( DCS_TEAR_ON , 0 ); <nl> + if ( r ) <nl> + goto err ; <nl> + } <nl>  <nl> dsi_bus_unlock (); <nl> 
static int __devinit ci13xxx_pci_probe ( struct pci_dev * pdev , <nl> struct resource res [ 3 ]; <nl> int retval = 0 , nres = 2 ; <nl>  <nl> + if (! driver ) { <nl> + dev_err (& pdev -> dev , " device doesn ' t provide driver data \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> retval = pci_enable_device ( pdev ); <nl> if ( retval ) <nl> goto done ;
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
int dlm_posix_lock ( dlm_lockspace_t * lockspace , u64 number , struct file * file , <nl> send_op ( op ); <nl>  <nl> if ( xop -> callback == NULL ) { <nl> - rv = wait_event_killable ( recv_wq , ( op -> done != 0 )); <nl> + rv = wait_event_interruptible ( recv_wq , ( op -> done != 0 )); <nl> if ( rv == - ERESTARTSYS ) { <nl> log_debug ( ls , " dlm_posix_lock : wait killed % llx ", <nl> ( unsigned long long ) number );
int rtl8188eu_init_recv_priv ( struct adapter * padapter ) <nl> _rtw_init_queue (& precvpriv -> free_recv_buf_queue ); <nl>  <nl> precvpriv -> pallocated_recv_buf = <nl> - kzalloc ( NR_RECVBUFF * sizeof ( struct recv_buf ), GFP_KERNEL ); <nl> + kcalloc ( NR_RECVBUFF , sizeof ( struct recv_buf ), GFP_KERNEL ); <nl> if ( precvpriv -> pallocated_recv_buf == NULL ) { <nl> res = _FAIL ; <nl> RT_TRACE ( _module_rtl871x_recv_c_ , _drv_err_ ,
static int atmel_pdmic_cpu_dai_startup ( struct snd_pcm_substream * substream , <nl> return ret ; <nl>  <nl> ret = clk_prepare_enable ( dd -> pclk ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + clk_disable_unprepare ( dd -> gclk ); <nl> return ret ; <nl> + } <nl>  <nl> /* Clear all bits in the Control Register ( PDMIC_CR ) */ <nl> regmap_write ( dd -> regmap , PDMIC_CR , 0 );
static int gb_loopback_transfer ( struct gb_loopback * gb , u32 len ) <nl> return - ENOMEM ; <nl> } <nl>  <nl> + memset ( request -> data , 0x5A , len ); <nl> + <nl> request -> len = cpu_to_le32 ( len ); <nl>  <nl> do_gettimeofday (& ts );
noinline int btrfs_truncate_inode_items ( struct btrfs_trans_handle * trans , <nl> if ( root -> ref_cows ) <nl> btrfs_drop_extent_cache ( inode , new_size & (~ mask ), ( u64 )- 1 , 0 ); <nl> path = btrfs_alloc_path (); <nl> - path -> reada = - 1 ; <nl> BUG_ON (! path ); <nl> + path -> reada = - 1 ; <nl>  <nl> /* FIXME , add redo link to tree so we don ' t leak on crash */ <nl> key . objectid = inode -> i_ino ;
static int chap_server_compute_md5 ( <nl> pr_err (" Unable to convert incoming challenge \ n "); <nl> goto out ; <nl> } <nl> + if ( challenge_len > 1024 ) { <nl> + pr_err (" CHAP_C exceeds maximum binary size of 1024 bytes \ n "); <nl> + goto out ; <nl> + } <nl> /* <nl> * During mutual authentication , the CHAP_C generated by the <nl> * initiator must not match the original CHAP_C generated by
struct map * machine__new_module ( struct machine * machine , u64 start , <nl> if ( kmod_path__parse_name (& m , filename )) <nl> return NULL ; <nl>  <nl> + map = map_groups__find_by_name (& machine -> kmaps , MAP__FUNCTION , <nl> + m . name ); <nl> + if ( map ) <nl> + goto out ; <nl> + <nl> dso = machine__module_dso ( machine , & m , filename ); <nl> if ( dso == NULL ) <nl> goto out ;
int iwl_mvm_scan_request ( struct iwl_mvm * mvm , <nl> basic_ssid ? 1 : 0 ); <nl>  <nl> cmd -> tx_cmd . tx_flags = cpu_to_le32 ( TX_CMD_FLG_SEQ_CTL | <nl> - TX_CMD_FLG_BT_DIS ); <nl> + 3 << TX_CMD_FLG_BT_PRIO_POS ); <nl> + <nl> cmd -> tx_cmd . sta_id = mvm -> aux_sta . sta_id ; <nl> cmd -> tx_cmd . life_time = cpu_to_le32 ( TX_CMD_LIFE_TIME_INFINITE ); <nl> cmd -> tx_cmd . rate_n_flags =
static const char * sata_pmp_spec_rev_str ( const u32 * gscr ) <nl> { <nl> u32 rev = gscr [ SATA_PMP_GSCR_REV ]; <nl>  <nl> + if ( rev & ( 1 << 3 )) <nl> + return " 1 . 2 "; <nl> if ( rev & ( 1 << 2 )) <nl> return " 1 . 1 "; <nl> if ( rev & ( 1 << 1 ))
mwifiex_wmm_get_highest_priolist_ptr ( struct mwifiex_adapter * adapter , <nl> list_for_each_entry ( ptr , & tid_ptr -> ra_list , <nl> list ) { <nl>  <nl> - if (! skb_queue_empty (& ptr -> skb_head )) <nl> + if (! ptr -> tx_paused && <nl> + ! skb_queue_empty (& ptr -> skb_head )) <nl> /* holds both locks */ <nl> goto found ; <nl> }
int snd_hda_parse_pin_def_config ( struct hda_codec * codec , <nl> memmove ( sequences_hp + i , sequences_hp + i + 1 , <nl> sizeof ( sequences_hp [ 0 ]) * ( cfg -> hp_outs - i )); <nl> } <nl> + memset ( cfg -> hp_pins + cfg -> hp_outs , 0 , <nl> + sizeof ( hda_nid_t ) * ( AUTO_CFG_MAX_OUTS - cfg -> hp_outs )); <nl> } <nl>  <nl> /* sort by sequence */
static void alloc_mem ( void ** dst , void ** src , size_t length ) <nl> * src = zalloc ( length ); <nl> if (!* src ) <nl> die (" memory allocation failed - maybe length is too large ?\ n "); <nl> + /* Make sure to always replace the zero pages even if MMAP_THRESH is crossed */ <nl> + memset (* src , 0 , length ); <nl> } <nl>  <nl> static u64 do_memcpy_cycle ( memcpy_t fn , size_t len , bool prefault )
static void __init setup_hwcaps ( void ) <nl> void __init <nl> setup_arch ( char ** cmdline_p ) <nl> { <nl> + /* set up preferred console */ <nl> + add_preferred_console (" ttyS ", 0 , NULL ); <nl> + <nl> /* <nl> * print what head . S has found out about the machine <nl> */
static int ucc_hdlc_probe ( struct platform_device * pdev ) <nl>  <nl> err_hdlc_init : <nl> err_miss_tsa_property : <nl> - kfree ( uhdlc_priv ); <nl> if ( uhdlc_priv -> tsa ) <nl> kfree ( utdm ); <nl> err_alloc_utdm :
void vmbus_close ( struct vmbus_channel * channel ) <nl> free_channel ( channel ); <nl> } <nl> } <nl> + EXPORT_SYMBOL_GPL ( vmbus_close ); <nl>  <nl> /** <nl> * vmbus_sendpacket () - Send the specified buffer on the given channel
static void blk_stat_flush_batch ( struct blk_rq_stat * stat ) <nl>  <nl> static void blk_stat_sum ( struct blk_rq_stat * dst , struct blk_rq_stat * src ) <nl> { <nl> + blk_stat_flush_batch ( src ); <nl> + <nl> if (! src -> nr_samples ) <nl> return ; <nl>  <nl> - blk_stat_flush_batch ( src ); <nl> - <nl> dst -> min = min ( dst -> min , src -> min ); <nl> dst -> max = max ( dst -> max , src -> max ); <nl> 
ecryptfs_add_global_auth_tok ( struct ecryptfs_mount_crypt_stat * mount_crypt_stat , <nl> struct ecryptfs_global_auth_tok * new_auth_tok ; <nl> int rc = 0 ; <nl>  <nl> - new_auth_tok = kmem_cache_alloc ( ecryptfs_global_auth_tok_cache , <nl> + new_auth_tok = kmem_cache_zalloc ( ecryptfs_global_auth_tok_cache , <nl> GFP_KERNEL ); <nl> if (! new_auth_tok ) { <nl> rc = - ENOMEM ;
static int moxart_gpio_probe ( struct platform_device * pdev ) <nl> gc -> parent = dev ; <nl> gc -> owner = THIS_MODULE ; <nl>  <nl> - ret = gpiochip_add_data ( gc , NULL ); <nl> + ret = devm_gpiochip_add_data ( dev , gc , NULL ); <nl> if ( ret ) { <nl> dev_err ( dev , "% s : gpiochip_add failed \ n ", <nl> dev -> of_node -> full_name );
LNetCtl ( unsigned int cmd , void * arg ) <nl> int rc ; <nl> unsigned long secs_passed ; <nl>  <nl> + BUILD_BUG_ON ( LIBCFS_IOC_DATA_MAX < <nl> + sizeof ( struct lnet_ioctl_net_config ) + <nl> + sizeof ( struct lnet_ioctl_config_data )); <nl> + <nl> switch ( cmd ) { <nl> case IOC_LIBCFS_GET_NI : <nl> rc = LNetGetId ( data -> ioc_count , & id );
static void WILC_WFI_mon_setup ( struct net_device * dev ) <nl> ether_setup ( dev ); <nl> dev -> tx_queue_len = 0 ; <nl> dev -> type = ARPHRD_IEEE80211_RADIOTAP ; <nl> - memset ( dev -> dev_addr , 0 , ETH_ALEN ); <nl> + eth_zero_addr ( dev -> dev_addr ); <nl>  <nl> # ifdef USE_WIRELESS <nl> {
static int hostap_enable_hostapd ( PSDevice pDevice , int rtnl_locked ) <nl>  <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO "% s : Enabling hostapd mode \ n ", dev -> name ); <nl>  <nl> - pDevice -> apdev = kzalloc ( sizeof ( struct net_device ), GFP_KERNEL ); <nl> + pDevice -> apdev = alloc_etherdev ( sizeof (* apdev_priv )); <nl> if ( pDevice -> apdev == NULL ) <nl> return - ENOMEM ; <nl> 
static int s3c2440_nand_calculate_ecc ( struct mtd_info * mtd , const u_char * dat , u <nl> ecc_code [ 1 ] = ecc >> 8 ; <nl> ecc_code [ 2 ] = ecc >> 16 ; <nl>  <nl> - pr_debug ("% s : returning ecc % 06x \ n ", __func__ , ecc ); <nl> + pr_debug ("% s : returning ecc % 06lx \ n ", __func__ , ecc ); <nl>  <nl> return 0 ; <nl> }
static ssize_t ab8500_subscribe_write ( struct file * file , <nl> */ <nl> dev_attr [ irq_index ] = kmalloc ( sizeof ( struct device_attribute ), <nl> GFP_KERNEL ); <nl> + if (! dev_attr [ irq_index ]) <nl> + return - ENOMEM ; <nl> + <nl> event_name [ irq_index ] = kmalloc ( count , GFP_KERNEL ); <nl> sprintf ( event_name [ irq_index ], "% lu ", user_val ); <nl> dev_attr [ irq_index ]-> show = show_irq ;
struct hisi_clock_data * hisi_clk_init ( struct device_node * np , <nl> goto err ; <nl> } <nl> clk_data -> base = base ; <nl> - <nl> - clk_table = kzalloc ( sizeof ( struct clk *) * nr_clks , GFP_KERNEL ); <nl> + clk_table = kcalloc ( nr_clks , sizeof (* clk_table ), GFP_KERNEL ); <nl> if (! clk_table ) { <nl> pr_err ("% s : could not allocate clock lookup table \ n ", __func__ ); <nl> goto err_data ;
unsigned long do_mremap ( unsigned long addr , <nl> map_flags |= MAP_SHARED ; <nl>  <nl> new_addr = get_unmapped_area ( vma -> vm_file , 0 , new_len , <nl> - vma -> vm_pgoff , map_flags ); <nl> + vma -> vm_pgoff + <nl> + (( addr - vma -> vm_start ) >> PAGE_SHIFT ), <nl> + map_flags ); <nl> if ( new_addr & ~ PAGE_MASK ) { <nl> ret = new_addr ; <nl> goto out ;
int t4vf_wr_mbox_core ( struct adapter * adapter , const void * cmd , int size , <nl> delay_idx = 0 ; <nl> ms = delay [ 0 ]; <nl>  <nl> - for ( i = 0 ; i < 500 ; i += ms ) { <nl> + for ( i = 0 ; i < FW_CMD_MAX_TIMEOUT ; i += ms ) { <nl> if ( sleep_ok ) { <nl> ms = delay [ delay_idx ]; <nl> if ( delay_idx < ARRAY_SIZE ( delay ) - 1 )
struct img_hash_request_ctx { <nl> unsigned long op ; <nl>  <nl> size_t bufcnt ; <nl> - u8 buffer [ 0 ] __aligned ( sizeof ( u32 )); <nl> struct ahash_request fallback_req ; <nl> + <nl> + /* Zero length buffer must remain last member of struct */ <nl> + u8 buffer [ 0 ] __aligned ( sizeof ( u32 )); <nl> }; <nl>  <nl> struct img_hash_ctx {
static int ixgbe_ndo_fdb_add ( struct ndmsg * ndm , struct nlattr * tb [], <nl> { <nl> /* guarantee we can provide a unique filter for the unicast address */ <nl> if ( is_unicast_ether_addr ( addr ) || is_link_local_ether_addr ( addr )) { <nl> - if ( IXGBE_MAX_PF_MACVLANS <= netdev_uc_count ( dev )) <nl> + struct ixgbe_adapter * adapter = netdev_priv ( dev ); <nl> + u16 pool = VMDQ_P ( 0 ); <nl> + <nl> + if ( netdev_uc_count ( dev ) >= ixgbe_available_rars ( adapter , pool )) <nl> return - ENOMEM ; <nl> } <nl> 
# define OP_31_XOP_SLBIA 498 <nl> # define OP_31_XOP_MFSR 595 <nl> # define OP_31_XOP_MFSRIN 659 <nl> +# define OP_31_XOP_DCBA 758 <nl> # define OP_31_XOP_SLBMFEV 851 <nl> # define OP_31_XOP_EIOIO 854 <nl> # define OP_31_XOP_SLBMFEE 915 <nl> int kvmppc_core_emulate_op ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> kvmppc_set_gpr ( vcpu , get_rt ( inst ), t ); <nl> } <nl> break ; <nl> + case OP_31_XOP_DCBA : <nl> + /* Gets treated as NOP */ <nl> + break ; <nl> case OP_31_XOP_DCBZ : <nl> { <nl> ulong rb = kvmppc_get_gpr ( vcpu , get_rb ( inst ));
static int ucode_init ( loader_block * lb , amb_dev * dev ) <nl> const struct firmware * fw ; <nl> unsigned long start_address ; <nl> const struct ihex_binrec * rec ; <nl> - const char * errmsg = 0 ; <nl> + const char * errmsg = NULL ; <nl> int res ; <nl>  <nl> res = request_ihex_firmware (& fw , " atmsar11 . fw ", & dev -> pci_dev -> dev );
unapply_new_state : <nl> pinmux_enable_setting ( setting ); <nl> } <nl> } <nl> + <nl> + p -> state = old_state ; <nl> return ret ; <nl> } <nl> 
static ssize_t chars_in_buffer ( struct tty_struct * tty ) <nl>  <nl> static ssize_t n_tty_chars_in_buffer ( struct tty_struct * tty ) <nl> { <nl> + WARN_ONCE ( 1 , "% s is deprecated and scheduled for removal .", __func__ ); <nl> return chars_in_buffer ( tty ); <nl> } <nl> 
static struct neighbour * fake_neigh_lookup ( const struct dst_entry * dst , const vo <nl> return NULL ; <nl> } <nl>  <nl> + static unsigned int fake_mtu ( const struct dst_entry * dst ) <nl> +{ <nl> + return dst -> dev -> mtu ; <nl> +} <nl> + <nl> static struct dst_ops fake_dst_ops = { <nl> . family = AF_INET , <nl> . protocol = cpu_to_be16 ( ETH_P_IP ), <nl> . update_pmtu = fake_update_pmtu , <nl> . cow_metrics = fake_cow_metrics , <nl> . neigh_lookup = fake_neigh_lookup , <nl> + . mtu = fake_mtu , <nl> }; <nl>  <nl> /*
static int __init pasic3_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> if ( pdata && pdata -> led_pdata ) { <nl> + led_cell . platform_data = pdata -> led_pdata ; <nl> + led_cell . pdata_size = sizeof ( struct pasic3_leds_machinfo ); <nl> ret = mfd_add_devices (& pdev -> dev , pdev -> id , & led_cell , 1 , r , 0 ); <nl> if ( ret < 0 ) <nl> dev_warn ( dev , " failed to register LED device \ n ");
struct fpgaimage { <nl> char part [ MAX_STR ]; <nl> char date [ MAX_STR ]; <nl> char time [ MAX_STR ]; <nl> - int32_t lendata ; <nl> + int lendata ; <nl> char * fpgadata ; <nl> };
int msm_proc_comm ( unsigned cmd , unsigned * data1 , unsigned * data2 ) <nl> * and unknown state . This function should be called early to <nl> * wait on the ARM9 . <nl> */ <nl> - void __init proc_comm_boot_wait ( void ) <nl> + void __devinit proc_comm_boot_wait ( void ) <nl> { <nl> void __iomem * base = MSM_SHARED_RAM_BASE ; <nl> 
static char * hidpp_get_unifying_name ( struct hidpp_device * hidpp_dev ) <nl>  <nl> len = response . rap . params [ 1 ]; <nl>  <nl> + if ( 2 + len > sizeof ( response . rap . params )) <nl> + return NULL ; <nl> + <nl> name = kzalloc ( len + 1 , GFP_KERNEL ); <nl> if (! name ) <nl> return NULL ;
static int snd_fm801_free ( struct fm801 * chip ) <nl> cmdw |= 0x00c3 ; <nl> fm801_writew ( chip , IRQ_MASK , cmdw ); <nl>  <nl> + devm_free_irq (& chip -> pci -> dev , chip -> irq , chip ); <nl> + <nl> __end_hw : <nl> # ifdef CONFIG_SND_FM801_TEA575X_BOOL <nl> if (!( chip -> tea575x_tuner & TUNER_DISABLED )) {
static int omap_i2c_remove ( struct platform_device * pdev ) <nl> return ret ; <nl>  <nl> omap_i2c_write_reg ( omap , OMAP_I2C_CON_REG , 0 ); <nl> - pm_runtime_put (& pdev -> dev ); <nl> + pm_runtime_put_sync (& pdev -> dev ); <nl> pm_runtime_disable (& pdev -> dev ); <nl> return 0 ; <nl> }
int reiserfs_acl_chmod ( struct inode * inode ) <nl> return 0 ; <nl> } <nl>  <nl> + reiserfs_write_unlock ( inode -> i_sb ); <nl> acl = reiserfs_get_acl ( inode , ACL_TYPE_ACCESS ); <nl> + reiserfs_write_lock ( inode -> i_sb ); <nl> if (! acl ) <nl> return 0 ; <nl> if ( IS_ERR ( acl ))
static long intel_vgpu_ioctl ( struct mdev_device * mdev , unsigned int cmd , <nl> sparse -> areas [ 0 ]. offset = <nl> PAGE_ALIGN ( vgpu_aperture_offset ( vgpu )); <nl> sparse -> areas [ 0 ]. size = vgpu_aperture_sz ( vgpu ); <nl> - if (! caps . buf ) { <nl> - kfree ( caps . buf ); <nl> - caps . buf = NULL ; <nl> - caps . size = 0 ; <nl> - } <nl> break ; <nl>  <nl> case VFIO_PCI_BAR3_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX :
static inline void dec_valid_block_count ( struct f2fs_sb_info * sbi , <nl> static inline void inc_page_count ( struct f2fs_sb_info * sbi , int count_type ) <nl> { <nl> percpu_counter_inc (& sbi -> nr_pages [ count_type ]); <nl> + <nl> + if ( count_type == F2FS_DIRTY_DATA || count_type == F2FS_INMEM_PAGES ) <nl> + return ; <nl> + <nl> set_sbi_flag ( sbi , SBI_IS_DIRTY ); <nl> } <nl> 
static long lineevent_ioctl ( struct file * filep , unsigned int cmd , <nl> if ( cmd == GPIOHANDLE_GET_LINE_VALUES_IOCTL ) { <nl> int val ; <nl>  <nl> + memset (& ghd , 0 , sizeof ( ghd )); <nl> + <nl> val = gpiod_get_value_cansleep ( le -> desc ); <nl> if ( val < 0 ) <nl> return val ;
int max8998_irq_init ( struct max8998_dev * max8998 ) <nl>  <nl> void max8998_irq_exit ( struct max8998_dev * max8998 ) <nl> { <nl> + if ( max8998 -> ono ) <nl> + free_irq ( max8998 -> ono , max8998 ); <nl> + <nl> if ( max8998 -> irq ) <nl> free_irq ( max8998 -> irq , max8998 ); <nl> }
int btrfs_open_devices ( struct btrfs_fs_devices * fs_devices , <nl> goto error_brelse ; <nl>  <nl> transid = btrfs_super_generation ( disk_super ); <nl> - if ( transid > latest_transid ) { <nl> + if (! latest_transid || transid > latest_transid ) { <nl> latest_devid = devid ; <nl> latest_transid = transid ; <nl> latest_bdev = bdev ;
static int idmouse_probe ( struct usb_interface * interface , <nl> if ( iface_desc -> desc . bInterfaceClass != 0x0A ) <nl> return - ENODEV ; <nl>  <nl> + if ( iface_desc -> desc . bNumEndpoints < 1 ) <nl> + return - ENODEV ; <nl> + <nl> /* allocate memory for our device state and initialize it */ <nl> dev = kzalloc ( sizeof (* dev ), GFP_KERNEL ); <nl> if ( dev == NULL )
static u32 crc32c_vpmsum ( u32 crc , unsigned char const * p , size_t len ) <nl> } <nl>  <nl> if ( len & ~ VMX_ALIGN_MASK ) { <nl> + preempt_disable (); <nl> pagefault_disable (); <nl> enable_kernel_altivec (); <nl> crc = __crc32c_vpmsum ( crc , p , len & ~ VMX_ALIGN_MASK ); <nl> + disable_kernel_altivec (); <nl> pagefault_enable (); <nl> + preempt_enable (); <nl> } <nl>  <nl> tail = len & VMX_ALIGN_MASK ;
nfqnl_recv_config ( struct sock * ctnl , struct sk_buff * skb , <nl>  <nl> if ( nfqa [ NFQA_CFG_PARAMS - 1 ]) { <nl> struct nfqnl_msg_config_params * params ; <nl> - params = NFA_DATA ( nfqa [ NFQA_CFG_PARAMS - 1 ]); <nl>  <nl> + if (! queue ) { <nl> + ret = - ENOENT ; <nl> + goto out_put ; <nl> + } <nl> + params = NFA_DATA ( nfqa [ NFQA_CFG_PARAMS - 1 ]); <nl> nfqnl_set_mode ( queue , params -> copy_mode , <nl> ntohl ( params -> copy_range )); <nl> }
static int generic_set_freq ( struct dvb_frontend * fe , u32 freq /* in HZ */, <nl> offset += 200000 ; <nl> } <nl> # endif <nl> + break ; <nl> default : <nl> tuner_err (" Unsupported tuner type % d .\ n ", new_type ); <nl> break ;
int sst_hsw_stream_get_volume ( struct sst_hsw * hsw , struct sst_hsw_stream * stream <nl> return - EINVAL ; <nl>  <nl> sst_dsp_read ( hsw -> dsp , volume , <nl> - stream -> reply . volume_register_address [ channel ], sizeof ( volume )); <nl> + stream -> reply . volume_register_address [ channel ], <nl> + sizeof (* volume )); <nl>  <nl> return 0 ; <nl> }
static int do_garbage_collect ( struct f2fs_sb_info * sbi , <nl>  <nl> for ( segno = start_segno ; segno < end_segno ; segno ++) { <nl>  <nl> - if ( get_valid_blocks ( sbi , segno , 1 ) == 0 ) <nl> + if ( get_valid_blocks ( sbi , segno , 1 ) == 0 || <nl> + unlikely ( f2fs_cp_error ( sbi ))) <nl> goto next ; <nl>  <nl> /* find segment summary of victim */
static ssize_t gt_max_freq_mhz_show ( struct device * kdev , struct device_attribute <nl> int ret ; <nl>  <nl> mutex_lock (& dev_priv -> rps . hw_lock ); <nl> - ret = dev_priv -> rps . hw_max * GT_FREQUENCY_MULTIPLIER ; <nl> + ret = dev_priv -> rps . max_delay * GT_FREQUENCY_MULTIPLIER ; <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> return snprintf ( buf , PAGE_SIZE , "% d \ n ", ret );
static void omap_pcm_limit_supported_formats ( void ) <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < SNDRV_PCM_FORMAT_LAST ; i ++) { <nl> + for ( i = 0 ; i <= SNDRV_PCM_FORMAT_LAST ; i ++) { <nl> switch ( snd_pcm_format_physical_width ( i )) { <nl> case 8 : <nl> case 16 :
static int vxlan_newlink ( struct net * net , struct net_device * dev , <nl>  <nl> if (! tb [ IFLA_MTU ]) <nl> dev -> mtu = lowerdev -> mtu - VXLAN_HEADROOM ; <nl> + <nl> + /* update header length based on lower device */ <nl> + dev -> hard_header_len = lowerdev -> hard_header_len + <nl> + VXLAN_HEADROOM ; <nl> } <nl>  <nl> if ( data [ IFLA_VXLAN_TOS ])
bnad_get_strings ( struct net_device * netdev , u32 stringset , u8 * string ) <nl> for ( i = 0 ; i < BNAD_ETHTOOL_STATS_NUM ; i ++) { <nl> BUG_ON (!( strlen ( bnad_net_stats_strings [ i ]) < <nl> ETH_GSTRING_LEN )); <nl> - memcpy ( string , bnad_net_stats_strings [ i ], <nl> - ETH_GSTRING_LEN ); <nl> + strncpy ( string , bnad_net_stats_strings [ i ], <nl> + ETH_GSTRING_LEN ); <nl> string += ETH_GSTRING_LEN ; <nl> } <nl> bmap = bna_tx_rid_mask (& bnad -> bna );
static void atl1_free_ring_resources ( struct atl1_adapter * adapter ) <nl>  <nl> rrd_ring -> desc = NULL ; <nl> rrd_ring -> dma = 0 ; <nl> + <nl> + adapter -> cmb . dma = 0 ; <nl> + adapter -> cmb . cmb = NULL ; <nl> + <nl> + adapter -> smb . dma = 0 ; <nl> + adapter -> smb . smb = NULL ; <nl> } <nl>  <nl> static void atl1_setup_mac_ctrl ( struct atl1_adapter * adapter )
svga3dsurface_get_mip_size ( surf_size_struct base_level , u32 mip_level ) <nl> size . width = max_t ( u32 , base_level . width >> mip_level , 1 ); <nl> size . height = max_t ( u32 , base_level . height >> mip_level , 1 ); <nl> size . depth = max_t ( u32 , base_level . depth >> mip_level , 1 ); <nl> + size . pad64 = 0 ; <nl> + <nl> return size ; <nl> } <nl> 
static int soc_camera_close ( struct file * file ) <nl> pm_runtime_suspend (& icd -> vdev -> dev ); <nl> pm_runtime_disable (& icd -> vdev -> dev ); <nl>  <nl> - ici -> ops -> remove ( icd ); <nl> if ( ici -> ops -> init_videobuf2 ) <nl> vb2_queue_release (& icd -> vb2_vidq ); <nl> + ici -> ops -> remove ( icd ); <nl>  <nl> soc_camera_power_off ( icd , icl ); <nl> }
u32 __tcp_select_window ( struct sock * sk ) <nl> int full_space = min_t ( int , tp -> window_clamp , allowed_space ); <nl> int window ; <nl>  <nl> - if ( mss > full_space ) <nl> + if ( unlikely ( mss > full_space )) { <nl> mss = full_space ; <nl> - <nl> + if ( mss <= 0 ) <nl> + return 0 ; <nl> + } <nl> if ( free_space < ( full_space >> 1 )) { <nl> icsk -> icsk_ack . quick = 0 ; <nl> 
EXPORT_SYMBOL ( set_memory_array_uc ); <nl>  <nl> int _set_memory_wc ( unsigned long addr , int numpages ) <nl> { <nl> - return change_page_attr_set (& addr , numpages , <nl> + int ret ; <nl> + ret = change_page_attr_set (& addr , numpages , <nl> + __pgprot ( _PAGE_CACHE_UC_MINUS ), 0 ); <nl> + <nl> + if (! ret ) { <nl> + ret = change_page_attr_set (& addr , numpages , <nl> __pgprot ( _PAGE_CACHE_WC ), 0 ); <nl> + } <nl> + return ret ; <nl> } <nl>  <nl> int set_memory_wc ( unsigned long addr , int numpages )
static void si_apply_state_adjust_rules ( struct radeon_device * rdev , <nl> } <nl> ++ p ; <nl> } <nl> + /* limit mclk on all R7 370 parts for stability */ <nl> + if ( rdev -> pdev -> device == 0x6811 && <nl> + rdev -> pdev -> revision == 0x81 ) <nl> + max_mclk = 120000 ; <nl>  <nl> if ( rps -> vce_active ) { <nl> rps -> evclk = rdev -> pm . dpm . vce_states [ rdev -> pm . dpm . vce_level ]. evclk ;
static int dac33_set_bias_level ( struct snd_soc_codec * codec , <nl> } <nl> break ; <nl> case SND_SOC_BIAS_OFF : <nl> + /* Do not power off , when the codec is already off */ <nl> + if ( codec -> bias_level == SND_SOC_BIAS_OFF ) <nl> + return 0 ; <nl> ret = dac33_hard_power ( codec , 0 ); <nl> if ( ret != 0 ) <nl> return ret ;
static netdev_tx_t eth_start_xmit ( struct sk_buff * skb , <nl> /* Multi frame CDC protocols may store the frame for <nl> * later which is not a dropped frame . <nl> */ <nl> - if ( dev -> port_usb -> supports_multi_frame ) <nl> + if ( dev -> port_usb && <nl> + dev -> port_usb -> supports_multi_frame ) <nl> goto multiframe ; <nl> goto drop ; <nl> }
int simple_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl>  <nl> if ( new_dentry -> d_inode ) { <nl> simple_unlink ( new_dir , new_dentry ); <nl> - if ( they_are_dirs ) <nl> + if ( they_are_dirs ) { <nl> + drop_nlink ( new_dentry -> d_inode ); <nl> drop_nlink ( old_dir ); <nl> + } <nl> } else if ( they_are_dirs ) { <nl> drop_nlink ( old_dir ); <nl> inc_nlink ( new_dir );
out_disable_phy : <nl> out_unregister_bus : <nl> phy_exit ( host -> generic_phy ); <nl> out_host_free : <nl> - devm_kfree ( dev , host ); <nl> ufshcd_set_variant ( hba , NULL ); <nl> out : <nl> return err ;
static unsigned long nid_range ( unsigned long start , unsigned long end , <nl> start += PAGE_SIZE ; <nl> } <nl>  <nl> + if ( start > end ) <nl> + start = end ; <nl> + <nl> return start ; <nl> } <nl> # else
static int ixgbe_open ( struct net_device * netdev ) <nl> int err ; <nl> u32 num_rx_queues = adapter -> num_rx_queues ; <nl>  <nl> + /* disallow open during test */ <nl> + if ( test_bit ( __IXGBE_TESTING , & adapter -> state )) <nl> + return - EBUSY ; <nl> + <nl> try_intr_reinit : <nl> /* allocate transmit descriptors */ <nl> err = ixgbe_setup_all_tx_resources ( adapter );
static int ch7006_encoder_create_resources ( struct drm_encoder * encoder , <nl> drm_mode_create_tv_properties ( dev , NUM_TV_NORMS , ch7006_tv_norm_names ); <nl>  <nl> priv -> scale_property = drm_property_create_range ( dev , 0 , " scale ", 0 , 2 ); <nl> + if (! priv -> scale_property ) <nl> + return - ENOMEM ; <nl>  <nl> drm_object_attach_property (& connector -> base , conf -> tv_select_subconnector_property , <nl> priv -> select_subconnector );
static void bat_iv_ogm_iface_enable ( struct hard_iface * hard_iface ) <nl> { <nl> struct batman_ogm_packet * batman_ogm_packet ; <nl> + uint32_t random_seqno ; <nl> + <nl> + /* randomize initial seqno to avoid collision */ <nl> + get_random_bytes (& random_seqno , sizeof ( random_seqno )); <nl> + atomic_set (& hard_iface -> seqno , random_seqno ); <nl>  <nl> hard_iface -> packet_len = BATMAN_OGM_LEN ; <nl> hard_iface -> packet_buff = kmalloc ( hard_iface -> packet_len , GFP_ATOMIC );
static void dump_dev_cap_flags ( struct mlx4_dev * dev , u32 flags ) <nl> int i ; <nl>  <nl> mlx4_dbg ( dev , " DEV_CAP flags :\ n "); <nl> - for ( i = 0 ; i < 32 ; ++ i ) <nl> + for ( i = 0 ; i < ARRAY_SIZE ( fname ); ++ i ) <nl> if ( fname [ i ] && ( flags & ( 1 << i ))) <nl> mlx4_dbg ( dev , " % s \ n ", fname [ i ]); <nl> }
static int em28xx_dvb_init ( struct em28xx * dev ) <nl> dvb -> i2c_client_demod = client ; <nl>  <nl> /* attach tuner */ <nl> + memset (& si2157_config , 0 , sizeof ( si2157_config )); <nl> si2157_config . fe = dvb -> fe [ 0 ]; <nl> memset (& info , 0 , sizeof ( struct i2c_board_info )); <nl> strlcpy ( info . type , " si2157 ", I2C_NAME_SIZE );
static loff_t fuse_file_llseek ( struct file * file , loff_t offset , int origin ) <nl> mutex_lock (& inode -> i_mutex ); <nl> switch ( origin ) { <nl> case SEEK_END : <nl> + retval = fuse_update_attributes ( inode , NULL , file , NULL ); <nl> + if ( retval ) <nl> + return retval ; <nl> offset += i_size_read ( inode ); <nl> break ; <nl> case SEEK_CUR :
static int wm8904_remove ( struct snd_soc_codec * codec ) <nl>  <nl> wm8904_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> regulator_bulk_free ( ARRAY_SIZE ( wm8904 -> supplies ), wm8904 -> supplies ); <nl> + kfree ( wm8904 -> retune_mobile_texts ); <nl> + kfree ( wm8904 -> drc_texts ); <nl>  <nl> return 0 ; <nl> }
static int mv643xx_eth_shared_probe ( struct platform_device * pdev ) <nl> * Detect hardware parameters . <nl> */ <nl> msp -> t_clk = ( pd != NULL && pd -> t_clk != 0 ) ? pd -> t_clk : 133000000 ; <nl> - msp -> tx_csum_limit = pd -> tx_csum_limit ? pd -> tx_csum_limit : 9 * 1024 ; <nl> + msp -> tx_csum_limit = ( pd != NULL && pd -> tx_csum_limit ) ? <nl> + pd -> tx_csum_limit : 9 * 1024 ; <nl> infer_hw_params ( msp ); <nl>  <nl> platform_set_drvdata ( pdev , msp );
static int tegra_kbc_start ( struct tegra_kbc * kbc ) <nl> /* Reset the KBC controller to clear all previous status .*/ <nl> reset_control_assert ( kbc -> rst ); <nl> udelay ( 100 ); <nl> - reset_control_assert ( kbc -> rst ); <nl> + reset_control_deassert ( kbc -> rst ); <nl> udelay ( 100 ); <nl>  <nl> tegra_kbc_config_pins ( kbc );
long kernel_wait4 ( pid_t upid , int __user * stat_addr , int options , <nl> __WNOTHREAD | __WCLONE | __WALL )) <nl> return - EINVAL ; <nl>  <nl> + /* - INT_MIN is not defined */ <nl> + if ( upid == INT_MIN ) <nl> + return - ESRCH ; <nl> + <nl> if ( upid == - 1 ) <nl> type = PIDTYPE_MAX ; <nl> else if ( upid < 0 ) {
static int vc4_plane_mode_set ( struct drm_plane * plane , <nl> /* Control word */ <nl> vc4_dlist_write ( vc4_state , <nl> SCALER_CTL0_VALID | <nl> + VC4_SET_FIELD ( SCALER_CTL0_RGBA_EXPAND_ROUND , SCALER_CTL0_RGBA_EXPAND ) | <nl> ( format -> pixel_order << SCALER_CTL0_ORDER_SHIFT ) | <nl> ( format -> hvs << SCALER_CTL0_PIXEL_FORMAT_SHIFT ) | <nl> VC4_SET_FIELD ( tiling , SCALER_CTL0_TILING ) |
static int rsi_send_beacon ( struct rsi_common * common ) <nl> skb_pull ( skb , ( 64 - dword_align_bytes )); <nl> if ( rsi_prepare_beacon ( common , skb )) { <nl> rsi_dbg ( ERR_ZONE , " Failed to prepare beacon \ n "); <nl> + dev_kfree_skb ( skb ); <nl> return - EINVAL ; <nl> } <nl> skb_queue_tail (& common -> tx_queue [ MGMT_BEACON_Q ], skb );
static int soc_cleanup_card_resources ( struct snd_soc_card * card ) <nl> if ( card -> remove ) <nl> card -> remove ( card ); <nl>  <nl> + snd_soc_dapm_free (& card -> dapm ); <nl> + <nl> kfree ( card -> rtd ); <nl> snd_card_free ( card -> snd_card ); <nl> return 0 ;
static int moxart_probe ( struct platform_device * pdev ) <nl> goto out ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out ; <nl>  <nl> dma_cap_zero ( mask ); <nl> dma_cap_set ( DMA_SLAVE , mask );
static void smsc_ircc_sir_wait_hw_transmitter_finish ( struct smsc_ircc_cb * self ) <nl> while ( count -- > 0 && !( inb ( iobase + UART_LSR ) & UART_LSR_TEMT )) <nl> udelay ( 1 ); <nl>  <nl> - if ( count == 0 ) <nl> + if ( count < 0 ) <nl> IRDA_DEBUG ( 0 , "% s (): stuck transmitter \ n ", __func__ ); <nl> } <nl> 
iblock_execute_rw ( struct se_cmd * cmd , struct scatterlist * sgl , u32 sgl_nents , <nl> sg_num --; <nl> } <nl>  <nl> - if ( cmd -> prot_type ) { <nl> + if ( cmd -> prot_type && dev -> dev_attrib . pi_prot_type ) { <nl> int rc = iblock_alloc_bip ( cmd , bio_start ); <nl> if ( rc ) <nl> goto fail_put_bios ;
static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> . mc = gp100_mc_new , <nl> + . pci = gp100_pci_new , <nl> . top = gk104_top_new , <nl> }; <nl> 
static int agpioc_info_wrap ( struct agp_file_private * priv , void __user * arg ) <nl>  <nl> agp_copy_info ( agp_bridge , & kerninfo ); <nl>  <nl> + memset (& userinfo , 0 , sizeof ( userinfo )); <nl> userinfo . version . major = kerninfo . version . major ; <nl> userinfo . version . minor = kerninfo . version . minor ; <nl> userinfo . bridge_id = kerninfo . device -> vendor |
int CIFSFindNext ( const int xid , struct cifs_tcon * tcon , <nl> T2_FNEXT_RSP_PARMS * parms ; <nl> char * response_data ; <nl> int rc = 0 ; <nl> - int bytes_returned , name_len ; <nl> + int bytes_returned ; <nl> + unsigned int name_len ; <nl> __u16 params , byte_count ; <nl>  <nl> cFYI ( 1 , " In FindNext ");
static int goldfish_audio_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_misc_register_failed : <nl> + free_irq ( data -> irq , data ); <nl> err_request_irq_failed : <nl> dma_free_coherent (& pdev -> dev , COMBINED_BUFFER_SIZE , <nl> data -> buffer_virt , data -> buffer_phys );
int __init acpi_table_init ( void ) <nl>  <nl> static int __init acpi_parse_apic_instance ( char * str ) <nl> { <nl> + if (! str ) <nl> + return - EINVAL ; <nl>  <nl> acpi_apic_instance = simple_strtoul ( str , NULL , 0 ); <nl> 
static void arm_ccn_pmu_xp_dt_config ( struct perf_event * event , int enable ) <nl> struct arm_ccn_component * xp ; <nl> u32 val , dt_cfg ; <nl>  <nl> + /* Nothing to do for cycle counter */ <nl> + if ( hw -> idx == CCN_IDX_PMU_CYCLE_COUNTER ) <nl> + return ; <nl> + <nl> if ( CCN_CONFIG_TYPE ( event -> attr . config ) == CCN_TYPE_XP ) <nl> xp = & ccn -> xp [ CCN_CONFIG_XP ( event -> attr . config )]; <nl> else
static const struct soc_device_attribute gen3_soc_whitelist [] = { <nl> /* generic ones */ <nl> { . soc_id = " r8a7795 " }, <nl> { . soc_id = " r8a7796 " }, <nl> + { . soc_id = " r8a77980 " }, <nl> { . soc_id = " r8a77995 " }, <nl> { /* sentinel */ } <nl> };
static int usbat_probe ( struct usb_interface * intf , <nl> us -> transport_name = " Shuttle USBAT "; <nl> us -> transport = usbat_flash_transport ; <nl> us -> transport_reset = usb_stor_CB_reset ; <nl> - us -> max_lun = 1 ; <nl> + us -> max_lun = 0 ; <nl>  <nl> result = usb_stor_probe2 ( us ); <nl> return result ;
int iwl_trans_pcie_dyn_txq_alloc ( struct iwl_trans * trans , <nl> rsp = ( void *) hcmd . resp_pkt -> data ; <nl> qid = le16_to_cpu ( rsp -> queue_number ); <nl>  <nl> - if ( qid > ARRAY_SIZE ( trans_pcie -> txq )) { <nl> + if ( qid >= ARRAY_SIZE ( trans_pcie -> txq )) { <nl> WARN_ONCE ( 1 , " queue index % d unsupported ", qid ); <nl> ret = - EIO ; <nl> goto error_free_resp ;
struct snd_usb_endpoint * snd_usb_add_endpoint ( struct snd_usb_audio * chip , <nl> struct snd_usb_endpoint * ep ; <nl> int is_playback = direction == SNDRV_PCM_STREAM_PLAYBACK ; <nl>  <nl> + if ( WARN_ON (! alts )) <nl> + return NULL ; <nl> + <nl> mutex_lock (& chip -> mutex ); <nl>  <nl> list_for_each_entry ( ep , & chip -> ep_list , list ) {
static int acm_probe ( struct usb_interface * intf , <nl> } <nl>  <nl>  <nl> - if ( data_interface -> cur_altsetting -> desc . bNumEndpoints < 2 ) <nl> + if ( data_interface -> cur_altsetting -> desc . bNumEndpoints < 2 || <nl> + control_interface -> cur_altsetting -> desc . bNumEndpoints == 0 ) <nl> return - EINVAL ; <nl>  <nl> epctrl = & control_interface -> cur_altsetting -> endpoint [ 0 ]. desc ;
static int eseqiv_givencrypt ( struct skcipher_givcrypt_request * req ) <nl> if ( err ) <nl> goto out ; <nl>  <nl> - eseqiv_complete2 ( req ); <nl> + if ( giv != req -> giv ) <nl> + eseqiv_complete2 ( req ); <nl>  <nl> out : <nl> return err ;
static void iomd_get_next_sg ( struct scatterlist * sg , struct iomd_dma * idma ) <nl>  <nl> if ( idma -> dma . sg -> length == 0 ) { <nl> if ( idma -> dma . sgcount > 1 ) { <nl> - idma -> dma . sg ++; <nl> + idma -> dma . sg = sg_next ( idma -> dma . sg ); <nl> idma -> dma . sgcount --; <nl> } else { <nl> idma -> dma . sg = NULL ;
static void option_instat_callback ( struct urb * urb ) <nl> dev_dbg ( dev , "% s : type % x req % x \ n ", __func__ , <nl> req_pkt -> bRequestType , req_pkt -> bRequest ); <nl> } <nl> + } else if ( status == - ENOENT || status == - ESHUTDOWN ) { <nl> + dev_dbg ( dev , "% s : urb stopped : % d \ n ", __func__ , status ); <nl> } else <nl> dev_err ( dev , "% s : error % d \ n ", __func__ , status ); <nl> 
validate_event ( struct pmu_hw_events * hw_events , <nl> struct arm_pmu * armpmu = to_arm_pmu ( event -> pmu ); <nl> struct pmu * leader_pmu = event -> group_leader -> pmu ; <nl>  <nl> + if ( is_software_event ( event )) <nl> + return 1 ; <nl> + <nl> if ( event -> pmu != leader_pmu || event -> state < PERF_EVENT_STATE_OFF ) <nl> return 1 ; <nl> 
static int eb_copy_relocations ( const struct i915_execbuffer * eb ) <nl> min_t ( u64 , BIT_ULL ( 31 ), size - copied ); <nl>  <nl> if ( __copy_from_user (( char *) relocs + copied , <nl> - ( char *) urelocs + copied , <nl> + ( char __user *) urelocs + copied , <nl> len )) { <nl> kvfree ( relocs ); <nl> err = - EFAULT ;
static u32 atombios_adjust_pll ( struct drm_crtc * crtc , <nl> if ( radeon_crtc -> ss . refdiv ) { <nl> radeon_crtc -> pll_flags |= RADEON_PLL_USE_REF_DIV ; <nl> radeon_crtc -> pll_reference_div = radeon_crtc -> ss . refdiv ; <nl> - if ( rdev -> family >= CHIP_RV770 ) <nl> + if ( ASIC_IS_AVIVO ( rdev ) && <nl> + rdev -> family != CHIP_RS780 && <nl> + rdev -> family != CHIP_RS880 ) <nl> radeon_crtc -> pll_flags |= RADEON_PLL_USE_FRAC_FB_DIV ; <nl> } <nl> }
static int lmp91000_probe ( struct i2c_client * client , <nl> indio_dev -> channels = lmp91000_channels ; <nl> indio_dev -> num_channels = ARRAY_SIZE ( lmp91000_channels ); <nl> indio_dev -> name = LMP91000_DRV_NAME ; <nl> + indio_dev -> dev . parent = & client -> dev ; <nl> indio_dev -> modes = INDIO_DIRECT_MODE ; <nl> i2c_set_clientdata ( client , indio_dev ); <nl> 
void kvm_check_async_pf_completion ( struct kvm_vcpu * vcpu ) <nl> spin_unlock (& vcpu -> async_pf . lock ); <nl>  <nl> kvm_arch_async_page_ready ( vcpu , work ); <nl> - kvm_arch_async_page_present ( vcpu , work ); <nl> + kvm_async_page_present_async ( vcpu , work ); <nl>  <nl> list_del (& work -> queue ); <nl> vcpu -> async_pf . queued --;
static int octeon_usb_hub_status_data ( struct usb_hcd * hcd , char * buf ) <nl> buf [ 0 ] = 0 ; <nl> buf [ 0 ] = port_status . connect_change << 1 ; <nl>  <nl> - return ( buf [ 0 ] != 0 ); <nl> + return buf [ 0 ] != 0 ; <nl> } <nl>  <nl> static int octeon_usb_hub_control ( struct usb_hcd * hcd , u16 typeReq , u16 wValue , u16 wIndex , char * buf , u16 wLength )
int mdfld_dsi_send_gen_short ( struct mdfld_dsi_pkg_sender * sender , u8 param0 , <nl> unsigned long flags ; <nl> u8 data_type ; <nl>  <nl> - if (! sender || param_num < 0 || param_num > 2 ) { <nl> + if (! sender || param_num > 2 ) { <nl> DRM_ERROR (" Invalid parameter \ n "); <nl> return - EINVAL ; <nl> }
struct vmxnet3_adapter { <nl> struct net_device * netdev ; <nl> struct pci_dev * pdev ; <nl>  <nl> - u8 * hw_addr0 ; /* for BAR 0 */ <nl> - u8 * hw_addr1 ; /* for BAR 1 */ <nl> + u8 __iomem * hw_addr0 ; /* for BAR 0 */ <nl> + u8 __iomem * hw_addr1 ; /* for BAR 1 */ <nl>  <nl> /* feature control */ <nl> bool rxcsum ;
void intel_display_power_get ( struct drm_i915_private * dev_priv , <nl> struct i915_power_well * power_well ; <nl> int i ; <nl>  <nl> + intel_runtime_pm_get ( dev_priv ); <nl> + <nl> power_domains = & dev_priv -> power_domains ; <nl>  <nl> mutex_lock (& power_domains -> lock ); <nl> void intel_display_power_put ( struct drm_i915_private * dev_priv , <nl> } <nl>  <nl> mutex_unlock (& power_domains -> lock ); <nl> + <nl> + intel_runtime_pm_put ( dev_priv ); <nl> } <nl>  <nl> static struct i915_power_domains * hsw_pwr ;
static int __f2fs_setxattr ( struct inode * inode , int index , <nl> goto exit ; <nl> } <nl>  <nl> - if ( f2fs_xattr_value_same ( here , value , size )) <nl> + if ( value && f2fs_xattr_value_same ( here , value , size )) <nl> goto exit ; <nl> } else if (( flags & XATTR_REPLACE )) { <nl> error = - ENODATA ;
beiscsi_create_wrb_rings ( struct beiscsi_hba * phba , <nl> if ( status != 0 ) { <nl> shost_printk ( KERN_ERR , phba -> shost , <nl> " wrbq create failed ."); <nl> + kfree ( pwrb_arr ); <nl> return status ; <nl> } <nl> phwi_ctrlr -> wrb_context [ i * 2 ]. cid = phwi_context -> be_wrbq [ i ].
static void save_microcode_patch ( void * data , unsigned int size ) <nl> p = memdup_patch ( data , size ); <nl> if (! p ) <nl> pr_err (" Error allocating buffer % p \ n ", data ); <nl> - else <nl> + else { <nl> list_replace (& iter -> plist , & p -> plist ); <nl> + kfree ( iter -> data ); <nl> + kfree ( iter ); <nl> + } <nl> } <nl> } <nl> 
int kvm_timer_hyp_init ( void ) <nl> info = arch_timer_get_kvm_info (); <nl> timecounter = & info -> timecounter ; <nl>  <nl> + if (! timecounter -> cc ) { <nl> + kvm_err (" kvm_arch_timer : uninitialized timecounter \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> if ( info -> virtual_irq <= 0 ) { <nl> kvm_err (" kvm_arch_timer : invalid virtual timer IRQ : % d \ n ", <nl> info -> virtual_irq );
static int cnic_start_hw ( struct cnic_dev * dev ) <nl> return 0 ; <nl>  <nl> err1 : <nl> - cp -> free_resc ( dev ); <nl> + if ( ethdev -> drv_state & CNIC_DRV_STATE_HANDLES_IRQ ) <nl> + cp -> stop_hw ( dev ); <nl> + else <nl> + cp -> free_resc ( dev ); <nl> pci_dev_put ( dev -> pcidev ); <nl> return err ; <nl> }
dcb_outp_parse ( struct nouveau_bios * bios , u8 idx , u8 * ver , u8 * len , <nl> struct dcb_output * outp ) <nl> { <nl> u16 dcb = dcb_outp ( bios , idx , ver , len ); <nl> + memset ( outp , 0x00 , sizeof (* outp )); <nl> if ( dcb ) { <nl> if (* ver >= 0x20 ) { <nl> u32 conn = nv_ro32 ( bios , dcb + 0x00 );
int hugetlb_mcopy_atomic_pte ( struct mm_struct * dst_mm , <nl> return ret ; <nl> out_release_unlock : <nl> spin_unlock ( ptl ); <nl> - out_release_nounlock : <nl> if ( vm_shared ) <nl> unlock_page ( page ); <nl> + out_release_nounlock : <nl> put_page ( page ); <nl> goto out ; <nl> }
static int __init kvmppc_e500_init ( void ) <nl> return kvm_init ( NULL , sizeof ( struct kvmppc_vcpu_e500 ), 0 , THIS_MODULE ); <nl> } <nl>  <nl> - static void __init kvmppc_e500_exit ( void ) <nl> + static void __exit kvmppc_e500_exit ( void ) <nl> { <nl> kvmppc_booke_exit (); <nl> }
static irqreturn_t sil24_interrupt ( int irq , void * dev_instance , struct pt_regs * <nl>  <nl> status = readl ( hpriv -> host_base + HOST_IRQ_STAT ); <nl>  <nl> + if ( status == 0xffffffff ) { <nl> + printk ( KERN_ERR DRV_NAME ": IRQ status == 0xffffffff , " <nl> + " PCI fault or device removal ?\ n "); <nl> + goto out ; <nl> + } <nl> + <nl> if (!( status & IRQ_STAT_4PORTS )) <nl> goto out ; <nl> 
static int fsl_elbc_chip_remove ( struct fsl_elbc_mtd * priv ) <nl>  <nl> elbc_fcm_ctrl -> chips [ priv -> bank ] = NULL ; <nl> kfree ( priv ); <nl> - kfree ( elbc_fcm_ctrl ); <nl> return 0 ; <nl> } <nl> 
int cap_bprm_set_creds ( struct linux_binprm * bprm ) <nl> } <nl> skip : <nl>  <nl> + /* if we have fs caps , clear dangerous personality flags */ <nl> + if (! cap_issubset ( new -> cap_permitted , old -> cap_permitted )) <nl> + bprm -> per_clear |= PER_CLEAR_ON_SETID ; <nl> + <nl> + <nl> /* Don ' t let someone trace a set [ ug ] id / setpcap binary with the revised <nl> * credentials unless they have the appropriate permit <nl> */
int dso__load_sym ( struct dso * dso , struct map * map , const char * name , int fd , <nl> goto new_symbol ; <nl> } <nl>  <nl> - if ( curr_dso -> adjust_symbols ) { <nl> + if ( curr_dso -> adjust_symbols && sym . st_value ) { <nl> pr_debug4 ("% s : adjusting symbol : st_value : %#" PRIx64 " " <nl> " sh_addr : %#" PRIx64 " sh_offset : %#" PRIx64 "\ n ", __func__ , <nl> ( u64 ) sym . st_value , ( u64 ) shdr . sh_addr ,
static void imon_incoming_packet ( struct imon_context * ictx , <nl> if ( press_type == 0 ) <nl> rc_keyup ( ictx -> rdev ); <nl> else { <nl> - if ( ictx -> rc_type == RC_BIT_RC6_MCE ) <nl> + if ( ictx -> rc_type == RC_BIT_RC6_MCE || <nl> + ictx -> rc_type == RC_BIT_OTHER ) <nl> rc_keydown ( ictx -> rdev , <nl> ictx -> rc_type == RC_BIT_RC6_MCE ? RC_TYPE_RC6_MCE : RC_TYPE_OTHER , <nl> ictx -> rc_scancode , ictx -> rc_toggle );
unsigned long __init lmb_alloc_base ( unsigned long size , unsigned long align , <nl>  <nl> alloc = __lmb_alloc_base ( size , align , max_addr ); <nl>  <nl> - if ( alloc < 0 ) <nl> + if ( alloc == 0 ) <nl> panic (" ERROR : Failed to allocate 0x % lx bytes below 0x % lx .\ n ", <nl> size , max_addr ); <nl> 
static int raw_cmd_copyout ( int cmd , void __user * param , <nl> int ret ; <nl>  <nl> while ( ptr ) { <nl> - ret = copy_to_user ( param , ptr , sizeof (* ptr )); <nl> + struct floppy_raw_cmd cmd = * ptr ; <nl> + cmd . next = NULL ; <nl> + cmd . kernel_data = NULL ; <nl> + ret = copy_to_user ( param , & cmd , sizeof ( cmd )); <nl> if ( ret ) <nl> return - EFAULT ; <nl> param += sizeof ( struct floppy_raw_cmd );
static int ieee80211_fragment ( struct ieee80211_tx_data * tx , <nl> } <nl>  <nl> /* adjust first fragment ' s length */ <nl> - skb -> len = hdrlen + per_fragm ; <nl> + skb_trim ( skb , hdrlen + per_fragm ); <nl> return 0 ; <nl> } <nl> 
static int host_start ( struct ci13xxx * ci ) <nl> else <nl> ci -> hcd = hcd ; <nl>  <nl> + if ( ci -> platdata -> flags & CI13XXX_DISABLE_STREAMING ) <nl> + hw_write ( ci , OP_USBMODE , USBMODE_CI_SDIS , USBMODE_CI_SDIS ); <nl> + <nl> return ret ; <nl> } <nl> 
int inet_recv_error ( struct sock * sk , struct msghdr * msg , int len , <nl>  <nl> static inline void inet_ctl_sock_destroy ( struct sock * sk ) <nl> { <nl> - sock_release ( sk -> sk_socket ); <nl> + if ( sk ) <nl> + sock_release ( sk -> sk_socket ); <nl> } <nl>  <nl> # endif
bool i40e_dcb_need_reconfig ( struct i40e_pf * pf , <nl> /* Check if APP Table has changed */ <nl> if ( memcmp (& new_cfg -> app , <nl> & old_cfg -> app , <nl> - sizeof ( new_cfg -> app ))) <nl> + sizeof ( new_cfg -> app ))) { <nl> need_reconfig = true ; <nl> dev_info (& pf -> pdev -> dev , " APP Table change detected .\ n "); <nl> + } <nl>  <nl> return need_reconfig ; <nl> }
int ieee80211_register_hw ( struct ieee80211_hw * hw ) <nl> * and we need some headroom for passing the frame to monitor <nl> * interfaces , but never both at the same time . <nl> */ <nl> - local -> tx_headroom = max ( local -> hw . extra_tx_headroom , <nl> - sizeof ( struct ieee80211_tx_status_rtap_hdr )); <nl> + local -> tx_headroom = max_t ( unsigned int , local -> hw . extra_tx_headroom , <nl> + sizeof ( struct ieee80211_tx_status_rtap_hdr )); <nl>  <nl> debugfs_hw_add ( local ); <nl> 
static inline u8 rc5_data ( struct rc_map_table * key ) <nl> return key -> scancode & 0xff ; <nl> } <nl>  <nl> - static inline u8 rc5_scan ( struct rc_map_table * key ) <nl> + static inline u16 rc5_scan ( struct rc_map_table * key ) <nl> { <nl> return key -> scancode & 0xffff ; <nl> }
static int perf_sched__read_events ( struct perf_sched * sched ) <nl> struct perf_data_file file = { <nl> . path = input_name , <nl> . mode = PERF_DATA_MODE_READ , <nl> + . force = sched -> force , <nl> }; <nl> int rc = - 1 ; <nl> 
static struct omap_system_dma_plat_info dma_plat_info __initdata = { <nl> . dma_read = dma_read , <nl> }; <nl>  <nl> - static struct platform_device_info omap_dma_dev_info = { <nl> + static struct platform_device_info omap_dma_dev_info __initdata = { <nl> . name = " omap - dma - engine ", <nl> . id = - 1 , <nl> . dma_mask = DMA_BIT_MASK ( 32 ),
struct btrfs_root * open_ctree ( struct super_block * sb , <nl> kfree ( tree_root ); <nl> bdi_destroy (& fs_info -> bdi ); <nl> kfree ( fs_info ); <nl> + kfree ( chunk_root ); <nl> + kfree ( dev_root ); <nl> return ERR_PTR ( err ); <nl> } <nl> 
static void handle_keypress ( int c ) <nl> switch ( c ) { <nl> case ' d ': <nl> prompt_integer (& delay_secs , " Enter display delay "); <nl> + if ( delay_secs < 1 ) <nl> + delay_secs = 1 ; <nl> break ; <nl> case ' e ': <nl> prompt_integer (& print_entries , " Enter display entries ( lines )");
int vcc_recvmsg ( struct kiocb * iocb , struct socket * sock , struct msghdr * msg , <nl> struct sk_buff * skb ; <nl> int copied , error = - EINVAL ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> if ( sock -> state != SS_CONNECTED ) <nl> return - ENOTCONN ; <nl> 
static int exec_drive_taskfile ( struct driver_data * dd , <nl> fis . device ); <nl>  <nl> /* check for erase mode support during secure erase .*/ <nl> - if (( fis . command == ATA_CMD_SEC_ERASE_UNIT ) <nl> - && ( outbuf [ 0 ] & MTIP_SEC_ERASE_MODE )) { <nl> + if (( fis . command == ATA_CMD_SEC_ERASE_UNIT ) && outbuf && <nl> + ( outbuf [ 0 ] & MTIP_SEC_ERASE_MODE )) { <nl> erasemode = 1 ; <nl> } <nl> 
static struct ctl_table sctp_net_table [] = { <nl> . mode = 0644 , <nl> . proc_handler = proc_sctp_do_auth , <nl> }, <nl> + { <nl> + . procname = " intl_enable ", <nl> + . data = & init_net . sctp . intl_enable , <nl> + . maxlen = sizeof ( int ), <nl> + . mode = 0644 , <nl> + . proc_handler = proc_dointvec , <nl> + }, <nl> { <nl> . procname = " addr_scope_policy ", <nl> . data = & init_net . sctp . scope_policy ,
i2c_dw_xfer ( struct i2c_adapter * adap , struct i2c_msg msgs [], int num ) <nl> i2c_dw_xfer_init ( dev ); <nl>  <nl> /* wait for tx to complete */ <nl> - if (! wait_for_completion_timeout (& dev -> cmd_complete , HZ )) { <nl> + if (! wait_for_completion_timeout (& dev -> cmd_complete , adap -> timeout )) { <nl> dev_err ( dev -> dev , " controller timed out \ n "); <nl> /* i2c_dw_init implicitly disables the adapter */ <nl> i2c_dw_init ( dev );
int __ceph_caps_used ( struct ceph_inode_info * ci ) <nl> used |= CEPH_CAP_PIN ; <nl> if ( ci -> i_rd_ref ) <nl> used |= CEPH_CAP_FILE_RD ; <nl> - if ( ci -> i_rdcache_ref || ci -> i_rdcache_gen ) <nl> + if ( ci -> i_rdcache_ref || ci -> vfs_inode . i_data . nrpages ) <nl> used |= CEPH_CAP_FILE_CACHE ; <nl> if ( ci -> i_wr_ref ) <nl> used |= CEPH_CAP_FILE_WR ;
static int adis16400_read_raw ( struct iio_dev * indio_dev , <nl> * val = st -> variant -> temp_scale_nano / 1000000 ; <nl> * val2 = ( st -> variant -> temp_scale_nano % 1000000 ); <nl> return IIO_VAL_INT_PLUS_MICRO ; <nl> + case IIO_PRESSURE : <nl> + /* 20 uBar = 0 . 002kPascal */ <nl> + * val = 0 ; <nl> + * val2 = 2000 ; <nl> + return IIO_VAL_INT_PLUS_MICRO ; <nl> default : <nl> return - EINVAL ; <nl> }
__append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> void append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> struct symbol ** syms ) <nl> { <nl> + if (! chain -> nr ) <nl> + return ; <nl> __append_chain_children ( root , chain , syms , 0 ); <nl> }
static void ixgbe_configure_dcb ( struct ixgbe_adapter * adapter ) <nl> if ( hw -> mac . type == ixgbe_mac_82598EB ) <nl> netif_set_gso_max_size ( adapter -> netdev , 32768 ); <nl>  <nl> - ixgbe_dcb_check_config (& adapter -> dcb_cfg ); <nl> ixgbe_dcb_calculate_tc_credits (& adapter -> dcb_cfg , DCB_TX_CONFIG ); <nl> ixgbe_dcb_calculate_tc_credits (& adapter -> dcb_cfg , DCB_RX_CONFIG ); <nl> 
static int r8a66597_hub_status_data ( struct usb_hcd * hcd , char * buf ) <nl> static void r8a66597_hub_descriptor ( struct r8a66597 * r8a66597 , <nl> struct usb_hub_descriptor * desc ) <nl> { <nl> - desc -> bDescriptorType = 0x29 ; <nl> + desc -> bDescriptorType = USB_DT_HUB ; <nl> desc -> bHubContrCurrent = 0 ; <nl> desc -> bNbrPorts = r8a66597 -> max_root_hub ; <nl> desc -> bDescLength = 9 ;
static inline int xfrm_replay_verify_len ( struct xfrm_replay_state_esn * replay_es <nl> if ( nla_len ( rp ) < ulen || xfrm_replay_state_esn_len ( replay_esn ) != ulen ) <nl> return - EINVAL ; <nl>  <nl> + if ( up -> replay_window > up -> bmp_len * sizeof ( __u32 ) * 8 ) <nl> + return - EINVAL ; <nl> + <nl> return 0 ; <nl> } <nl> 
flush_signal_handlers ( struct task_struct * t , int force_default ) <nl> if ( force_default || ka -> sa . sa_handler != SIG_IGN ) <nl> ka -> sa . sa_handler = SIG_DFL ; <nl> ka -> sa . sa_flags = 0 ; <nl> +# ifdef SA_RESTORER <nl> + ka -> sa . sa_restorer = NULL ; <nl> +# endif <nl> sigemptyset (& ka -> sa . sa_mask ); <nl> ka ++; <nl> }
static int quota_setinfo ( struct super_block * sb , int type , void __user * addr ) <nl>  <nl> static void copy_to_if_dqblk ( struct if_dqblk * dst , struct fs_disk_quota * src ) <nl> { <nl> + memset ( dst , 0 , sizeof (* dst )); <nl> dst -> dqb_bhardlimit = src -> d_blk_hardlimit ; <nl> dst -> dqb_bsoftlimit = src -> d_blk_softlimit ; <nl> dst -> dqb_curspace = src -> d_bcount ;
static int drbg_generate_long ( struct drbg_state * drbg , <nl> if ( 0 >= tmplen ) <nl> return tmplen ; <nl> len += tmplen ; <nl> - } while ( slice > 0 ); <nl> + } while ( slice > 0 && ( len < buflen )); <nl> return len ; <nl> } <nl> 
static int _mei_irq_thread_read ( struct mei_device * dev , s32 * slots , <nl> struct mei_cl * cl , <nl> struct mei_io_list * cmpl_list ) <nl> { <nl> - if ((* slots * sizeof ( u32 )) >= ( sizeof ( struct mei_msg_hdr ) + <nl> + if ((* slots * sizeof ( u32 )) < ( sizeof ( struct mei_msg_hdr ) + <nl> sizeof ( struct hbm_flow_control ))) { <nl> /* return the cancel routine */ <nl> list_del (& cb_pos -> cb_list );
static int i915_display_info ( struct seq_file * m , void * unused ) <nl> x , y , crtc -> cursor_addr , <nl> yesno ( active )); <nl> } <nl> + <nl> + seq_printf ( m , "\ tunderrun reporting : cpu =% s pch =% s \ n ", <nl> + yesno (! crtc -> cpu_fifo_underrun_disabled ), <nl> + yesno (! crtc -> pch_fifo_underrun_disabled )); <nl> } <nl>  <nl> seq_printf ( m , "\ n ");
int kvm_set_msr_common ( struct kvm_vcpu * vcpu , struct msr_data * msr_info ) <nl> /* ... but clean it before doing the actual write */ <nl> vcpu -> arch . time_offset = data & ~( PAGE_MASK | 1 ); <nl>  <nl> + /* Check that the address is 32 - byte aligned . */ <nl> + if ( vcpu -> arch . time_offset & <nl> + ( sizeof ( struct pvclock_vcpu_time_info ) - 1 )) <nl> + break ; <nl> + <nl> vcpu -> arch . time_page = <nl> gfn_to_page ( vcpu -> kvm , data >> PAGE_SHIFT ); <nl> 
i915_gem_execbuffer2 ( struct drm_device * dev , void * data , <nl> struct drm_i915_gem_exec_object2 * exec2_list = NULL ; <nl> int ret ; <nl>  <nl> - if ( args -> buffer_count < 1 ) { <nl> + if ( args -> buffer_count < 1 || <nl> + args -> buffer_count > UINT_MAX / sizeof (* exec2_list )) { <nl> DRM_DEBUG (" execbuf2 with % d buffers \ n ", args -> buffer_count ); <nl> return - EINVAL ; <nl> }
static int llc_ui_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> int target ; /* Read at least this many bytes */ <nl> long timeo ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> lock_sock ( sk ); <nl> copied = - ENOTCONN ; <nl> if ( unlikely ( sk -> sk_type == SOCK_STREAM && sk -> sk_state == TCP_LISTEN ))
xfs_rmap_convert_shared ( <nl> */ <nl> error = xfs_rmap_lookup_le_range ( cur , bno , owner , offset , flags , <nl> & PREV , & i ); <nl> + if ( error ) <nl> + goto done ; <nl> XFS_WANT_CORRUPTED_GOTO ( mp , i == 1 , done ); <nl>  <nl> ASSERT ( PREV . rm_offset <= offset );
static ssize_t bat_socket_read ( struct file * file , char __user * buf , <nl>  <nl> spin_unlock_bh (& socket_client -> lock ); <nl>  <nl> - error = copy_to_user ( buf , & socket_packet -> icmp_packet , <nl> - socket_packet -> icmp_len ); <nl> + packet_len = min ( count , socket_packet -> icmp_len ); <nl> + error = copy_to_user ( buf , & socket_packet -> icmp_packet , packet_len ); <nl>  <nl> - packet_len = socket_packet -> icmp_len ; <nl> kfree ( socket_packet ); <nl>  <nl> if ( error )
static int __xen_pcibk_add_pci_dev ( struct xen_pcibk_device * pdev , <nl> /* Publish this device . */ <nl> if (! err ) <nl> err = publish_cb ( pdev , 0 , 0 , PCI_DEVFN ( slot , func ), devid ); <nl> + else <nl> + kfree ( dev_entry ); <nl>  <nl> out : <nl> return err ;
static void kvm_write_wall_clock ( struct kvm * kvm , gpa_t wall_clock ) <nl> */ <nl> getboottime (& boot ); <nl>  <nl> + if ( kvm -> arch . kvmclock_offset ) { <nl> + struct timespec ts = ns_to_timespec ( kvm -> arch . kvmclock_offset ); <nl> + boot = timespec_sub ( boot , ts ); <nl> + } <nl> wc . sec = boot . tv_sec ; <nl> wc . nsec = boot . tv_nsec ; <nl> wc . version = version ;
mountpoint_last ( struct nameidata * nd , struct path * path ) <nl> goto out ; <nl> } <nl> path -> dentry = dentry ; <nl> - path -> mnt = mntget ( nd -> path . mnt ); <nl> + path -> mnt = nd -> path . mnt ; <nl> if ( should_follow_link ( dentry , nd -> flags & LOOKUP_FOLLOW )) <nl> return 1 ; <nl> + mntget ( path -> mnt ); <nl> follow_mount ( path ); <nl> error = 0 ; <nl> out :
xfs_rtfree_range ( <nl> */ <nl> error = xfs_rtfind_forw ( mp , tp , end , mp -> m_sb . sb_rextents - 1 , <nl> & postblock ); <nl> + if ( error ) <nl> + return error ; <nl> /* <nl> * If there are blocks not being freed at the front of the <nl> * old extent , add summary data for them to be allocated .
asmlinkage void math_emulate ( long arg ) <nl> entry_sel_off . offset = FPU_ORIG_EIP ; <nl> entry_sel_off . selector = FPU_CS ; <nl> entry_sel_off . opcode = ( byte1 << 8 ) | FPU_modrm ; <nl> + entry_sel_off . empty = 0 ; <nl>  <nl> FPU_rm = FPU_modrm & 7 ; <nl> 
static inline void get_page ( struct page * page ) <nl> page_ref_inc ( page ); <nl> } <nl>  <nl> + static inline __must_check bool try_get_page ( struct page * page ) <nl> +{ <nl> + page = compound_head ( page ); <nl> + if ( WARN_ON_ONCE ( page_ref_count ( page ) <= 0 )) <nl> + return false ; <nl> + page_ref_inc ( page ); <nl> + return true ; <nl> +} <nl> + <nl> static inline void put_page ( struct page * page ) <nl> { <nl> page = compound_head ( page );
void scatterwalk_map_and_copy ( void * buf , struct scatterlist * sg , <nl> struct scatter_walk walk ; <nl> unsigned int offset = 0 ; <nl>  <nl> + if (! nbytes ) <nl> + return ; <nl> + <nl> for (;;) { <nl> scatterwalk_start (& walk , sg ); <nl> 
tda998x_encoder_init ( struct i2c_client * client , <nl>  <nl> priv -> current_page = 0xff ; <nl> priv -> cec = i2c_new_dummy ( client -> adapter , 0x34 ); <nl> - if (! priv -> cec ) <nl> + if (! priv -> cec ) { <nl> + kfree ( priv ); <nl> return - ENODEV ; <nl> + } <nl> priv -> dpms = DRM_MODE_DPMS_OFF ; <nl>  <nl> encoder_slave -> slave_priv = priv ;
enum xgbe_conn_type { <nl> XGBE_CONN_TYPE_NONE = 0 , <nl> XGBE_CONN_TYPE_SFP , <nl> XGBE_CONN_TYPE_MDIO , <nl> + XGBE_CONN_TYPE_RSVD1 , <nl> XGBE_CONN_TYPE_BACKPLANE , <nl> XGBE_CONN_TYPE_MAX , <nl> }; <nl> static int xgbe_phy_init ( struct xgbe_prv_data * pdata ) <nl> if ( xgbe_phy_conn_type_mismatch ( pdata )) { <nl> dev_err ( pdata -> dev , " phy mode / connection mismatch (%# x /%# x )\ n ", <nl> phy_data -> port_mode , phy_data -> conn_type ); <nl> + return - EINVAL ; <nl> } <nl>  <nl> /* Validate the mode requested */
static void binder_send_failed_reply ( struct binder_transaction * t , <nl> if ( target_thread -> return_error == BR_OK ) { <nl> binder_debug ( BINDER_DEBUG_FAILED_TRANSACTION , <nl> " send failed reply for transaction % d to % d :% d \ n ", <nl> - t -> debug_id , target_thread -> proc -> pid , <nl> + t -> debug_id , <nl> + target_thread -> proc -> pid , <nl> target_thread -> pid ); <nl>  <nl> binder_pop_transaction ( target_thread , t );
static int qat_hal_init_esram ( struct icp_qat_fw_loader_handle * handle ) <nl> unsigned int csr_val ; <nl> int times = 30 ; <nl>  <nl> + if ( handle -> pci_dev -> device == ADF_C3XXX_PCI_DEVICE_ID ) <nl> + return 0 ; <nl> + <nl> csr_val = ADF_CSR_RD ( csr_addr , 0 ); <nl> if (( csr_val & ESRAM_AUTO_TINIT ) && ( csr_val & ESRAM_AUTO_TINIT_DONE )) <nl> return 0 ;
static inline void tcp_check_send_head ( struct sock * sk , struct sk_buff * skb_unli <nl> { <nl> if ( sk -> sk_send_head == skb_unlinked ) <nl> sk -> sk_send_head = NULL ; <nl> + if ( tcp_sk ( sk )-> highest_sack == skb_unlinked ) <nl> + tcp_sk ( sk )-> highest_sack = NULL ; <nl> } <nl>  <nl> static inline void tcp_init_send_head ( struct sock * sk )
static void bdw_load_gamma_lut ( struct drm_crtc_state * state , u32 offset ) <nl> } <nl>  <nl> /* Program the max register to clamp values > 1 . 0 . */ <nl> + i = lut_size - 1 ; <nl> I915_WRITE ( PREC_PAL_GC_MAX ( pipe , 0 ), <nl> drm_color_lut_extract ( lut [ i ]. red , 16 )); <nl> I915_WRITE ( PREC_PAL_GC_MAX ( pipe , 1 ),
static int davinci_pcm_open ( struct snd_pcm_substream * substream ) <nl> int ret = 0 ; <nl>  <nl> snd_soc_set_runtime_hwparams ( substream , & davinci_pcm_hardware ); <nl> + /* ensure that buffer size is a multiple of period size */ <nl> + ret = snd_pcm_hw_constraint_integer ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> prtd = kzalloc ( sizeof ( struct davinci_runtime_data ), GFP_KERNEL ); <nl> if ( prtd == NULL )
static int create_in_format_blob ( struct drm_device * dev , struct drm_plane * plane <nl> plane -> format_types [ j ], <nl> plane -> modifiers [ i ])) { <nl>  <nl> - mod -> formats |= 1 << j ; <nl> + mod -> formats |= 1ULL << j ; <nl> } <nl> } <nl> 
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> tnl_hlen = skb_tnl_header_len ( skb ); <nl> - if ( skb_headroom ( skb ) < ( tnl_hlen + frag_hdr_sz )) { <nl> + if ( skb -> mac_header < ( tnl_hlen + frag_hdr_sz )) { <nl> if ( gso_pskb_expand_head ( skb , tnl_hlen + frag_hdr_sz )) <nl> goto out ; <nl> }
static __devexit int wm831x_power_remove ( struct platform_device * pdev ) <nl> power_supply_unregister (& wm831x_power -> battery ); <nl> power_supply_unregister (& wm831x_power -> wall ); <nl> power_supply_unregister (& wm831x_power -> usb ); <nl> + kfree ( wm831x_power ); <nl> return 0 ; <nl> } <nl> 
iwl_parse_nvm_data ( struct device * dev , const struct iwl_cfg * cfg , <nl> if (! nvm_calib ) { <nl> IWL_ERR_DEV ( dev , <nl> " Can ' t parse empty Calib NVM sections \ n "); <nl> + kfree ( data ); <nl> return NULL ; <nl> } <nl> /* in family 8000 Xtal calibration values moved to OTP */
ath10k_wmi_10_4_gen_update_fw_tdls_state ( struct ath10k * ar , u32 vdev_id , <nl> if (! skb ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> - if ( test_bit ( WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY , ar -> wmi . svc_map )) <nl> + if ( test_bit ( WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY , ar -> wmi . svc_map ) && <nl> + state == WMI_TDLS_ENABLE_ACTIVE ) <nl> state = WMI_TDLS_ENABLE_PASSIVE ; <nl>  <nl> if ( test_bit ( WMI_SERVICE_TDLS_UAPSD_BUFFER_STA , ar -> wmi . svc_map ))
static void bpf_tcp_release ( struct sock * sk ) <nl> psock -> cork = NULL ; <nl> } <nl>  <nl> - sk -> sk_prot = psock -> sk_proto ; <nl> - psock -> sk_proto = NULL ; <nl> + if ( psock -> sk_proto ) { <nl> + sk -> sk_prot = psock -> sk_proto ; <nl> + psock -> sk_proto = NULL ; <nl> + } <nl> out : <nl> rcu_read_unlock (); <nl> }
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> __le16 fc ; <nl> struct ieee80211_hdr hdr ; <nl> struct ieee80211s_hdr mesh_hdr __maybe_unused ; <nl> - struct mesh_path * mppath = NULL ; <nl> + struct mesh_path __maybe_unused * mppath = NULL ; <nl> const u8 * encaps_data ; <nl> int encaps_len , skip_header_bytes ; <nl> int nh_pos , h_pos ;
subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> err = filter_add_subsystem_pred ( system , pred ); <nl> if ( err < 0 ) { <nl> - filter_free_subsystem_preds ( system ); <nl> filter_free_pred ( pred ); <nl> return err ; <nl> }
static int macronix_quad_enable ( struct spi_nor * nor ) <nl> val = read_sr ( nor ); <nl> if ( val < 0 ) <nl> return val ; <nl> + if ( val & SR_QUAD_EN_MX ) <nl> + return 0 ; <nl> + <nl> write_enable ( nor ); <nl>  <nl> write_sr ( nor , val | SR_QUAD_EN_MX );
static int sierra_resume ( struct usb_serial * serial ) <nl> if ( err < 0 ) { <nl> intfdata -> in_flight --; <nl> usb_unanchor_urb ( urb ); <nl> - usb_scuttle_anchored_urbs (& portdata -> delayed ); <nl> - break ; <nl> + kfree ( urb -> transfer_buffer ); <nl> + usb_free_urb ( urb ); <nl> + spin_lock (& portdata -> lock ); <nl> + portdata -> outstanding_urbs --; <nl> + spin_unlock (& portdata -> lock ); <nl> + continue ; <nl> } <nl> } <nl> 
int btrfs_search_slot_for_read ( struct btrfs_root * root , <nl> if ( ret < 0 ) <nl> return ret ; <nl> if (! ret ) { <nl> - p -> slots [ 0 ] = btrfs_header_nritems ( leaf ) - 1 ; <nl> + leaf = p -> nodes [ 0 ]; <nl> + if ( p -> slots [ 0 ] == btrfs_header_nritems ( leaf )) <nl> + p -> slots [ 0 ]--; <nl> return 0 ; <nl> } <nl> if (! return_any )
static int tomoyo_mount_acl ( struct tomoyo_request_info * r , char * dev_name , <nl> } <nl> if ( need_dev ) { <nl> /* Get mount point or device file . */ <nl> - if ( kern_path ( dev_name , LOOKUP_FOLLOW , & path )) { <nl> + if (! dev_name || kern_path ( dev_name , LOOKUP_FOLLOW , & path )) { <nl> error = - ENOENT ; <nl> goto out ; <nl> }
aoenet_xmit ( struct sk_buff_head * queue ) <nl> { <nl> struct sk_buff * skb , * tmp ; <nl>  <nl> - skb_queue_walk_safe ( queue , skb , tmp ) <nl> + skb_queue_walk_safe ( queue , skb , tmp ) { <nl> + __skb_unlink ( skb , queue ); <nl> dev_queue_xmit ( skb ); <nl> + } <nl> } <nl>  <nl> /*
static int ina2xx_probe ( struct i2c_client * client , <nl> data -> config = & ina2xx_config [ data -> kind ]; <nl> data -> client = client ; <nl>  <nl> - if ( data -> rshunt <= 0 ) <nl> + if ( data -> rshunt <= 0 || <nl> + data -> rshunt > data -> config -> calibration_factor ) <nl> return - ENODEV ; <nl>  <nl> ret = ina2xx_init ( data );
int core_kernel_text ( unsigned long addr ) <nl> addr <= ( unsigned long ) _etext ) <nl> return 1 ; <nl>  <nl> - if ( addr >= ( unsigned long ) _sinittext && <nl> + if ( system_state == SYSTEM_BOOTING && <nl> + addr >= ( unsigned long ) _sinittext && <nl> addr <= ( unsigned long ) _einittext ) <nl> return 1 ; <nl> return 0 ;
int dvb_create_media_graph ( struct dvb_adapter * adap , <nl> return - ENOMEM ; <nl> adap -> conn = conn ; <nl>  <nl> - adap -> conn_pads = kcalloc ( 1 , sizeof (* adap -> conn_pads ), <nl> - GFP_KERNEL ); <nl> + adap -> conn_pads = kzalloc ( sizeof (* adap -> conn_pads ), GFP_KERNEL ); <nl> if (! adap -> conn_pads ) <nl> return - ENOMEM ; <nl> 
static int st33zp24_spi_evaluate_latency ( void * phy_id ) <nl> & data , 1 ); <nl> latency ++; <nl> } <nl> + if ( status < 0 ) <nl> + return status ; <nl> + if ( latency == MAX_SPI_LATENCY ) <nl> + return - ENODEV ; <nl> + <nl> return latency - 1 ; <nl> } /* evaluate_latency () */ <nl> 
static int __init alloc_node_page_cgroup ( int nid ) <nl> start_pfn = NODE_DATA ( nid )-> node_start_pfn ; <nl> nr_pages = NODE_DATA ( nid )-> node_spanned_pages ; <nl>  <nl> + if (! nr_pages ) <nl> + return 0 ; <nl> + <nl> table_size = sizeof ( struct page_cgroup ) * nr_pages ; <nl>  <nl> base = __alloc_bootmem_node_nopanic ( NODE_DATA ( nid ),
static int kvm_vm_ioctl_create_vcpu ( struct kvm * kvm , u32 id ) <nl> int r ; <nl> struct kvm_vcpu * vcpu , * v ; <nl>  <nl> + if ( id >= KVM_MAX_VCPUS ) <nl> + return - EINVAL ; <nl> + <nl> vcpu = kvm_arch_vcpu_create ( kvm , id ); <nl> if ( IS_ERR ( vcpu )) <nl> return PTR_ERR ( vcpu );
static void flush_tlb_others_ipi ( const struct cpumask * cpumask , <nl> * We have to send the IPI only to <nl> * CPUs affected . <nl> */ <nl> - send_IPI_mask ( cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl> + send_IPI_mask ( f -> flush_cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl>  <nl> while (! cpumask_empty ( to_cpumask ( f -> flush_cpumask ))) <nl> cpu_relax ();
int cfg80211_get_station ( struct net_device * dev , const u8 * mac_addr , <nl> if (! rdev -> ops -> get_station ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + memset ( sinfo , 0 , sizeof (* sinfo )); <nl> + <nl> return rdev_get_station ( rdev , dev , mac_addr , sinfo ); <nl> } <nl> EXPORT_SYMBOL ( cfg80211_get_station );
static int hba_setup_cid_tbls ( struct beiscsi_hba * phba ) <nl> kfree ( phba -> ep_array ); <nl> phba -> ep_array = NULL ; <nl> ret = - ENOMEM ; <nl> + <nl> + goto free_memory ; <nl> } <nl>  <nl> for ( i = 0 ; i < phba -> params . cxns_per_ctrl ; i ++) {
static struct pci_resource * get_max_resource ( struct pci_resource ** head , u32 siz <nl> temp = temp -> next ; <nl> } <nl>  <nl> - temp -> next = max -> next ; <nl> + if ( temp ) <nl> + temp -> next = max -> next ; <nl> } <nl>  <nl> max -> next = NULL ;
static int hist_entry__srcline_snprintf ( struct hist_entry * self , char * bf , <nl> if ( path != NULL ) <nl> goto out_path ; <nl>  <nl> + if (! self -> ms . map ) <nl> + goto out_ip ; <nl> + <nl> snprintf ( cmd , sizeof ( cmd ), " addr2line - e % s % 016 " PRIx64 , <nl> self -> ms . map -> dso -> long_name , self -> ip ); <nl> fp = popen ( cmd , " r ");
static inline void clear_operand_string ( struct filter_parse_state * ps ) <nl>  <nl> static inline int append_operand_char ( struct filter_parse_state * ps , char c ) <nl> { <nl> - if ( ps -> operand . tail == MAX_FILTER_STR_VAL ) <nl> + if ( ps -> operand . tail == MAX_FILTER_STR_VAL - 1 ) <nl> return - EINVAL ; <nl>  <nl> ps -> operand . string [ ps -> operand . tail ++] = c ;
int cvm_oct_common_init ( struct net_device * dev ) <nl> mac = of_get_mac_address ( priv -> of_node ); <nl>  <nl> if ( mac ) <nl> - memcpy ( dev -> dev_addr , mac , ETH_ALEN ); <nl> + ether_addr_copy ( dev -> dev_addr , mac ); <nl> else <nl> eth_hw_addr_random ( dev ); <nl> 
static void efx_filter_rfs_work ( struct work_struct * data ) <nl> struct efx_channel * channel = efx_get_channel ( efx , req -> rxq_index ); <nl> int rc ; <nl>  <nl> - rc = efx -> type -> filter_insert ( efx , & req -> spec , false ); <nl> + rc = efx -> type -> filter_insert ( efx , & req -> spec , true ); <nl> if ( rc >= 0 ) { <nl> /* Remember this so we can check whether to expire the filter <nl> * later .
void dlm_lowcomms_stop ( void ) <nl> con = __nodeid2con ( i , 0 ); <nl> if ( con ) { <nl> close_connection ( con , true ); <nl> + if ( con -> othercon ) <nl> + kmem_cache_free ( con_cache , con -> othercon ); <nl> kmem_cache_free ( con_cache , con ); <nl> } <nl> }
int ubifs_run_commit ( struct ubifs_info * c ) <nl>  <nl> spin_lock (& c -> cs_lock ); <nl> if ( c -> cmt_state == COMMIT_BROKEN ) { <nl> - err = - EINVAL ; <nl> + err = - EROFS ; <nl> goto out ; <nl> } <nl>  <nl> int ubifs_run_commit ( struct ubifs_info * c ) <nl> * re - check it . <nl> */ <nl> if ( c -> cmt_state == COMMIT_BROKEN ) { <nl> - err = - EINVAL ; <nl> + err = - EROFS ; <nl> goto out_cmt_unlock ; <nl> } <nl> 
void gspca_frame_add ( struct gspca_dev * gspca_dev , <nl> } else { <nl> switch ( gspca_dev -> last_packet_type ) { <nl> case DISCARD_PACKET : <nl> - if ( packet_type == LAST_PACKET ) <nl> + if ( packet_type == LAST_PACKET ) { <nl> gspca_dev -> last_packet_type = packet_type ; <nl> + gspca_dev -> image = NULL ; <nl> + gspca_dev -> image_len = 0 ; <nl> + } <nl> return ; <nl> case LAST_PACKET : <nl> return ;
static void aead_release ( void * private ) <nl> struct aead_tfm * tfm = private ; <nl>  <nl> crypto_free_aead ( tfm -> aead ); <nl> + crypto_put_default_null_skcipher2 (); <nl> kfree ( tfm ); <nl> } <nl>  <nl> static void aead_sock_destruct ( struct sock * sk ) <nl> unsigned int ivlen = crypto_aead_ivsize ( tfm ); <nl>  <nl> af_alg_pull_tsgl ( sk , ctx -> used , NULL , 0 ); <nl> - crypto_put_default_null_skcipher2 (); <nl> sock_kzfree_s ( sk , ctx -> iv , ivlen ); <nl> sock_kfree_s ( sk , ctx , ctx -> len ); <nl> af_alg_release_parent ( sk );
static int usbduxsigma_auto_attach ( struct comedi_device * dev , <nl> } <nl>  <nl> ret = usbduxsigma_alloc_usb_buffers ( dev ); <nl> - if ( ret ) { <nl> - tidy_up ( devpriv ); <nl> + if ( ret ) <nl> return ret ; <nl> - } <nl>  <nl> ret = comedi_load_firmware ( dev , & usb -> dev , FIRMWARE , <nl> usbduxsigma_firmware_upload , 0 );
int btrfs_defrag_file ( struct inode * inode , struct file * file , <nl> i = range -> start >> PAGE_CACHE_SHIFT ; <nl> } <nl> if (! max_to_defrag ) <nl> - max_to_defrag = last_index - 1 ; <nl> + max_to_defrag = last_index ; <nl>  <nl> while ( i <= last_index && defrag_count < max_to_defrag ) { <nl> /*
static int check_leaf ( struct fib_table * tb , struct trie * t , struct leaf * l , <nl>  <nl> if ( fa -> fa_tos && fa -> fa_tos != flp -> flowi4_tos ) <nl> continue ; <nl> + if ( fi -> fib_dead ) <nl> + continue ; <nl> if ( fa -> fa_info -> fib_scope < flp -> flowi4_scope ) <nl> continue ; <nl> fib_alias_accessed ( fa );
static void iwl_mvm_scan_fill_ssids ( struct iwl_ssid_ie * cmd_ssid , <nl> static u16 iwl_mvm_get_active_dwell ( enum ieee80211_band band , int n_ssids ) <nl> { <nl> if ( band == IEEE80211_BAND_2GHZ ) <nl> - return 30 + 3 * ( n_ssids + 1 ); <nl> - return 20 + 2 * ( n_ssids + 1 ); <nl> + return 20 + 3 * ( n_ssids + 1 ); <nl> + return 10 + 2 * ( n_ssids + 1 ); <nl> } <nl>  <nl> static u16 iwl_mvm_get_passive_dwell ( enum ieee80211_band band )
static int falcon_spi_device_init ( struct efx_nic * efx , <nl> struct efx_spi_device * spi_device ; <nl>  <nl> if ( device_type != 0 ) { <nl> - spi_device = kmalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> + spi_device = kzalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> if (! spi_device ) <nl> return - ENOMEM ; <nl> spi_device -> device_id = device_id ;
static int __devinit ipr_probe_ioa ( struct pci_dev * pdev , <nl> uproc = readl ( ioa_cfg -> regs . sense_uproc_interrupt_reg32 ); <nl> if (( mask & IPR_PCII_HRRQ_UPDATED ) == 0 || ( uproc & IPR_UPROCI_RESET_ALERT )) <nl> ioa_cfg -> needs_hard_reset = 1 ; <nl> - if ( interrupts & IPR_PCII_ERROR_INTERRUPTS ) <nl> + if (( interrupts & IPR_PCII_ERROR_INTERRUPTS ) || reset_devices ) <nl> ioa_cfg -> needs_hard_reset = 1 ; <nl> if ( interrupts & IPR_PCII_IOA_UNIT_CHECKED ) <nl> ioa_cfg -> ioa_unit_checked = 1 ;
static int __access_remote_vm ( struct task_struct * tsk , struct mm_struct * mm , <nl> */ <nl> # ifdef CONFIG_HAVE_IOREMAP_PROT <nl> vma = find_vma ( mm , addr ); <nl> - if (! vma ) <nl> + if (! vma || vma -> vm_start > addr ) <nl> break ; <nl> if ( vma -> vm_ops && vma -> vm_ops -> access ) <nl> ret = vma -> vm_ops -> access ( vma , addr , buf ,
static struct mfd_cell max77686_devs [] = { <nl> { . name = " max77686 - pmic ", }, <nl> { . name = " max77686 - rtc ", }, <nl> + { . name = " max77686 - clk ", }, <nl> }; <nl>  <nl> static struct regmap_config max77686_regmap_config = {
static struct platform_driver wm8505fb_driver = { <nl> . driver = { <nl> . owner = THIS_MODULE , <nl> . name = DRIVER_NAME , <nl> - . of_match_table = of_match_ptr ( wmt_dt_ids ), <nl> + . of_match_table = wmt_dt_ids , <nl> }, <nl> }; <nl> 
int imx1_pinctrl_core_probe ( struct platform_device * pdev , <nl>  <nl> ipctl -> base = devm_ioremap_nocache (& pdev -> dev , res -> start , <nl> resource_size ( res )); <nl> - if ( IS_ERR ( ipctl -> base )) <nl> - return PTR_ERR ( ipctl -> base ); <nl> + if (! ipctl -> base ) <nl> + return - ENOMEM ; <nl>  <nl> pctl_desc = & imx1_pinctrl_desc ; <nl> pctl_desc -> name = dev_name (& pdev -> dev );
static int __init setup_maxnodemem ( char * str ) <nl> { <nl> char * endp ; <nl> unsigned long long maxnodemem ; <nl> - long node ; <nl> + unsigned long node ; <nl>  <nl> node = str ? simple_strtoul ( str , & endp , 0 ) : INT_MAX ; <nl> if ( node >= MAX_NUMNODES || * endp != ':')
static int snd_timer_user_params ( struct file * file , <nl> if ( tu -> timeri -> flags & SNDRV_TIMER_IFLG_EARLY_EVENT ) { <nl> if ( tu -> tread ) { <nl> struct snd_timer_tread tread ; <nl> + memset (& tread , 0 , sizeof ( tread )); <nl> tread . event = SNDRV_TIMER_EVENT_EARLY ; <nl> tread . tstamp . tv_sec = 0 ; <nl> tread . tstamp . tv_nsec = 0 ;
static struct pernet_operations unix_net_ops = { <nl> static int __init af_unix_init ( void ) <nl> { <nl> int rc = - 1 ; <nl> - struct sk_buff * dummy_skb ; <nl>  <nl> - BUILD_BUG_ON ( sizeof ( struct unix_skb_parms ) > sizeof ( dummy_skb -> cb )); <nl> + BUILD_BUG_ON ( sizeof ( struct unix_skb_parms ) > FIELD_SIZEOF ( struct sk_buff , cb )); <nl>  <nl> rc = proto_register (& unix_proto , 1 ); <nl> if ( rc != 0 ) {
i915_gem_execbuffer_relocate_entry ( struct drm_i915_gem_object * obj , <nl> else <nl> ret = relocate_entry_gtt ( obj , reloc ); <nl>  <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> /* and update the user ' s relocation entry */ <nl> reloc -> presumed_offset = target_offset ; <nl> 
static int dtl1_hci_send_frame ( struct sk_buff * skb ) <nl> nsh . len = skb -> len ; <nl>  <nl> s = bt_skb_alloc ( NSHL + skb -> len + 1 , GFP_ATOMIC ); <nl> + if (! s ) <nl> + return - ENOMEM ; <nl> + <nl> skb_reserve ( s , NSHL ); <nl> memcpy ( skb_put ( s , skb -> len ), skb -> data , skb -> len ); <nl> if ( skb -> len & 0x0001 )
static int mpc5121_nfc_probe ( struct platform_device * op ) <nl> chip = & prv -> chip ; <nl>  <nl> mtd -> priv = chip ; <nl> + mtd -> dev . parent = dev ; <nl> chip -> priv = prv ; <nl> prv -> dev = dev ; <nl> 
static int video_open ( struct file * file ) <nl>  <nl> if ( NULL == dev ) { <nl> mutex_unlock (& cx25821_devlist_mutex ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> 
static inline void TCP_ECN_send ( struct sock * sk , struct sk_buff * skb , <nl> */ <nl> static void tcp_init_nondata_skb ( struct sk_buff * skb , u32 seq , u8 flags ) <nl> { <nl> + skb -> ip_summed = CHECKSUM_PARTIAL ; <nl> skb -> csum = 0 ; <nl>  <nl> TCP_SKB_CB ( skb )-> flags = flags ;
void ceph_check_caps ( struct ceph_inode_info * ci , int flags , <nl>  <nl> if ( cap == ci -> i_auth_cap && ci -> i_dirty_caps ) <nl> flushing = __mark_caps_flushing ( inode , session ); <nl> + else <nl> + flushing = 0 ; <nl>  <nl> mds = cap -> mds ; /* remember mds , so we don ' t repeat */ <nl> sent ++;
static int get_free_pipe_id_locked ( struct goldfish_pipe_dev * dev ) <nl> /* Reallocate the array */ <nl> u32 new_capacity = 2 * dev -> pipes_capacity ; <nl> struct goldfish_pipe ** pipes = <nl> - kcalloc ( new_capacity , sizeof (* pipes ), GFP_KERNEL ); <nl> + kcalloc ( new_capacity , sizeof (* pipes ), GFP_ATOMIC ); <nl> if (! pipes ) <nl> return - ENOMEM ; <nl> memcpy ( pipes , dev -> pipes , sizeof (* pipes ) * dev -> pipes_capacity );
static acpi_status setup_res ( struct acpi_resource * acpi_res , void * data ) <nl> struct resource * res = data ; <nl> struct resource_win win ; <nl>  <nl> + /* <nl> + * We might assign this to ' res ' later , make sure all pointers are <nl> + * cleared before the resource is added to the global list <nl> + */ <nl> + memset (& win , 0 , sizeof ( win )); <nl> + <nl> res -> flags = 0 ; <nl> if ( acpi_dev_filter_resource_type ( acpi_res , IORESOURCE_MEM )) <nl> return AE_OK ;
static int sdhci_st_probe ( struct platform_device * pdev ) <nl> if ( IS_ERR ( icnclk )) <nl> icnclk = NULL ; <nl>  <nl> - rstc = devm_reset_control_get (& pdev -> dev , NULL ); <nl> + rstc = devm_reset_control_get_exclusive (& pdev -> dev , NULL ); <nl> if ( IS_ERR ( rstc )) <nl> rstc = NULL ; <nl> else
static void conf_message_callback ( const char * fmt , va_list ap ) <nl>  <nl> static void show_help ( struct menu * menu ) <nl> { <nl> - struct gstr help = str_new (); <nl> + struct gstr help ; <nl> + <nl> + if (! menu ) <nl> + return ; <nl> + <nl> + help = str_new (); <nl> menu_get_ext_help ( menu , & help ); <nl> show_scroll_win ( main_window , _ ( menu_get_prompt ( menu )), str_get (& help )); <nl> str_free (& help );
static int sc16is7xx_probe ( struct device * dev , <nl> else <nl> return PTR_ERR ( s -> clk ); <nl> } else { <nl> + clk_prepare_enable ( s -> clk ); <nl> freq = clk_get_rate ( s -> clk ); <nl> } <nl> 
static void efifb_fixup_resources ( struct pci_dev * dev ) <nl> if (! base ) <nl> return ; <nl>  <nl> - for ( i = 0 ; i < PCI_STD_RESOURCE_END ; i ++) { <nl> + for ( i = 0 ; i <= PCI_STD_RESOURCE_END ; i ++) { <nl> struct resource * res = & dev -> resource [ i ]; <nl>  <nl> if (!( res -> flags & IORESOURCE_MEM ))
extern int vdso_enabled ; <nl>  <nl> # endif /* ! CONFIG_X86_32 */ <nl>  <nl> +# define CORE_DUMP_USE_REGSET <nl> # define USE_ELF_CORE_DUMP <nl> # define ELF_EXEC_PAGESIZE 4096 <nl> 
long drm_ioctl ( struct file * filp , <nl> retcode = - EFAULT ; <nl> goto err_i1 ; <nl> } <nl> - } <nl> + } else <nl> + memset ( kdata , 0 , _IOC_SIZE ( cmd )); <nl> + <nl> if ( ioctl -> flags & DRM_UNLOCKED ) <nl> retcode = func ( dev , kdata , file_priv ); <nl> else {
static int wcn36xx_start ( struct ieee80211_hw * hw ) <nl> wcn36xx_smd_stop ( wcn ); <nl> out_free_smd_buf : <nl> kfree ( wcn -> hal_buf ); <nl> - out_free_dxe_pool : <nl> - wcn36xx_dxe_free_mem_pools ( wcn ); <nl> out_free_dxe_ctl : <nl> wcn36xx_dxe_free_ctl_blks ( wcn ); <nl> + out_free_dxe_pool : <nl> + wcn36xx_dxe_free_mem_pools ( wcn ); <nl> out_smd_close : <nl> wcn36xx_smd_close ( wcn ); <nl> out_err :
static struct gfs2_leaf * new_leaf ( struct inode * inode , struct buffer_head ** pbh , <nl> leaf = ( struct gfs2_leaf *) bh -> b_data ; <nl> leaf -> lf_depth = cpu_to_be16 ( depth ); <nl> leaf -> lf_entries = 0 ; <nl> - leaf -> lf_dirent_format = cpu_to_be16 ( GFS2_FORMAT_DE ); <nl> + leaf -> lf_dirent_format = cpu_to_be32 ( GFS2_FORMAT_DE ); <nl> leaf -> lf_next = 0 ; <nl> memset ( leaf -> lf_reserved , 0 , sizeof ( leaf -> lf_reserved )); <nl> dent = ( struct gfs2_dirent *)( leaf + 1 );
long btrfs_ioctl_send ( struct file * mnt_file , void __user * arg_ ) <nl> goto out ; <nl> } <nl>  <nl> + if ( arg -> clone_sources_count > <nl> + ULLONG_MAX / sizeof (* arg -> clone_sources )) { <nl> + ret = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> if (! access_ok ( VERIFY_READ , arg -> clone_sources , <nl> sizeof (* arg -> clone_sources ) * <nl> arg -> clone_sources_count )) {
static struct i2c_board_info __initdata snapper9260_i2c_devices [] = { <nl> { <nl> /* RTC */ <nl> I2C_BOARD_INFO (" isl1208 ", 0x6f ), <nl> + . irq = gpio_to_irq ( AT91_PIN_PA31 ), <nl> }, <nl> }; <nl> 
static int ieee80211_change_station ( struct wiphy * wiphy , <nl> } <nl>  <nl> if ( params -> vlan -> ieee80211_ptr -> use_4addr ) { <nl> - if ( vlansdata -> u . vlan . sta ) <nl> + if ( vlansdata -> u . vlan . sta ) { <nl> + rcu_read_unlock (); <nl> return - EBUSY ; <nl> + } <nl>  <nl> rcu_assign_pointer ( vlansdata -> u . vlan . sta , sta ); <nl> }
static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> - memset ( sax , 0 , sizeof ( sax )); <nl> + memset ( sax , 0 , sizeof (* sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
static int GLOB_SBD_init ( void ) <nl> int i ; <nl>  <nl> /* Set debug output level ( 0 ~ 3 ) here . 3 is most verbose */ <nl> - nand_debug_level = 0 ; <nl> - <nl> printk ( KERN_ALERT " Spectra : % s \ n ", GLOB_version ); <nl>  <nl> mutex_init (& spectra_lock );
static int handle_conflicting_encoders ( struct drm_atomic_state * state , <nl>  <nl> if ( funcs -> atomic_best_encoder ) <nl> new_encoder = funcs -> atomic_best_encoder ( connector , conn_state ); <nl> - else <nl> + else if ( funcs -> best_encoder ) <nl> new_encoder = funcs -> best_encoder ( connector ); <nl> + else <nl> + new_encoder = drm_atomic_helper_best_encoder ( connector ); <nl>  <nl> if ( new_encoder ) { <nl> if ( encoder_mask & ( 1 << drm_encoder_index ( new_encoder ))) {
static int blkif_release ( struct inode * inode , struct file * filep ) <nl> struct xenbus_device * dev = info -> xbdev ; <nl> enum xenbus_state state = xenbus_read_driver_state ( dev -> otherend ); <nl>  <nl> - if ( state == XenbusStateClosing ) <nl> + if ( state == XenbusStateClosing && info -> is_ready ) <nl> blkfront_closing ( dev ); <nl> } <nl> return 0 ;
static void i40e_service_task ( struct work_struct * work ) <nl> service_task ); <nl> unsigned long start_time = jiffies ; <nl>  <nl> + /* don ' t bother with service tasks if a reset is in progress */ <nl> + if ( test_bit ( __I40E_RESET_RECOVERY_PENDING , & pf -> state )) { <nl> + i40e_service_event_complete ( pf ); <nl> + return ; <nl> + } <nl> + <nl> i40e_reset_subtask ( pf ); <nl> i40e_handle_mdd_event ( pf ); <nl> i40e_vc_process_vflr_event ( pf );
ip_vs_new_dest ( struct ip_vs_service * svc , struct ip_vs_dest_user_kern * udest , <nl> # ifdef CONFIG_IP_VS_IPV6 <nl> if ( svc -> af == AF_INET6 ) { <nl> atype = ipv6_addr_type (& udest -> addr . in6 ); <nl> - if (!( atype & IPV6_ADDR_UNICAST ) && <nl> + if ((!( atype & IPV6_ADDR_UNICAST ) || <nl> + atype & IPV6_ADDR_LINKLOCAL ) && <nl> ! __ip_vs_addr_is_local_v6 (& udest -> addr . in6 )) <nl> return - EINVAL ; <nl> } else
struct perf_evsel * perf_evlist__id2evsel ( struct perf_evlist * evlist , u64 id ) <nl> hlist_for_each_entry ( sid , pos , head , node ) <nl> if ( sid -> id == id ) <nl> return sid -> evsel ; <nl> + <nl> + if (! perf_evlist__sample_id_all ( evlist )) <nl> + return list_entry ( evlist -> entries . next , struct perf_evsel , node ); <nl> + <nl> return NULL ; <nl> } <nl> 
xfs_btree_simple_query_range ( <nl> if ( error ) <nl> goto out ; <nl>  <nl> + /* Nothing ? See if there ' s anything to the right . */ <nl> + if (! stat ) { <nl> + error = xfs_btree_increment ( cur , 0 , & stat ); <nl> + if ( error ) <nl> + goto out ; <nl> + } <nl> + <nl> while ( stat ) { <nl> /* Find the record . */ <nl> error = xfs_btree_get_rec ( cur , & recp , & stat );
static int clone_backref_node ( struct btrfs_trans_handle * trans , <nl> new_node -> bytenr = dest -> node -> start ; <nl> new_node -> level = node -> level ; <nl> new_node -> lowest = node -> lowest ; <nl> + new_node -> checked = 1 ; <nl> new_node -> root = dest ; <nl>  <nl> if (! node -> lowest ) {
static int mv_udc_get_frame ( struct usb_gadget * gadget ) <nl>  <nl> udc = container_of ( gadget , struct mv_udc , gadget ); <nl>  <nl> - retval = readl ( udc -> op_regs -> frindex ) & USB_FRINDEX_MASKS ; <nl> + retval = readl (& udc -> op_regs -> frindex ) & USB_FRINDEX_MASKS ; <nl>  <nl> return retval ; <nl> }
void recalc_intercepts ( struct vcpu_svm * svm ) <nl> /* If SMI is not intercepted , ignore guest SMI intercept as well */ <nl> if (! intercept_smi ) <nl> vmcb_clr_intercept ( c , INTERCEPT_SMI ); <nl> + <nl> + vmcb_set_intercept ( c , INTERCEPT_VMLOAD ); <nl> + vmcb_set_intercept ( c , INTERCEPT_VMSAVE ); <nl> } <nl>  <nl> static void copy_vmcb_control_area ( struct vmcb_control_area * dst ,
static void bus_reset_work ( struct work_struct * work ) <nl> { <nl> struct fw_ohci * ohci = <nl> container_of ( work , struct fw_ohci , bus_reset_work ); <nl> - int self_id_count , i , j , reg ; <nl> - int generation , new_generation ; <nl> + int self_id_count , generation , new_generation , i , j ; <nl> + u32 reg ; <nl> unsigned long flags ; <nl> void * free_rom = NULL ; <nl> dma_addr_t free_rom_bus = 0 ;
build_unc_path_to_root ( const struct smb_vol * vol , <nl> pos = full_path + unc_len ; <nl>  <nl> if ( pplen ) { <nl> - * pos ++ = CIFS_DIR_SEP ( cifs_sb ); <nl> - strncpy ( pos , vol -> prepath , pplen ); <nl> + * pos = CIFS_DIR_SEP ( cifs_sb ); <nl> + strncpy ( pos + 1 , vol -> prepath , pplen ); <nl> pos += pplen ; <nl> } <nl> 
static int __init d40_of_probe ( struct platform_device * pdev , <nl> list = of_get_property ( np , " disabled - channels ", & num_disabled ); <nl> num_disabled /= sizeof (* list ); <nl>  <nl> - if ( num_disabled > STEDMA40_MAX_PHYS || num_disabled < 0 ) { <nl> + if ( num_disabled >= STEDMA40_MAX_PHYS || num_disabled < 0 ) { <nl> d40_err (& pdev -> dev , <nl> " Invalid number of disabled channels specified (% d )\ n ", <nl> num_disabled );
static int skge_down ( struct net_device * dev ) <nl> struct skge_hw * hw = skge -> hw ; <nl> int port = skge -> port ; <nl>  <nl> - if ( skge -> mem == NULL ) <nl> + if (! skge -> mem ) <nl> return 0 ; <nl>  <nl> netif_info ( skge , ifdown , skge -> netdev , " disabling interface \ n ");
static void process_init_reply ( struct fuse_conn * fc , struct fuse_req * req ) <nl> int i ; <nl> struct fuse_init_out * arg = & req -> misc . init_out ; <nl>  <nl> - if ( arg -> major != FUSE_KERNEL_VERSION ) <nl> + if ( req -> out . h . error || arg -> major != FUSE_KERNEL_VERSION ) <nl> fc -> conn_error = 1 ; <nl> else { <nl> fc -> minor = arg -> minor ;
static ssize_t write_ports ( struct file * file , char * buf , size_t size ) <nl> /* Decrease the count , but don ' t shutdown the <nl> * the service <nl> */ <nl> + lock_kernel (); <nl> nfsd_serv -> sv_nrthreads --; <nl> + unlock_kernel (); <nl> } <nl> return err ; <nl> }
static void ds1374_set_tlet ( ulong arg ) <nl> " can ' t confirm time set from rtc chip \ n "); <nl> } <nl>  <nl> - ulong new_time ; <nl> + static ulong new_time ; <nl>  <nl> DECLARE_TASKLET_DISABLED ( ds1374_tasklet , ds1374_set_tlet , ( ulong ) & new_time ); <nl> 
static int wm97xx_init_pen_irq ( struct wm97xx * wm ) <nl> * provided . */ <nl> BUG_ON (! wm -> mach_ops -> irq_enable ); <nl>  <nl> - if ( request_irq ( wm -> pen_irq , wm97xx_pen_interrupt , IRQF_SHARED , <nl> + if ( request_irq ( wm -> pen_irq , wm97xx_pen_interrupt , <nl> + IRQF_SHARED | IRQF_SAMPLE_RANDOM , <nl> " wm97xx - pen ", wm )) { <nl> dev_err ( wm -> dev , <nl> " Failed to register pen down interrupt , polling ");
static bool g4x_compute_wm0 ( struct drm_device * dev , <nl> int entries , tlb_miss ; <nl>  <nl> crtc = intel_get_crtc_for_plane ( dev , plane ); <nl> - if ( crtc -> fb == NULL || ! crtc -> enabled ) <nl> + if ( crtc -> fb == NULL || ! crtc -> enabled ) { <nl> + * cursor_wm = cursor -> guard_size ; <nl> + * plane_wm = display -> guard_size ; <nl> return false ; <nl> + } <nl>  <nl> htotal = crtc -> mode . htotal ; <nl> hdisplay = crtc -> mode . hdisplay ;
static int ov965x_enum_frame_sizes ( struct v4l2_subdev * sd , <nl> { <nl> int i = ARRAY_SIZE ( ov965x_formats ); <nl>  <nl> - if ( fse -> index > ARRAY_SIZE ( ov965x_framesizes )) <nl> + if ( fse -> index >= ARRAY_SIZE ( ov965x_framesizes )) <nl> return - EINVAL ; <nl>  <nl> while (-- i )
static ssize_t cxlflash_show_port_status ( struct device * dev , <nl> u64 * fc_regs ; <nl>  <nl> rc = kstrtouint (( attr -> attr . name + 4 ), 10 , & port ); <nl> - if ( rc || ( port > NUM_FC_PORTS )) <nl> + if ( rc || ( port >= NUM_FC_PORTS )) <nl> return 0 ; <nl>  <nl> fc_regs = & afu -> afu_map -> global . fc_regs [ port ][ 0 ];
static inline int verify_replay ( struct xfrm_usersa_info * p , <nl> if (! rt ) <nl> return 0 ; <nl>  <nl> + if ( p -> id . proto != IPPROTO_ESP ) <nl> + return - EINVAL ; <nl> + <nl> if ( p -> replay_window != 0 ) <nl> return - EINVAL ; <nl> 
static int init_phy ( struct net_device * dev ) <nl> if ( priv -> phy_interface == PHY_INTERFACE_MODE_SGMII ) <nl> uec_configure_serdes ( dev ); <nl>  <nl> - phydev -> supported &= ( ADVERTISED_10baseT_Half | <nl> - ADVERTISED_10baseT_Full | <nl> - ADVERTISED_100baseT_Half | <nl> - ADVERTISED_100baseT_Full ); <nl> + phydev -> supported &= ( SUPPORTED_MII | <nl> + SUPPORTED_Autoneg | <nl> + ADVERTISED_10baseT_Half | <nl> + ADVERTISED_10baseT_Full | <nl> + ADVERTISED_100baseT_Half | <nl> + ADVERTISED_100baseT_Full ); <nl>  <nl> if ( priv -> max_speed == SPEED_1000 ) <nl> phydev -> supported |= ADVERTISED_1000baseT_Full ;
static void release_resources ( struct ibmvnic_adapter * adapter ) <nl> } <nl> } <nl> } <nl> + kfree ( adapter -> napi ); <nl> + adapter -> napi = NULL ; <nl>  <nl> release_login_rsp_buffer ( adapter ); <nl> }
static int rfcomm_tty_open ( struct tty_struct * tty , struct file * filp ) <nl> break ; <nl> } <nl>  <nl> + tty_unlock (); <nl> schedule (); <nl> + tty_lock (); <nl> } <nl> set_current_state ( TASK_RUNNING ); <nl> remove_wait_queue (& dev -> wait , & wait );
static void intel_pt_insn_decoder ( struct insn * insn , <nl> enum intel_pt_insn_branch branch = INTEL_PT_BR_NO_BRANCH ; <nl> int ext ; <nl>  <nl> + intel_pt_insn -> rel = 0 ; <nl> + <nl> if ( insn_is_avx ( insn )) { <nl> intel_pt_insn -> op = INTEL_PT_OP_OTHER ; <nl> intel_pt_insn -> branch = INTEL_PT_BR_NO_BRANCH ;
static const struct snd_pci_quirk stac92hd73xx_cfg_tbl [] = { <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x02bd , <nl> " Dell Studio 1557 ", STAC_DELL_M6_DMIC ), <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x02fe , <nl> - " Dell Studio XPS 1645 ", STAC_DELL_M6_BOTH ), <nl> + " Dell Studio XPS 1645 ", STAC_DELL_M6_DMIC ), <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x0413 , <nl> " Dell Studio 1558 ", STAC_DELL_M6_DMIC ), <nl> {} /* terminator */
static int mp_wait_modem_status ( struct sb_uart_state * state , unsigned long arg ) <nl>  <nl> static int mp_get_count ( struct sb_uart_state * state , struct serial_icounter_struct * icnt ) <nl> { <nl> - struct serial_icounter_struct icount ; <nl> + struct serial_icounter_struct icount = {}; <nl> struct sb_uart_icount cnow ; <nl> struct sb_uart_port * port = state -> port ; <nl> 
static void mwifiex_tdls_add_aid ( struct mwifiex_private * priv , <nl> pos = ( void *) skb_put ( skb , 4 ); <nl> * pos ++ = WLAN_EID_AID ; <nl> * pos ++ = 2 ; <nl> - * pos ++ = le16_to_cpu ( assoc_rsp -> a_id ); <nl> + memcpy ( pos , & assoc_rsp -> a_id , sizeof ( assoc_rsp -> a_id )); <nl>  <nl> return ; <nl> }
static int pcie_find_smpss ( struct pci_dev * dev , void * data ) <nl> * will occur as normal . <nl> */ <nl> if ( dev -> is_hotplug_bridge && (! list_is_singular (& dev -> bus -> devices ) || <nl> - dev -> bus -> self -> pcie_type != PCI_EXP_TYPE_ROOT_PORT )) <nl> + ( dev -> bus -> self && <nl> + dev -> bus -> self -> pcie_type != PCI_EXP_TYPE_ROOT_PORT ))) <nl> * smpss = 0 ; <nl>  <nl> if (* smpss > dev -> pcie_mpss )
mlxreg_hotplug_health_work_helper ( struct mlxreg_hotplug_priv_data * priv , <nl> { <nl> struct mlxreg_core_data * data = item -> data ; <nl> u32 regval ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> for ( i = 0 ; i < item -> count ; i ++, data ++) { <nl> /* Mask event . */
static int scmi_hwmon_probe ( struct scmi_device * sdev ) <nl> scmi_chip_info . info = ptr_scmi_ci ; <nl> chip_info = & scmi_chip_info ; <nl>  <nl> - for ( type = 0 ; type < hwmon_max && nr_count [ type ]; type ++) { <nl> + for ( type = 0 ; type < hwmon_max ; type ++) { <nl> + if (! nr_count [ type ]) <nl> + continue ; <nl> + <nl> scmi_hwmon_add_chan_info ( scmi_hwmon_chan , dev , nr_count [ type ], <nl> type , hwmon_attributes [ type ]); <nl> * ptr_scmi_ci ++ = scmi_hwmon_chan ++;
rpcrdma_register_internal ( struct rpcrdma_ia * ia , void * va , int len , <nl> */ <nl> iov -> addr = ib_dma_map_single ( ia -> ri_id -> device , <nl> va , len , DMA_BIDIRECTIONAL ); <nl> + if ( ib_dma_mapping_error ( ia -> ri_id -> device , iov -> addr )) <nl> + return - ENOMEM ; <nl> + <nl> iov -> length = len ; <nl>  <nl> if ( ia -> ri_have_dma_lkey ) {
static long media_device_enum_entities ( struct media_device * mdev , <nl> struct media_entity * ent ; <nl> struct media_entity_desc u_ent ; <nl>  <nl> + memset (& u_ent , 0 , sizeof ( u_ent )); <nl> if ( copy_from_user (& u_ent . id , & uent -> id , sizeof ( u_ent . id ))) <nl> return - EFAULT ; <nl> 
int brcmf_fws_hdrpull ( struct brcmf_pub * drvr , int ifidx , s16 signal_len , <nl> if (! signal_len ) <nl> return 0 ; <nl> /* if flow control disabled , skip to packet data and leave */ <nl> - if (! fws -> fw_signals ) { <nl> + if ((! fws ) || (! fws -> fw_signals )) { <nl> skb_pull ( skb , signal_len ); <nl> return 0 ; <nl> }
static int tvp5150_fill_fmt ( struct v4l2_subdev * sd , <nl> struct v4l2_mbus_framefmt * f ; <nl> struct tvp5150 * decoder = to_tvp5150 ( sd ); <nl>  <nl> - if (! format || format -> pad ) <nl> + if (! format || ( format -> pad != DEMOD_PAD_VID_OUT )) <nl> return - EINVAL ; <nl>  <nl> f = & format -> format ;
static int may_commit_transaction ( struct btrfs_fs_info * fs_info , <nl>  <nl> spin_lock (& delayed_rsv -> lock ); <nl> if ( percpu_counter_compare (& space_info -> total_bytes_pinned , <nl> - bytes - delayed_rsv -> size ) >= 0 ) { <nl> + bytes - delayed_rsv -> size ) < 0 ) { <nl> spin_unlock (& delayed_rsv -> lock ); <nl> return - ENOSPC ; <nl> }
static int adu_release ( struct inode * inode , struct file * file ) <nl> retval = adu_release_internal ( dev ); <nl>  <nl> exit : <nl> - up (& dev -> sem ); <nl> + if ( dev ) <nl> + up (& dev -> sem ); <nl> dbg ( 2 ," % s : leave , return value % d ", __FUNCTION__ , retval ); <nl> return retval ; <nl> }
static const struct attribute_group isl29108_group = { <nl> static int isl29018_chip_init ( struct isl29018_chip * chip ) <nl> { <nl> int status ; <nl> - int new_adc_bit ; <nl> + unsigned int new_adc_bit ; <nl> unsigned int new_range ; <nl>  <nl> /* Code added per Intersil Application Note 1534 :
static int copy_to_user_tmpl ( struct xfrm_policy * xp , struct sk_buff * skb ) <nl> struct xfrm_user_tmpl * up = & vec [ i ]; <nl> struct xfrm_tmpl * kp = & xp -> xfrm_vec [ i ]; <nl>  <nl> + memset ( up , 0 , sizeof (* up )); <nl> memcpy (& up -> id , & kp -> id , sizeof ( up -> id )); <nl> up -> family = kp -> encap_family ; <nl> memcpy (& up -> saddr , & kp -> saddr , sizeof ( up -> saddr ));
static void sl_tx_timeout ( struct net_device * dev , unsigned int txqueue ) <nl> spin_lock (& sl -> lock ); <nl>  <nl> if ( netif_queue_stopped ( dev )) { <nl> - if (! netif_running ( dev )) <nl> + if (! netif_running ( dev ) || ! sl -> tty ) <nl> goto out ; <nl>  <nl> /* May be we must check transmitter timeout here ?
static int twl6040_probe ( struct i2c_client * client , <nl> init_completion (& twl6040 -> ready ); <nl>  <nl> twl6040 -> rev = twl6040_reg_read ( twl6040 , TWL6040_REG_ASICREV ); <nl> + if ( twl6040 -> rev < 0 ) { <nl> + dev_err (& client -> dev , " Failed to read revision register : % d \ n ", <nl> + twl6040 -> rev ); <nl> + goto gpio_err ; <nl> + } <nl>  <nl> /* ERRATA : Automatic power - up is not possible in ES1 . 0 */ <nl> if ( twl6040_get_revid ( twl6040 ) > TWL6040_REV_ES1_0 )
static struct console usbcons = { <nl>  <nl> void usb_serial_console_disconnect ( struct usb_serial * serial ) <nl> { <nl> - if ( serial -> port [ 0 ] == usbcons_info . port ) { <nl> + if ( serial -> port [ 0 ] && serial -> port [ 0 ] == usbcons_info . port ) { <nl> usb_serial_console_exit (); <nl> usb_serial_put ( serial ); <nl> }
skl_tplg_init_pipe_modules ( struct skl * skl , struct skl_pipe * pipe ) <nl> if ( mconfig -> id . module_id < 0 ) { <nl> struct skl_dfw_module * dfw_config ; <nl>  <nl> - dfw_config = kzalloc ( sizeof ( dfw_config ), GFP_KERNEL ); <nl> + dfw_config = kzalloc ( sizeof (* dfw_config ), GFP_KERNEL ); <nl> if (! dfw_config ) <nl> return - ENOMEM ; <nl> 
static void execlists_submission_tasklet ( unsigned long data ) <nl> trace_i915_request_out ( rq ); <nl> i915_request_put ( rq ); <nl>  <nl> + GEM_TRACE ("% s completed ctx =% d \ n ", <nl> + engine -> name , port -> context_id ); <nl> + <nl> execlists_port_complete ( execlists , port ); <nl> } else { <nl> port_set ( port , port_pack ( rq , count ));
xlog_recover_do_reg_buffer ( <nl> stale_buf = 1 ; <nl> break ; <nl> } <nl> - if ( be16_to_cpu ( dip -> di_core . di_mode )) <nl> + if ( dip -> di_core . di_mode ) <nl> mode_count ++; <nl> - if ( be16_to_cpu ( dip -> di_core . di_gen )) <nl> + if ( dip -> di_core . di_gen ) <nl> gen_count ++; <nl> } <nl> 
static bool msr_mtrr_valid ( unsigned msr ) <nl> case MSR_MTRRdefType : <nl> case MSR_IA32_CR_PAT : <nl> return true ; <nl> - case 0x2f8 : <nl> - return true ; <nl> } <nl> return false ; <nl> }
static void cache_set_flush ( struct closure * cl ) <nl> struct btree * b ; <nl> unsigned i ; <nl>  <nl> + if (! c ) <nl> + closure_return ( cl ); <nl> + <nl> bch_cache_accounting_destroy (& c -> accounting ); <nl>  <nl> kobject_put (& c -> internal );
static struct dma_page * pool_find_page ( struct dma_pool * pool , dma_addr_t dma ) <nl> list_for_each_entry ( page , & pool -> page_list , page_list ) { <nl> if ( dma < page -> dma ) <nl> continue ; <nl> - if ( dma < ( page -> dma + pool -> allocation )) <nl> + if (( dma - page -> dma ) < pool -> allocation ) <nl> return page ; <nl> } <nl> return NULL ;
rio_dma_transfer ( struct file * filp , u32 transfer_mode , <nl> goto err_req ; <nl> } <nl>  <nl> - pinned = get_user_pages_unlocked ( <nl> + pinned = get_user_pages_fast ( <nl> ( unsigned long ) xfer -> loc_addr & PAGE_MASK , <nl> - nr_pages , <nl> - page_list , <nl> - dir == DMA_FROM_DEVICE ? FOLL_WRITE : 0 ); <nl> + nr_pages , dir == DMA_FROM_DEVICE , page_list ); <nl>  <nl> if ( pinned != nr_pages ) { <nl> if ( pinned < 0 ) {
static bool is_fullscreen ( struct drm_crtc_state * cstate , <nl> (( pstate -> crtc_y + pstate -> crtc_h ) >= cstate -> mode . vdisplay ); <nl> } <nl>  <nl> - enum mdp_mixer_stage_id get_start_stage ( struct drm_crtc * crtc , <nl> + static enum mdp_mixer_stage_id get_start_stage ( struct drm_crtc * crtc , <nl> struct drm_crtc_state * new_crtc_state , <nl> struct drm_plane_state * bpstate ) <nl> {
static struct task_struct * select_bad_process ( unsigned long * ppoints ) <nl> unsigned long points ; <nl> int releasing ; <nl>  <nl> + /* skip kernel threads */ <nl> + if (! p -> mm ) <nl> + continue ; <nl> /* skip the init task with pid == 1 */ <nl> if ( p -> pid == 1 ) <nl> continue ;
static unsigned long ext4_get_stripe_size ( struct ext4_sb_info * sbi ) <nl>  <nl> if ( sbi -> s_stripe && sbi -> s_stripe <= sbi -> s_blocks_per_group ) <nl> ret = sbi -> s_stripe ; <nl> - else if ( stripe_width <= sbi -> s_blocks_per_group ) <nl> + else if ( stripe_width && stripe_width <= sbi -> s_blocks_per_group ) <nl> ret = stripe_width ; <nl> - else if ( stride <= sbi -> s_blocks_per_group ) <nl> + else if ( stride && stride <= sbi -> s_blocks_per_group ) <nl> ret = stride ; <nl> else <nl> ret = 0 ;
bool kvm_irq_delivery_to_apic_fast ( struct kvm * kvm , struct kvm_lapic * src , <nl> * r = - 1 ; <nl>  <nl> if ( irq -> shorthand == APIC_DEST_SELF ) { <nl> + if ( KVM_BUG_ON (! src , kvm )) { <nl> + * r = 0 ; <nl> + return true ; <nl> + } <nl> * r = kvm_apic_set_irq ( src -> vcpu , irq , dest_map ); <nl> return true ; <nl> }
bool ath9k_hw_eeprom_set_board_values ( struct ath_hal * ah , <nl>  <nl> txRxAttenLocal = IS_CHAN_2GHZ ( chan ) ? 23 : 44 ; <nl>  <nl> - ath9k_hw_get_eeprom_antenna_cfg ( ah , chan , 1 , & ant_config ); <nl> + ath9k_hw_get_eeprom_antenna_cfg ( ah , chan , 0 , & ant_config ); <nl> REG_WRITE ( ah , AR_PHY_SWITCH_COM , ant_config ); <nl>  <nl> for ( i = 0 ; i < AR5416_MAX_CHAINS ; i ++) {
static void snd_soc_instantiate_card ( struct snd_soc_card * card ) <nl> snd_soc_dapm_add_routes (& card -> dapm , card -> dapm_routes , <nl> card -> num_dapm_routes ); <nl>  <nl> + snd_soc_dapm_new_widgets (& card -> dapm ); <nl> + <nl> for ( i = 0 ; i < card -> num_links ; i ++) { <nl> dai_link = & card -> dai_link [ i ]; <nl> 
static int copy_to_user_auth ( struct xfrm_algo_auth * auth , struct sk_buff * skb ) <nl> return - EMSGSIZE ; <nl>  <nl> algo = nla_data ( nla ); <nl> - strcpy ( algo -> alg_name , auth -> alg_name ); <nl> + strncpy ( algo -> alg_name , auth -> alg_name , sizeof ( algo -> alg_name )); <nl> memcpy ( algo -> alg_key , auth -> alg_key , ( auth -> alg_key_len + 7 ) / 8 ); <nl> algo -> alg_key_len = auth -> alg_key_len ; <nl> 
static struct afs_volume * afs_alloc_volume ( struct afs_mount_params * params , <nl> error_2 : <nl> afs_put_serverlist ( params -> net , slist ); <nl> error_1 : <nl> + afs_put_cell ( params -> net , volume -> cell ); <nl> kfree ( volume ); <nl> error_0 : <nl> return ERR_PTR ( ret );
static void cx_auto_check_auto_mic ( struct hda_codec * codec ) <nl> int pset [ INPUT_PIN_ATTR_NORMAL + 1 ]; <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < INPUT_PIN_ATTR_NORMAL ; i ++) <nl> + for ( i = 0 ; i < ARRAY_SIZE ( pset ); i ++) <nl> pset [ i ] = - 1 ; <nl> for ( i = 0 ; i < spec -> private_imux . num_items ; i ++) { <nl> hda_nid_t pin = spec -> imux_info [ i ]. pin ;
static int __ext4_ext_check ( const char * function , unsigned int line , <nl> error_msg = " invalid extent entries "; <nl> goto corrupted ; <nl> } <nl> + if ( unlikely ( depth > 32 )) { <nl> + error_msg = " too large eh_depth "; <nl> + goto corrupted ; <nl> + } <nl> /* Verify checksum on non - root extent tree nodes */ <nl> if ( ext_depth ( inode ) != depth && <nl> ! ext4_extent_block_csum_verify ( inode , eh )) {
static int rtl8xxxu_submit_int_urb ( struct ieee80211_hw * hw ) <nl> ret = usb_submit_urb ( urb , GFP_KERNEL ); <nl> if ( ret ) { <nl> usb_unanchor_urb ( urb ); <nl> + usb_free_urb ( urb ); <nl> goto error ; <nl> } <nl> 
int st_sensors_check_device_support ( struct iio_dev * indio_dev , <nl> break ; <nl> } <nl> if ( n == ARRAY_SIZE ( sensor_settings [ i ]. sensors_supported )) { <nl> - dev_err (& indio_dev -> dev , " device name and WhoAmI mismatch .\ n "); <nl> + dev_err (& indio_dev -> dev , " device name \"% s \" and WhoAmI ( 0x % 02x ) mismatch ", <nl> + indio_dev -> name , wai ); <nl> goto sensor_name_mismatch ; <nl> } <nl> 
static int rpmsg_dev_probe ( struct device * dev ) <nl> goto out ; <nl> } <nl>  <nl> - if ( rpdev -> ops -> announce_create ) <nl> + if ( ept && rpdev -> ops -> announce_create ) <nl> err = rpdev -> ops -> announce_create ( rpdev ); <nl> out : <nl> return err ;
static int __posix_lock_file ( struct inode * inode , struct file_lock * request , str <nl> } <nl> locks_copy_lock ( new_fl , request ); <nl> locks_insert_lock_ctx ( new_fl , & fl -> fl_list ); <nl> + fl = new_fl ; <nl> new_fl = NULL ; <nl> } <nl> if ( right ) {
static void ohci_stop ( struct usb_hcd * hcd ) <nl>  <nl> ohci_usb_reset ( ohci ); <nl> ohci_writel ( ohci , OHCI_INTR_MIE , & ohci -> regs -> intrdisable ); <nl> - <nl> + free_irq ( hcd -> irq , hcd ); <nl> + hcd -> irq = - 1 ; <nl> + <nl> remove_debug_files ( ohci ); <nl> ohci_mem_cleanup ( ohci ); <nl> if ( ohci -> hcca ) {
int ext4_expand_extra_isize_ea ( struct inode * inode , int new_extra_isize , <nl> s_min_extra_isize ) { <nl> tried_min_extra_isize ++; <nl> new_extra_isize = s_min_extra_isize ; <nl> + kfree ( is ); is = NULL ; <nl> + kfree ( bs ); bs = NULL ; <nl> goto retry ; <nl> } <nl> error = - 1 ;
static long cgroup_create ( struct cgroup * parent , struct dentry * dentry , <nl> } <nl>  <nl> err = percpu_ref_init (& css -> refcnt , css_release ); <nl> - if ( err ) <nl> + if ( err ) { <nl> + ss -> css_free ( cgrp ); <nl> goto err_free_all ; <nl> + } <nl>  <nl> init_cgroup_css ( css , ss , cgrp ); <nl> 
static int mb86s70_gpio_request ( struct gpio_chip * gc , unsigned gpio ) <nl> spin_lock_irqsave (& gchip -> lock , flags ); <nl>  <nl> val = readl ( gchip -> base + PFR ( gpio )); <nl> + if (!( val & OFFSET ( gpio ))) { <nl> + spin_unlock_irqrestore (& gchip -> lock , flags ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> val &= ~ OFFSET ( gpio ); <nl> writel ( val , gchip -> base + PFR ( gpio )); <nl> 
static int find_first_block_group ( struct btrfs_root * root , <nl> } else { <nl> ret = 0 ; <nl> } <nl> + free_extent_map ( em ); <nl> goto out ; <nl> } <nl> path -> slots [ 0 ]++;
static int wait_for_connected ( struct usb_device * udev , <nl> while ( delay_ms < 2000 ) { <nl> if ( status || * portstatus & USB_PORT_STAT_CONNECTION ) <nl> break ; <nl> + if (! port_is_power_on ( hub , * portstatus )) { <nl> + status = - ENODEV ; <nl> + break ; <nl> + } <nl> msleep ( 20 ); <nl> delay_ms += 20 ; <nl> status = hub_port_status ( hub , * port1 , portstatus , portchange );
struct kvm_pit * kvm_create_pit ( struct kvm * kvm ) <nl> mutex_lock (& kvm -> lock ); <nl> pit -> irq_source_id = kvm_request_irq_source_id ( kvm ); <nl> mutex_unlock (& kvm -> lock ); <nl> - if ( pit -> irq_source_id < 0 ) <nl> + if ( pit -> irq_source_id < 0 ) { <nl> + kfree ( pit ); <nl> return NULL ; <nl> + } <nl>  <nl> mutex_init (& pit -> pit_state . lock ); <nl> mutex_lock (& pit -> pit_state . lock );
static void <nl> iscsi_tcp_conn_stop ( struct iscsi_cls_conn * cls_conn , int flag ) <nl> { <nl> struct iscsi_conn * conn = cls_conn -> dd_data ; <nl> + struct iscsi_tcp_conn * tcp_conn = conn -> dd_data ; <nl>  <nl> iscsi_conn_stop ( cls_conn , flag ); <nl> iscsi_tcp_release_conn ( conn ); <nl> + tcp_conn -> hdr_size = sizeof ( struct iscsi_hdr ); <nl> } <nl>  <nl> static int
store_vrm_reg ( struct device * dev , struct device_attribute * attr , const char * buf <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> data -> vrm = val ; <nl>  <nl> return count ;
static int __init parse_options ( struct early_uart_device * device , char * options ) <nl>  <nl> if (( options = strchr ( options , ','))) { <nl> options ++; <nl> - device -> baud = simple_strtoul ( options , 0 , 0 ); <nl> + device -> baud = simple_strtoul ( options , NULL , 0 ); <nl> length = min ( strcspn ( options , " "), sizeof ( device -> options )); <nl> strncpy ( device -> options , options , length ); <nl> } else {
static int __devinit tg3_get_invariants ( struct tg3 * tp ) <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( GET_ASIC_REV ( tp -> pci_chip_rev_id ) == ASIC_REV_5717 && <nl> + ( tp -> pci_chip_rev_id != CHIPREV_ID_5717_A0 || <nl> + ( tp -> tg3_flags2 & TG3_FLG2_MII_SERDES ))) <nl> + return - ENOTSUPP ; <nl> + <nl> /* Initialize data / descriptor byte / word swapping . */ <nl> val = tr32 ( GRC_MODE ); <nl> val &= GRC_MODE_HOST_STACKUP ;
static inline void wilc_wfi_cfg_parse_ch_attr ( u8 * buf , u32 len , u8 sta_ch ) <nl> if ( index + sizeof (* e ) + attr_size > len ) <nl> return ; <nl>  <nl> - if ( e -> attr_type == IEEE80211_P2P_ATTR_CHANNEL_LIST ) <nl> + if ( e -> attr_type == IEEE80211_P2P_ATTR_CHANNEL_LIST && <nl> + attr_size >= ( sizeof ( struct wilc_attr_ch_list ) - sizeof (* e ))) <nl> ch_list_idx = index ; <nl> else if ( e -> attr_type == IEEE80211_P2P_ATTR_OPER_CHANNEL && <nl> attr_size == ( sizeof ( struct wilc_attr_oper_ch ) - sizeof (* e )))
static void snd_timer_user_ccallback ( struct snd_timer_instance * timeri , <nl> tu -> tstamp = * tstamp ; <nl> if (( tu -> filter & ( 1 << event )) == 0 || ! tu -> tread ) <nl> return ; <nl> + memset (& r1 , 0 , sizeof ( r1 )); <nl> r1 . event = event ; <nl> r1 . tstamp = * tstamp ; <nl> r1 . val = resolution ;
static int omap_gpio_irq_type ( struct irq_data * d , unsigned type ) <nl>  <nl> spin_lock_irqsave (& bank -> lock , flags ); <nl> retval = omap_set_gpio_triggering ( bank , offset , type ); <nl> - if ( retval ) <nl> + if ( retval ) { <nl> + spin_unlock_irqrestore (& bank -> lock , flags ); <nl> goto error ; <nl> + } <nl> omap_gpio_init_irq ( bank , offset ); <nl> if (! omap_gpio_is_input ( bank , offset )) { <nl> spin_unlock_irqrestore (& bank -> lock , flags );
int extract_param ( <nl> if ( len < 0 ) <nl> return - 1 ; <nl>  <nl> - if ( len > max_length ) { <nl> + if ( len >= max_length ) { <nl> pr_err (" Length of input : % d exceeds max_length :" <nl> " % d \ n ", len , max_length ); <nl> return - 1 ;
# include < asm / bitops . h > <nl> # include < asm / uaccess . h > <nl>  <nl> -# include " mxser . h " <nl> +# include " mxser_new . h " <nl>  <nl> # define MXSER_VERSION " 1 . 8 " <nl> # define MXSERMAJOR 174
xfs_acl_from_disk ( struct xfs_acl * aclp ) <nl> struct posix_acl_entry * acl_e ; <nl> struct posix_acl * acl ; <nl> struct xfs_acl_entry * ace ; <nl> - int count , i ; <nl> + unsigned int count , i ; <nl>  <nl> count = be32_to_cpu ( aclp -> acl_cnt ); <nl> if ( count > XFS_ACL_MAX_ENTRIES )
static int stts751_read_chip_config ( struct stts751_priv * priv ) <nl> ret = i2c_smbus_read_byte_data ( priv -> client , STTS751_REG_RATE ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + if ( ret >= ARRAY_SIZE ( stts751_intervals )) { <nl> + dev_err ( priv -> dev , " Unrecognized conversion rate 0x % x \ n ", ret ); <nl> + return - ENODEV ; <nl> + } <nl> priv -> interval = ret ; <nl>  <nl> ret = stts751_read_reg16 ( priv , & priv -> event_max ,
xfrm_init_tempstate ( struct xfrm_state * x , const struct flowi * fl , <nl> { <nl> struct xfrm_state_afinfo * afinfo = xfrm_state_afinfo_get_rcu ( family ); <nl>  <nl> - if ( afinfo ) <nl> - afinfo -> init_tempsel (& x -> sel , fl ); <nl> + if (! afinfo ) <nl> + return ; <nl> + <nl> + afinfo -> init_tempsel (& x -> sel , fl ); <nl>  <nl> if ( family != tmpl -> encap_family ) { <nl> afinfo = xfrm_state_afinfo_get_rcu ( tmpl -> encap_family );
static noinline int btrfs_search_path_in_tree ( struct btrfs_fs_info * info , <nl> key . objectid = key . offset ; <nl> key . offset = ( u64 )- 1 ; <nl> dirid = key . objectid ; <nl> - <nl> } <nl> if ( ptr < name ) <nl> goto out ; <nl> - memcpy ( name , ptr , total_len ); <nl> + memmove ( name , ptr , total_len ); <nl> name [ total_len ]='\ 0 '; <nl> ret = 0 ; <nl> out :
void do_exit ( long code ) <nl>  <nl> module_put ( task_thread_info ( tsk )-> exec_domain -> module ); <nl>  <nl> - proc_exit_connector ( tsk ); <nl> /* <nl> * FIXME : do that only when needed , using sched_exit tracepoint <nl> */ <nl> flush_ptrace_hw_breakpoint ( tsk ); <nl>  <nl> exit_notify ( tsk , group_dead ); <nl> + proc_exit_connector ( tsk ); <nl> # ifdef CONFIG_NUMA <nl> task_lock ( tsk ); <nl> mpol_put ( tsk -> mempolicy );
static void __init imx6sl_init_late ( void ) <nl> if ( IS_ENABLED ( CONFIG_ARM_IMX6Q_CPUFREQ )) <nl> platform_device_register_simple (" imx6q - cpufreq ", - 1 , NULL , 0 ); <nl>  <nl> - if ( cpu_is_imx6sl ()) <nl> + if ( IS_ENABLED ( CONFIG_SOC_IMX6SL ) && cpu_is_imx6sl ()) <nl> imx6sl_cpuidle_init (); <nl> - else <nl> + else if ( IS_ENABLED ( CONFIG_SOC_IMX6SLL )) <nl> imx6sx_cpuidle_init (); <nl> } <nl> 
static struct perf_pmu * pmu_lookup ( const char * name ) <nl> LIST_HEAD ( aliases ); <nl> __u32 type ; <nl>  <nl> + /* No support for intel_bts or intel_pt so disallow them */ <nl> + if (! strcmp ( name , " intel_bts ") || ! strcmp ( name , " intel_pt ")) <nl> + return NULL ; <nl> + <nl> /* <nl> * The pmu data we store & need consists of the pmu <nl> * type value and format definitions . Load both right
static struct ip_conntrack_expect * find_expect ( struct ip_conntrack * ct , <nl> tuple . dst . protonum = IPPROTO_TCP ; <nl>  <nl> exp = __ip_conntrack_expect_find (& tuple ); <nl> - if ( exp -> master == ct ) <nl> + if ( exp && exp -> master == ct ) <nl> return exp ; <nl> return NULL ; <nl> }
int __init dmar_parse_dev_scope ( void * start , void * end , int * cnt , <nl> if ( scope -> entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT || <nl> scope -> entry_type == ACPI_DMAR_SCOPE_TYPE_BRIDGE ) <nl> (* cnt )++; <nl> - else if ( scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC ) { <nl> + else if ( scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC && <nl> + scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_HPET ) { <nl> pr_warn (" Unsupported device scope \ n "); <nl> } <nl> start += scope -> length ;
static struct dmi_system_id __initdata acpisleep_dmi_table [] = { <nl> }, <nl> { <nl> . callback = init_nvs_nosave , <nl> + . ident = " Sony Vaio VGN - FW41E_H ", <nl> + . matches = { <nl> + DMI_MATCH ( DMI_SYS_VENDOR , " Sony Corporation "), <nl> + DMI_MATCH ( DMI_PRODUCT_NAME , " VGN - FW41E_H "), <nl> + }, <nl> + }, <nl> + { <nl> + . callback = init_nvs_nosave , <nl> . ident = " Sony Vaio VGN - FW21E ", <nl> . matches = { <nl> DMI_MATCH ( DMI_SYS_VENDOR , " Sony Corporation "),
static int lz4_uncompress ( const char * source , char * dest , int osize ) <nl> len = * ip ++; <nl> for (; len == 255 ; length += 255 ) <nl> len = * ip ++; <nl> + if ( unlikely ( length > ( size_t )( length + len ))) <nl> + goto _output_error ; <nl> length += len ; <nl> } <nl> 
static int ccid3_hc_tx_getsockopt ( struct sock * sk , const int optname , int len , <nl> case DCCP_SOCKOPT_CCID_TX_INFO : <nl> if ( len < sizeof ( tfrc )) <nl> return - EINVAL ; <nl> + memset (& tfrc , 0 , sizeof ( tfrc )); <nl> tfrc . tfrctx_x = hc -> tx_x ; <nl> tfrc . tfrctx_x_recv = hc -> tx_x_recv ; <nl> tfrc . tfrctx_x_calc = hc -> tx_x_calc ;
static int fsl_lpspi_probe ( struct platform_device * pdev ) <nl> ret = pm_runtime_get_sync ( fsl_lpspi -> dev ); <nl> if ( ret < 0 ) { <nl> dev_err ( fsl_lpspi -> dev , " failed to enable clock \ n "); <nl> - return ret ; <nl> + goto out_controller_put ; <nl> } <nl>  <nl> temp = readl ( fsl_lpspi -> base + IMX7ULP_PARAM );
int i40e_ndo_set_vf_port_vlan ( struct net_device * netdev , int vf_id , <nl> VLAN_VID_MASK )); <nl> } <nl>  <nl> + spin_unlock_bh (& vsi -> mac_filter_hash_lock ); <nl> if ( vlan_id || qos ) <nl> ret = i40e_vsi_add_pvid ( vsi , vlanprio ); <nl> else <nl> i40e_vsi_remove_pvid ( vsi ); <nl> + spin_lock_bh (& vsi -> mac_filter_hash_lock ); <nl>  <nl> if ( vlan_id ) { <nl> dev_info (& pf -> pdev -> dev , " Setting VLAN % d , QOS 0x % x on VF % d \ n ",
int __init musb_platform_init ( struct musb * musb , void * board_data ) <nl>  <nl> usb_nop_xceiv_register (); <nl> musb -> xceiv = otg_get_transceiver (); <nl> - if (! musb -> xceiv ) <nl> + if (! musb -> xceiv ) { <nl> + gpio_free ( musb -> config -> gpio_vrsel ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> if ( ANOMALY_05000346 ) { <nl> bfin_write_USB_APHY_CALIB ( ANOMALY_05000346_value );
static void remap_cell_to_origin_clear_discard ( struct cache * cache , <nl> remap_to_origin ( cache , bio ); <nl> issue ( cache , bio ); <nl> } <nl> + <nl> + free_prison_cell ( cache , cell ); <nl> } <nl>  <nl> static void remap_cell_to_cache_dirty ( struct cache * cache , struct dm_bio_prison_cell * cell , <nl> static void remap_cell_to_cache_dirty ( struct cache * cache , struct dm_bio_prison_ <nl> remap_to_cache ( cache , bio , cblock ); <nl> issue ( cache , bio ); <nl> } <nl> + <nl> + free_prison_cell ( cache , cell ); <nl> } <nl>  <nl> /*----------------------------------------------------------------*/
static int mga_vram_init ( struct mga_device * mdev ) <nl> { <nl> void __iomem * mem ; <nl> struct apertures_struct * aper = alloc_apertures ( 1 ); <nl> + if (! aper ) <nl> + return - ENOMEM ; <nl>  <nl> /* BAR 0 is VRAM */ <nl> mdev -> mc . vram_base = pci_resource_start ( mdev -> dev -> pdev , 0 );
static struct phy * exynos_mipi_video_phy_xlate ( struct device * dev , <nl> { <nl> struct exynos_mipi_video_phy * state = dev_get_drvdata ( dev ); <nl>  <nl> - if ( WARN_ON ( args -> args [ 0 ] > EXYNOS_MIPI_PHYS_NUM )) <nl> + if ( WARN_ON ( args -> args [ 0 ] >= EXYNOS_MIPI_PHYS_NUM )) <nl> return ERR_PTR (- ENODEV ); <nl>  <nl> return state -> phys [ args -> args [ 0 ]]. phy ;
static void iwl4965_rx_reply_tx ( struct iwl_priv * priv , <nl> struct ieee80211_tx_info * info ; <nl> struct iwl4965_tx_resp * tx_resp = ( void *)& pkt -> u . raw [ 0 ]; <nl> u32 status = le32_to_cpu ( tx_resp -> u . status ); <nl> - int tid = MAX_TID_COUNT ; <nl> + int tid = MAX_TID_COUNT - 1 ; <nl> int sta_id ; <nl> int freed ; <nl> u8 * qc = NULL ;
static int irda_recvmsg_dgram ( struct kiocb * iocb , struct socket * sock , <nl>  <nl> IRDA_DEBUG ( 4 , "% s ()\ n ", __func__ ); <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> skb = skb_recv_datagram ( sk , flags & ~ MSG_DONTWAIT , <nl> flags & MSG_DONTWAIT , & err ); <nl> if (! skb )
static int blktrans_open ( struct block_device * bdev , fmode_t mode ) <nl>  <nl> mutex_lock (& dev -> lock ); <nl>  <nl> - if ( dev -> open ++) <nl> + if ( dev -> open ) <nl> goto unlock ; <nl>  <nl> kref_get (& dev -> ref ); <nl> static int blktrans_open ( struct block_device * bdev , fmode_t mode ) <nl> goto error_release ; <nl>  <nl> unlock : <nl> + dev -> open ++; <nl> mutex_unlock (& dev -> lock ); <nl> blktrans_dev_put ( dev ); <nl> return ret ;
int compat_get_timex ( struct timex * txc , const struct compat_timex __user * utp ) <nl> { <nl> struct compat_timex tx32 ; <nl>  <nl> + memset ( txc , 0 , sizeof ( struct timex )); <nl> if ( copy_from_user (& tx32 , utp , sizeof ( struct compat_timex ))) <nl> return - EFAULT ; <nl> 
int qla24xx_async_notify_ack ( scsi_qla_host_t * vha , fc_port_t * fcport , <nl> qla2x00_init_timer ( sp , qla2x00_get_async_timeout ( vha )+ 2 ); <nl>  <nl> sp -> u . iocb_cmd . u . nack . ntfy = ntfy ; <nl> - <nl> + sp -> u . iocb_cmd . timeout = qla2x00_async_iocb_timeout ; <nl> sp -> done = qla2x00_async_nack_sp_done ; <nl>  <nl> rval = qla2x00_start_sp ( sp );
__update_curr ( struct cfs_rq * cfs_rq , struct sched_entity * curr , <nl> schedstat_set ( curr -> exec_max , max (( u64 ) delta_exec , curr -> exec_max )); <nl>  <nl> curr -> sum_exec_runtime += delta_exec ; <nl> - cfs_rq -> exec_clock += delta_exec ; <nl> + schedstat_add ( cfs_rq , exec_clock , delta_exec ); <nl> delta_exec_weighted = delta_exec ; <nl> if ( unlikely ( curr -> load . weight != NICE_0_LOAD )) { <nl> delta_exec_weighted = calc_delta_fair ( delta_exec_weighted ,
static struct snd_emu_chip_details emu_chip_details [] = { <nl> . ca0151_chip = 1 , <nl> . spk71 = 1 , <nl> . spdif_bug = 1 , <nl> + . invert_shared_spdif = 1 , /* digital / analog switch swapped */ <nl> . ac97_chip = 1 } , <nl> {. vendor = 0x1102 , . device = 0x0004 , . subsystem = 0x10021102 , <nl> . driver = " Audigy2 ", . name = " SB Audigy 2 Platinum [ SB0240P ]",
struct stedma40_platform_data dma40_plat_data = { <nl> struct platform_device u8500_dma40_device = { <nl> . dev = { <nl> . platform_data = & dma40_plat_data , <nl> + . coherent_dma_mask = DMA_BIT_MASK ( 32 ), <nl> }, <nl> . name = " dma40 ", <nl> . id = 0 ,
xfs_da3_fixhashpath ( <nl> node = blk -> bp -> b_addr ; <nl> dp -> d_ops -> node_hdr_from_disk (& nodehdr , node ); <nl> btree = dp -> d_ops -> node_tree_p ( node ); <nl> - if ( be32_to_cpu ( btree -> hashval ) == lasthash ) <nl> + if ( be32_to_cpu ( btree [ blk -> index ]. hashval ) == lasthash ) <nl> break ; <nl> blk -> hashval = lasthash ; <nl> btree [ blk -> index ]. hashval = cpu_to_be32 ( lasthash );
static int ethtool_phys_id ( struct net_device * dev , void __user * useraddr ) <nl> if ( rc == 0 ) { <nl> /* Driver will handle this itself */ <nl> schedule_timeout_interruptible ( <nl> - id . data ? id . data : MAX_SCHEDULE_TIMEOUT ); <nl> + id . data ? ( id . data * HZ ) : MAX_SCHEDULE_TIMEOUT ); <nl> } else { <nl> /* Driver expects to be called periodically */ <nl> do {
static int asoc_simple_card_dai_link_of ( struct device_node * node , <nl> strlen ( dai_link -> cpu_dai_name ) + <nl> strlen ( dai_link -> codec_dai_name ) + 2 , <nl> GFP_KERNEL ); <nl> + if (! name ) { <nl> + ret = - ENOMEM ; <nl> + goto dai_link_of_err ; <nl> + } <nl> + <nl> sprintf ( name , "% s -% s ", dai_link -> cpu_dai_name , <nl> dai_link -> codec_dai_name ); <nl> dai_link -> name = dai_link -> stream_name = name ;
static int find_probe_functions ( struct map * map , char * name ) <nl> struct symbol * sym ; <nl> struct rb_node * tmp ; <nl>  <nl> + if ( map__load ( map , NULL ) < 0 ) <nl> + return 0 ; <nl> + <nl> map__for_each_symbol ( map , sym , tmp ) { <nl> if ( strglobmatch ( sym -> name , name )) <nl> found ++;
static int do_recover_data ( struct f2fs_sb_info * sbi , struct inode * inode , <nl> # endif <nl> /* We should not get - ENOSPC */ <nl> f2fs_bug_on ( sbi , err ); <nl> + if ( err ) <nl> + goto err ; <nl> } <nl>  <nl> /* Check the previous node page having this index */
static int uvc_v4l2_open ( struct file * file ) <nl> if ( atomic_inc_return (& stream -> dev -> users ) == 1 ) { <nl> ret = uvc_status_start ( stream -> dev ); <nl> if ( ret < 0 ) { <nl> - usb_autopm_put_interface ( stream -> dev -> intf ); <nl> atomic_dec (& stream -> dev -> users ); <nl> + usb_autopm_put_interface ( stream -> dev -> intf ); <nl> kfree ( handle ); <nl> return ret ; <nl> }
static inline int copy_linear_skb ( struct sk_buff * skb , int len , int off , <nl> { <nl> int n , copy = len - off ; <nl>  <nl> + if ( copy < 0 ) <nl> + return - EINVAL ; <nl> n = copy_to_iter ( skb -> data + off , copy , to ); <nl> if ( n == copy ) <nl> return 0 ;
EXPORT_SYMBOL ( vprintk_emit ); <nl>  <nl> asmlinkage int vprintk ( const char * fmt , va_list args ) <nl> { <nl> - return vprintk_emit ( 0 , LOGLEVEL_DEFAULT , NULL , 0 , fmt , args ); <nl> + return vprintk_func ( fmt , args ); <nl> } <nl> EXPORT_SYMBOL ( vprintk ); <nl> 
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
int smb2_handle_negotiate ( struct ksmbd_work * work ) <nl> status ); <nl> rsp -> hdr . Status = status ; <nl> rc = - EINVAL ; <nl> + kfree ( conn -> preauth_info ); <nl> + conn -> preauth_info = NULL ; <nl> goto err_out ; <nl> } <nl>  <nl> rc = init_smb3_11_server ( conn ); <nl> if ( rc < 0 ) { <nl> rsp -> hdr . Status = STATUS_INVALID_PARAMETER ; <nl> + kfree ( conn -> preauth_info ); <nl> + conn -> preauth_info = NULL ; <nl> goto err_out ; <nl> } <nl> 
static inline void rt2x00lib_set_if_combinations ( struct rt2x00_dev * rt2x00dev ) <nl> */ <nl> if_limit = & rt2x00dev -> if_limits_ap ; <nl> if_limit -> max = rt2x00dev -> ops -> max_ap_intf ; <nl> - if_limit -> types = BIT ( NL80211_IFTYPE_AP ); <nl> + if_limit -> types = BIT ( NL80211_IFTYPE_AP ) | <nl> + BIT ( NL80211_IFTYPE_MESH_POINT ); <nl>  <nl> /* <nl> * Build up AP interface combinations structure .
EXPORT_SYMBOL ( param_set_copystring ); <nl> int param_get_string ( char * buffer , const struct kernel_param * kp ) <nl> { <nl> const struct kparam_string * kps = kp -> str ; <nl> - return strlcpy ( buffer , kps -> string , kps -> maxlen ); <nl> + return strlcpy ( buffer , kps -> string , PAGE_SIZE ); <nl> } <nl> EXPORT_SYMBOL ( param_get_string ); <nl> 
static int __devinit nmk_gpio_probe ( struct platform_device * dev ) <nl> struct clk * clk ; <nl> int secondary_irq ; <nl> void __iomem * base ; <nl> - int irq_start = - 1 ; <nl> + int irq_start = 0 ; <nl> int irq ; <nl> int ret ; <nl> 
struct powerdomain * omap_hwmod_get_pwrdm ( struct omap_hwmod * oh ) <nl> c = oh -> slaves [ oh -> _mpu_port_index ]-> _clk ; <nl> } <nl>  <nl> + if (! c -> clkdm ) <nl> + return NULL ; <nl> + <nl> return c -> clkdm -> pwrdm . ptr ; <nl>  <nl> }
struct inet_peer * inet_getpeer ( const struct inetpeer_addr * daddr , int create ) <nl> p -> rate_last = 0 ; <nl> p -> pmtu_expires = 0 ; <nl> p -> pmtu_orig = 0 ; <nl> + p -> redirect_genid = 0 ; <nl> memset (& p -> redirect_learned , 0 , sizeof ( p -> redirect_learned )); <nl>  <nl> 
static struct snd_pci_quirk alc883_cfg_tbl [] = { <nl> SND_PCI_QUIRK ( 0x17c0 , 0x4071 , " MEDION MD2 ", ALC883_MEDION_MD2 ), <nl> SND_PCI_QUIRK ( 0x1991 , 0x5625 , " Haier W66 ", ALC883_HAIER_W66 ), <nl> SND_PCI_QUIRK ( 0x17aa , 0x3bfc , " Lenovo NB0763 ", ALC883_LENOVO_NB0763 ), <nl> + SND_PCI_QUIRK ( 0x1043 , 0x8249 , " Asus M2A - VM HDMI ", ALC883_3ST_6ch_DIG ), <nl> + SND_PCI_QUIRK ( 0x147b , 0x1083 , " Abit IP35 - PRO ", ALC883_6ST_DIG ), <nl> {} <nl> }; <nl> 
static void <nl> mt76x2_phy_adjust_vga_gain ( struct mt76x2_dev * dev ) <nl> { <nl> u32 false_cca ; <nl> - u8 limit = dev -> cal . low_gain > 1 ? 4 : 16 ; <nl> + u8 limit = dev -> cal . low_gain > 0 ? 16 : 4 ; <nl>  <nl> false_cca = FIELD_GET ( MT_RX_STAT_1_CCA_ERRORS , mt76_rr ( dev , MT_RX_STAT_1 )); <nl> if ( false_cca > 800 && dev -> cal . agc_gain_adjust < limit )
int drm_open ( struct inode * inode , struct file * filp ) <nl> retcode = drm_open_helper ( inode , filp , dev ); <nl> if (! retcode ) { <nl> atomic_inc (& dev -> counts [ _DRM_STAT_OPENS ]); <nl> - if (! dev -> open_count ++) <nl> + if (! dev -> open_count ++) { <nl> retcode = drm_setup ( dev ); <nl> + if ( retcode ) <nl> + dev -> open_count --; <nl> + } <nl> } <nl> if (! retcode ) { <nl> mutex_lock (& dev -> struct_mutex );
static int route4_change ( struct net * net , struct sk_buff * in_skb , <nl> fp = & b -> ht [ h ]; <nl> for ( pfp = rtnl_dereference (* fp ); pfp ; <nl> fp = & pfp -> next , pfp = rtnl_dereference (* fp )) { <nl> - if ( pfp == f ) { <nl> - * fp = f -> next ; <nl> + if ( pfp == fold ) { <nl> + rcu_assign_pointer (* fp , fold -> next ); <nl> break ; <nl> } <nl> }
u16 hpi_entity_alloc_and_pack ( const enum e_entity_type type , <nl> if ( hE ) <nl> return hE ; <nl>  <nl> - HPI_DEBUG_ASSERT ( role > entity_role_null && type < LAST_ENTITY_ROLE ); <nl> + HPI_DEBUG_ASSERT ( role > entity_role_null && type < LAST_ENTITY_TYPE ); <nl>  <nl> bytes_to_copy = entity_type_to_size [ type ] * item_count ; <nl> total_size = hpi_entity_header_size (* entity ) + bytes_to_copy ;
static int hwsim_new_radio_nl ( struct sk_buff * msg , struct genl_info * info ) <nl> if ( info -> attrs [ HWSIM_ATTR_REG_CUSTOM_REG ]) { <nl> u32 idx = nla_get_u32 ( info -> attrs [ HWSIM_ATTR_REG_CUSTOM_REG ]); <nl>  <nl> - if ( idx >= ARRAY_SIZE ( hwsim_world_regdom_custom )) <nl> + if ( idx >= ARRAY_SIZE ( hwsim_world_regdom_custom )) { <nl> + kfree ( hwname ); <nl> return - EINVAL ; <nl> + } <nl> param . regd = hwsim_world_regdom_custom [ idx ]; <nl> } <nl> 
EXPORT_SYMBOL_GPL ( memory_add_physaddr_to_nid ); <nl> void __init acpi_numa_slit_init ( struct acpi_table_slit * slit ) <nl> { <nl> } <nl> + <nl> + void __init <nl> + acpi_numa_processor_affinity_init ( struct acpi_srat_cpu_affinity * pa ) <nl> +{ <nl> +} <nl> # endif
asmlinkage void __sched schedule ( void ) <nl> } <nl> EXPORT_SYMBOL ( schedule ); <nl>  <nl> -# ifdef CONFIG_SMP <nl> +# ifdef CONFIG_MUTEX_SPIN_ON_OWNER <nl> /* <nl> * Look out ! " owner " is an entirely speculative pointer <nl> * access and not reliable .
static int f2fs_write_begin ( struct file * file , struct address_space * mapping , <nl>  <nl> /* check inline_data */ <nl> ipage = get_node_page ( sbi , inode -> i_ino ); <nl> - if ( IS_ERR ( ipage )) <nl> + if ( IS_ERR ( ipage )) { <nl> + err = PTR_ERR ( ipage ); <nl> goto unlock_fail ; <nl> + } <nl>  <nl> set_new_dnode (& dn , inode , ipage , ipage , 0 ); <nl> 
static int __proc_dobitmasks ( void * data , int write , <nl> } else { <nl> rc = cfs_trace_copyin_string ( tmpstr , tmpstrlen , buffer , nob ); <nl> if ( rc < 0 ) { <nl> - cfs_trace_free_string_buffer ( tmpstr , tmpstrlen ); <nl> + kfree ( tmpstr ); <nl> return rc ; <nl> } <nl>  <nl> static int __proc_dobitmasks ( void * data , int write , <nl> * mask |= D_EMERG ; <nl> } <nl>  <nl> - cfs_trace_free_string_buffer ( tmpstr , tmpstrlen ); <nl> + kfree ( tmpstr ); <nl> return rc ; <nl> } <nl> 
i915_gem_do_execbuffer ( struct drm_device * dev , void * data , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( args -> num_cliprects > UINT_MAX / sizeof (* cliprects )) { <nl> + DRM_DEBUG (" execbuf with % u cliprects \ n ", <nl> + args -> num_cliprects ); <nl> + return - EINVAL ; <nl> + } <nl> cliprects = kmalloc ( args -> num_cliprects * sizeof (* cliprects ), <nl> GFP_KERNEL ); <nl> if ( cliprects == NULL ) {
static int pcmciamtd_config ( struct pcmcia_device * link ) <nl> } <nl> dev_info (& dev -> p_dev -> dev , " mtd % d : % s \ n ", mtd -> index , mtd -> name ); <nl> return 0 ; <nl> - <nl> - dev_err (& dev -> p_dev -> dev , " CS Error , exiting \ n "); <nl> - pcmciamtd_release ( link ); <nl> - return - ENODEV ; <nl> } <nl>  <nl> 
static int ath10k_abort_scan ( struct ath10k * ar ) <nl> ret = ath10k_wmi_stop_scan ( ar , & arg ); <nl> if ( ret ) { <nl> ath10k_warn (" could not submit wmi stop scan (% d )\ n ", ret ); <nl> + spin_lock_bh (& ar -> data_lock ); <nl> + ar -> scan . in_progress = false ; <nl> + ath10k_offchan_tx_purge ( ar ); <nl> + spin_unlock_bh (& ar -> data_lock ); <nl> return - EIO ; <nl> } <nl> 
static int ar9002_hw_calibrate ( struct ath_hw * ah , struct ath9k_channel * chan , <nl> return 0 ; <nl>  <nl> ah -> cal_list_curr = currCal = currCal -> calNext ; <nl> - if ( currCal -> calState == CAL_WAITING ) { <nl> + if ( currCal -> calState == CAL_WAITING ) <nl> ath9k_hw_reset_calibration ( ah , currCal ); <nl> - return 0 ; <nl> - } <nl> + <nl> + return 0 ; <nl> } <nl>  <nl> /* Do NF cal only at longer intervals */
xfs_inumbers ( <nl> return error ; <nl>  <nl> bcount = MIN ( left , ( int )( PAGE_SIZE / sizeof (* buffer ))); <nl> - buffer = kmem_alloc ( bcount * sizeof (* buffer ), KM_SLEEP ); <nl> + buffer = kmem_zalloc ( bcount * sizeof (* buffer ), KM_SLEEP ); <nl> do { <nl> struct xfs_inobt_rec_incore r ; <nl> int stat ;
void task_tick_numa ( struct rq * rq , struct task_struct * curr ) <nl> now = curr -> se . sum_exec_runtime ; <nl> period = ( u64 ) curr -> numa_scan_period * NSEC_PER_MSEC ; <nl>  <nl> - if ( now - curr -> node_stamp > period ) { <nl> + if ( now > curr -> node_stamp + period ) { <nl> if (! curr -> node_stamp ) <nl> curr -> numa_scan_period = task_scan_min ( curr ); <nl> curr -> node_stamp += period ;
xfs_zero_remaining_bytes ( <nl> bp = xfs_buf_get_noaddr ( mp -> m_sb . sb_blocksize , <nl> XFS_IS_REALTIME_INODE ( ip ) ? <nl> mp -> m_rtdev_targp : mp -> m_ddev_targp ); <nl> + if (! bp ) <nl> + return XFS_ERROR ( ENOMEM ); <nl>  <nl> for ( offset = startoff ; offset <= endoff ; offset = lastoffset + 1 ) { <nl> offset_fsb = XFS_B_TO_FSBT ( mp , offset );
static int ax25_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> ax25_address src ; <nl> const unsigned char * mac = skb_mac_header ( skb ); <nl>  <nl> + memset ( sax , 0 , sizeof ( struct full_sockaddr_ax25 )); <nl> ax25_addr_parse ( mac + 1 , skb -> data - mac - 1 , & src , NULL , <nl> & digi , NULL , NULL ); <nl> sax -> sax25_family = AF_AX25 ;
static int log_one_block ( struct log_writes_c * lc , <nl> goto out ; <nl> sector ++; <nl>  <nl> + atomic_inc (& lc -> io_blocks ); <nl> bio = bio_alloc ( GFP_KERNEL , block -> vec_cnt ); <nl> if (! bio ) { <nl> DMERR (" Couldn ' t alloc log bio "); <nl> goto error ; <nl> } <nl> - atomic_inc (& lc -> io_blocks ); <nl> bio -> bi_iter . bi_size = 0 ; <nl> bio -> bi_iter . bi_sector = sector ; <nl> bio -> bi_bdev = lc -> logdev -> bdev ;
int do_huge_pmd_numa_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl>  <nl> check_same : <nl> spin_lock (& mm -> page_table_lock ); <nl> - if ( unlikely (! pmd_same ( pmd , * pmdp ))) <nl> + if ( unlikely (! pmd_same ( pmd , * pmdp ))) { <nl> + /* Someone else took our fault */ <nl> + current_nid = - 1 ; <nl> goto out_unlock ; <nl> + } <nl> clear_pmdnuma : <nl> pmd = pmd_mknonnuma ( pmd ); <nl> set_pmd_at ( mm , haddr , pmdp , pmd );
static void set_truncation ( <nl> REG_UPDATE_3 ( FMT_BIT_DEPTH_CONTROL , <nl> FMT_TRUNCATE_EN , 1 , <nl> FMT_TRUNCATE_DEPTH , <nl> - params -> flags . TRUNCATE_MODE , <nl> + params -> flags . TRUNCATE_DEPTH , <nl> FMT_TRUNCATE_MODE , <nl> - params -> flags . TRUNCATE_DEPTH ); <nl> + params -> flags . TRUNCATE_MODE ); <nl> } <nl>  <nl> 
static int tile_net_poll ( struct napi_struct * napi , int budget ) <nl> struct info_mpipe * info_mpipe = <nl> container_of ( napi , struct info_mpipe , napi ); <nl>  <nl> + if ( budget <= 0 ) <nl> + goto done ; <nl> + <nl> instance = info_mpipe -> instance ; <nl> while (( n = gxio_mpipe_iqueue_try_peek ( <nl> & info_mpipe -> iqueue ,
static netdev_tx_t xgene_enet_start_xmit ( struct sk_buff * skb , <nl> return NETDEV_TX_OK ; <nl> } <nl>  <nl> - pdata -> ring_ops -> wr_cmd ( tx_ring , count ); <nl> skb_tx_timestamp ( skb ); <nl>  <nl> pdata -> stats . tx_packets ++; <nl> pdata -> stats . tx_bytes += skb -> len ; <nl>  <nl> + pdata -> ring_ops -> wr_cmd ( tx_ring , count ); <nl> return NETDEV_TX_OK ; <nl> } <nl> 
crun_command_exec ( struct crun_global_arguments * global_args , int argc , char ** a <nl> capabilities -> effective = exec_options . cap ; <nl> capabilities -> effective_len = exec_options . cap_size ; <nl>  <nl> - capabilities -> inheritable = dup_array ( exec_options . cap , exec_options . cap_size ); <nl> - capabilities -> inheritable_len = exec_options . cap_size ; <nl> + capabilities -> inheritable = NULL ; <nl> + capabilities -> inheritable_len = 0 ; <nl>  <nl> capabilities -> bounding = dup_array ( exec_options . cap , exec_options . cap_size ); <nl> capabilities -> bounding_len = exec_options . cap_size ;
DU_getStringDOElement ( DcmItem * obj , DcmTagKey t , char * s , size_t bufsize ) <nl> s [ 0 ] = '\ 0 '; <nl> } else { <nl> ec = elem -> getString ( aString ); <nl> - OFStandard :: strlcpy ( s , aString , bufsize ); <nl> + if ( ec == EC_Normal ) <nl> + OFStandard :: strlcpy ( s , aString , bufsize ); <nl> } <nl> } <nl> return ( ec == EC_Normal );
vhost_user_check_and_alloc_queue_pair ( struct virtio_net * dev , <nl> case VHOST_USER_SET_VRING_ADDR : <nl> vring_idx = ctx -> msg . payload . addr . index ; <nl> break ; <nl> + case VHOST_USER_SET_INFLIGHT_FD : <nl> + vring_idx = ctx -> msg . payload . inflight . num_queues - 1 ; <nl> + break ; <nl> default : <nl> return 0 ; <nl> }
gnutls_ocsp_resp_check_crt ( gnutls_ocsp_resp_t resp , <nl> gnutls_assert (); <nl> goto cleanup ; <nl> } <nl> + cserial . size = t ; <nl>  <nl> if ( rserial . size != cserial . size <nl> || memcmp ( cserial . data , rserial . data , rserial . size ) != 0 ) {
int secure_decrypt ( void * data , unsigned int data_length , int is_signed ) <nl> /* Check the CMAC */ <nl> fixed_length = at91_aes_roundup ( data_length ); <nl> cmac = ( const unsigned int *)(( char *) data + fixed_length ); <nl> - if ( memcmp ( cmac , computed_cmac , AT91_AES_BLOCK_SIZE_BYTE )) <nl> + if (! consttime_memequal ( cmac , computed_cmac , AT91_AES_BLOCK_SIZE_BYTE )) <nl> goto exit ; <nl> } <nl> 
static int stream_process ( struct sip_msg * msg , struct sdp_stream_cell * cell , <nl> /* when trimming the very last payload , avoid trailing ws */ <nl> if ( cur == lmp -> u . value + lmp -> len ) { <nl> tmp = found . s ; <nl> - while (*(-- tmp ) == ' ') { <nl> + while ( tmp > lmp -> u . value && *(-- tmp ) == ' ') { <nl> found . s --; <nl> found . len ++; <nl> }
IMPEG2D_ERROR_CODES_T impeg2d_dec_p_b_slice ( dec_state_t * ps_dec ) <nl>  <nl> if ( ret ) <nl> return IMPEG2D_MB_TEX_DECODE_ERR ; <nl> + <nl> + if ( 0 >= ps_dec -> u2_num_mbs_left ) <nl> + { <nl> + break ; <nl> + } <nl> + <nl> IMPEG2D_TRACE_MB_START ( ps_dec -> u2_mb_x , ps_dec -> u2_mb_y ); <nl>  <nl> u4_x_dst_offset = u4_frm_offset + ( ps_dec -> u2_mb_x << 4 );
native_handle * Parcel :: readNativeHandle () const <nl> if ( err != NO_ERROR ) return 0 ; <nl>  <nl> native_handle * h = native_handle_create ( numFds , numInts ); <nl> + if (! h ) { <nl> + return 0 ; <nl> + } <nl> + <nl> for ( int i = 0 ; err == NO_ERROR && i < numFds ; i ++) { <nl> h -> data [ i ] = dup ( readFileDescriptor ()); <nl> if ( h -> data [ i ] < 0 ) err = BAD_VALUE ;
vq_endchains ( struct virtio_vq_info * vq , int used_all_avail ) <nl> uint16_t event_idx , new_idx , old_idx ; <nl> int intr ; <nl>  <nl> + if (! vq || ! vq -> used ) <nl> + return ; <nl> + <nl> /* <nl> * Interrupt generation : if we ' re using EVENT_IDX , <nl> * interrupt if we ' ve crossed the event threshold .
status_t OMXCodec :: allocateBuffersOnPort ( OMX_U32 portIndex ) { <nl>  <nl> for ( OMX_U32 i = 0 ; i < def . nBufferCountActual ; ++ i ) { <nl> sp < IMemory > mem = mDealer [ portIndex ]-> allocate ( def . nBufferSize ); <nl> - CHECK ( mem . get () != NULL ); <nl> + if ( mem == NULL || mem -> pointer () == NULL ) { <nl> + return NO_MEMORY ; <nl> + } <nl>  <nl> BufferInfo info ; <nl> info . mData = NULL ;
static bool MR_primality_test ( UnsignedBigInteger n , const Vector < UnsignedBigInte <nl> return n == 2 ; <nl> } <nl>  <nl> - for ( auto a : tests ) { <nl> + for ( auto & a : tests ) { <nl> // Technically : ASSERT ( 2 <= a && a <= n - 2 ) <nl> ASSERT ( a < n ); <nl> auto x = ModularPower ( a , d , n );
class c_single_allocator <nl>  <nl> std :: size_t size () const { return _buf_size ; } <nl>  <nl> - void resize ( std :: size_t new_size_ ) { _buf_size = new_size_ ; } <nl> + // This buffer is fixed , size must not be changed <nl> + void resize ( std :: size_t new_size_ ) { LIBZMQ_UNUSED ( new_size_ ); } <nl>  <nl> private : <nl> std :: size_t _buf_size ;
void csync_daemon_session () <nl> goto conn_without_ssl_ok ; <nl> } <nl> cmd_error = conn_response ( CR_ERR_SSL_EXPECTED ); <nl> + peer = NULL ; <nl> } <nl> conn_without_ssl_ok :; <nl> # endif
TfLiteStatus Eval ( TfLiteContext * context , TfLiteNode * node ) { <nl> TF_LITE_ENSURE_OK ( context , <nl> GetOutputSafe ( context , node , kOutputTensor , & output )); <nl>  <nl> + // Prevent division by 0 in the helper <nl> + TF_LITE_ENSURE ( context , NumElements ( params ) > 0 ); <nl> + <nl> switch ( indices -> type ) { <nl> case kTfLiteInt32 : <nl> return EvalGatherNd < int32_t >( context , params , indices , output );
class RaggedTensorToVariantOp : public OpKernel { <nl> batched_ragged_input . mutable_nested_splits ()-> reserve ( <nl> ragged_nested_splits_len ); <nl> for ( int i = 0 ; i < ragged_nested_splits_len ; i ++) { <nl> + OP_REQUIRES ( context , ragged_nested_splits_in [ i ]. dims () == 1 , <nl> + errors :: InvalidArgument (" Requires nested_row_splits [", i , "]", <nl> + " to be rank 1 but is rank ", <nl> + ragged_nested_splits_in [ i ]. dims ())); <nl> batched_ragged_input . append_splits ( ragged_nested_splits_in [ i ]); <nl> } <nl> 
Status ImportGenericFunction ( <nl> // Import the function attributes with a ` tf .` prefix to match the current <nl> // infrastructure expectations . <nl> for ( const auto & namedAttr : func . attr ()) { <nl> + if ( namedAttr . first . empty ()) <nl> + return InvalidArgument (" Invalid function attribute name "); <nl> const std :: string & name = " tf ." + namedAttr . first ; <nl> const AttrValue & tf_attr = namedAttr . second ; <nl> TF_ASSIGN_OR_RETURN ( Attribute attr ,
static void _jbn_add_item ( JBL_NODE parent , JBL_NODE node ); <nl>  <nl> void iwjson_ftoa ( long double val , char buf [ static IWNUMBUF_SIZE ], size_t * out_len ) { <nl> // TODO : review <nl> - int len = snprintf ( buf , 64 , "%. 8Lf ", val ); <nl> + int len = snprintf ( buf , IWNUMBUF_SIZE , "%. 8Lf ", val ); <nl> if ( len <= 0 ) { <nl> buf [ 0 ] = '\ 0 '; <nl> * out_len = 0 ;
void TabSpecificContentSettings :: OnContentBlocked ( <nl> const std :: string & resource_identifier ) { <nl> DCHECK ( type != CONTENT_SETTINGS_TYPE_GEOLOCATION ) <nl> << " Geolocation settings handled by OnGeolocationPermissionSet "; <nl> + if ( type < 0 || type >= CONTENT_SETTINGS_NUM_TYPES ) <nl> + return ; <nl> content_accessed_ [ type ] = true ; <nl> // Unless UI for resource content settings is enabled , ignore the resource <nl> // identifier .